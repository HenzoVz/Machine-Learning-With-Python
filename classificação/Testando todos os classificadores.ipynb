{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(random_state=1,\n",
    "                          solver='newton-cg',\n",
    "                          multi_class='multinomial')\n",
    "clf2 = RandomForestClassifier(max_depth=5,random_state=1, n_estimators=100)\n",
    "clf3 = DecisionTreeClassifier(max_depth=5, random_state=1)\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5 = QuadraticDiscriminantAnalysis()\n",
    "clf6 = GaussianNB()\n",
    "clf7 = SVC(kernel='linear', random_state=1)\n",
    "clf8 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001, activation='identity',\n",
    "                     solver='sgd', verbose=10,  random_state=1,tol=0.000000001)\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0,2]]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9600000000000002\n",
      "0.9466666666666667\n",
      "0.9333333333333333\n",
      "0.9\n",
      "0.96\n",
      "0.9133333333333333\n",
      "0.9533333333333334\n",
      "Iteration 1, loss = 1.51599959\n",
      "Iteration 2, loss = 1.41988690\n",
      "Iteration 3, loss = 1.35730414\n",
      "Iteration 4, loss = 1.32009140\n",
      "Iteration 5, loss = 1.28816694\n",
      "Iteration 6, loss = 1.24951303\n",
      "Iteration 7, loss = 1.20134296\n",
      "Iteration 8, loss = 1.14760504\n",
      "Iteration 9, loss = 1.09450278\n",
      "Iteration 10, loss = 1.04567186\n",
      "Iteration 11, loss = 1.00091952\n",
      "Iteration 12, loss = 0.95868437\n",
      "Iteration 13, loss = 0.91835354\n",
      "Iteration 14, loss = 0.88035038\n",
      "Iteration 15, loss = 0.84517746\n",
      "Iteration 16, loss = 0.81291416\n",
      "Iteration 17, loss = 0.78335716\n",
      "Iteration 18, loss = 0.75628806\n",
      "Iteration 19, loss = 0.73155181\n",
      "Iteration 20, loss = 0.70899777\n",
      "Iteration 21, loss = 0.68843216\n",
      "Iteration 22, loss = 0.66963426\n",
      "Iteration 23, loss = 0.65239873\n",
      "Iteration 24, loss = 0.63655897\n",
      "Iteration 25, loss = 0.62198197\n",
      "Iteration 26, loss = 0.60855165\n",
      "Iteration 27, loss = 0.59615763\n",
      "Iteration 28, loss = 0.58469383\n",
      "Iteration 29, loss = 0.57406198\n",
      "Iteration 30, loss = 0.56417425\n",
      "Iteration 31, loss = 0.55495343\n",
      "Iteration 32, loss = 0.54633165\n",
      "Iteration 33, loss = 0.53824913\n",
      "Iteration 34, loss = 0.53065319\n",
      "Iteration 35, loss = 0.52349753\n",
      "Iteration 36, loss = 0.51674129\n",
      "Iteration 37, loss = 0.51034820\n",
      "Iteration 38, loss = 0.50428579\n",
      "Iteration 39, loss = 0.49852480\n",
      "Iteration 40, loss = 0.49303890\n",
      "Iteration 41, loss = 0.48780434\n",
      "Iteration 42, loss = 0.48279980\n",
      "Iteration 43, loss = 0.47800606\n",
      "Iteration 44, loss = 0.47340589\n",
      "Iteration 45, loss = 0.46898375\n",
      "Iteration 46, loss = 0.46472567\n",
      "Iteration 47, loss = 0.46061905\n",
      "Iteration 48, loss = 0.45665246\n",
      "Iteration 49, loss = 0.45281560\n",
      "Iteration 50, loss = 0.44909906\n",
      "Iteration 51, loss = 0.44549430\n",
      "Iteration 52, loss = 0.44199356\n",
      "Iteration 53, loss = 0.43858972\n",
      "Iteration 54, loss = 0.43527632\n",
      "Iteration 55, loss = 0.43204743\n",
      "Iteration 56, loss = 0.42889765\n",
      "Iteration 57, loss = 0.42582204\n",
      "Iteration 58, loss = 0.42281608\n",
      "Iteration 59, loss = 0.41987563\n",
      "Iteration 60, loss = 0.41699690\n",
      "Iteration 61, loss = 0.41417639\n",
      "Iteration 62, loss = 0.41141092\n",
      "Iteration 63, loss = 0.40869754\n",
      "Iteration 64, loss = 0.40603353\n",
      "Iteration 65, loss = 0.40341640\n",
      "Iteration 66, loss = 0.40084386\n",
      "Iteration 67, loss = 0.39831376\n",
      "Iteration 68, loss = 0.39582414\n",
      "Iteration 69, loss = 0.39337319\n",
      "Iteration 70, loss = 0.39095922\n",
      "Iteration 71, loss = 0.38858069\n",
      "Iteration 72, loss = 0.38623614\n",
      "Iteration 73, loss = 0.38392423\n",
      "Iteration 74, loss = 0.38164373\n",
      "Iteration 75, loss = 0.37939349\n",
      "Iteration 76, loss = 0.37717242\n",
      "Iteration 77, loss = 0.37497953\n",
      "Iteration 78, loss = 0.37281390\n",
      "Iteration 79, loss = 0.37067465\n",
      "Iteration 80, loss = 0.36856098\n",
      "Iteration 81, loss = 0.36647214\n",
      "Iteration 82, loss = 0.36440741\n",
      "Iteration 83, loss = 0.36236614\n",
      "Iteration 84, loss = 0.36034771\n",
      "Iteration 85, loss = 0.35835155\n",
      "Iteration 86, loss = 0.35637711\n",
      "Iteration 87, loss = 0.35442387\n",
      "Iteration 88, loss = 0.35249138\n",
      "Iteration 89, loss = 0.35057916\n",
      "Iteration 90, loss = 0.34868681\n",
      "Iteration 91, loss = 0.34681391\n",
      "Iteration 92, loss = 0.34496011\n",
      "Iteration 93, loss = 0.34312503\n",
      "Iteration 94, loss = 0.34130835\n",
      "Iteration 95, loss = 0.33950974\n",
      "Iteration 96, loss = 0.33772891\n",
      "Iteration 97, loss = 0.33596557\n",
      "Iteration 98, loss = 0.33421945\n",
      "Iteration 99, loss = 0.33249029\n",
      "Iteration 100, loss = 0.33077784\n",
      "Iteration 101, loss = 0.32908187\n",
      "Iteration 102, loss = 0.32740216\n",
      "Iteration 103, loss = 0.32573850\n",
      "Iteration 104, loss = 0.32409067\n",
      "Iteration 105, loss = 0.32245849\n",
      "Iteration 106, loss = 0.32084176\n",
      "Iteration 107, loss = 0.31924031\n",
      "Iteration 108, loss = 0.31765395\n",
      "Iteration 109, loss = 0.31608254\n",
      "Iteration 110, loss = 0.31452589\n",
      "Iteration 111, loss = 0.31298386\n",
      "Iteration 112, loss = 0.31145629\n",
      "Iteration 113, loss = 0.30994305\n",
      "Iteration 114, loss = 0.30844398\n",
      "Iteration 115, loss = 0.30695894\n",
      "Iteration 116, loss = 0.30548782\n",
      "Iteration 117, loss = 0.30403046\n",
      "Iteration 118, loss = 0.30258675\n",
      "Iteration 119, loss = 0.30115656\n",
      "Iteration 120, loss = 0.29973977\n",
      "Iteration 121, loss = 0.29833626\n",
      "Iteration 122, loss = 0.29694591\n",
      "Iteration 123, loss = 0.29556861\n",
      "Iteration 124, loss = 0.29420424\n",
      "Iteration 125, loss = 0.29285270\n",
      "Iteration 126, loss = 0.29151387\n",
      "Iteration 127, loss = 0.29018765\n",
      "Iteration 128, loss = 0.28887393\n",
      "Iteration 129, loss = 0.28757261\n",
      "Iteration 130, loss = 0.28628358\n",
      "Iteration 131, loss = 0.28500674\n",
      "Iteration 132, loss = 0.28374199\n",
      "Iteration 133, loss = 0.28248923\n",
      "Iteration 134, loss = 0.28124836\n",
      "Iteration 135, loss = 0.28001928\n",
      "Iteration 136, loss = 0.27880190\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "gs = gridspec.GridSpec(15, 15)\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest','Decision Tree Classifier', 'AdaBoost Classifier', 'Quadratic Discriminant Analysis', 'Naive Bayes', 'SVM', 'MLP Classifier']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1, 2, 3, 4, 5, 6, 7 ], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    scores_dt = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "    print(scores_dt.mean())\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
