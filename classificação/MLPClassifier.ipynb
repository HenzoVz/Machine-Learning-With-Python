{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Iris.csv')\n",
    "data = df.rename(index=str, columns={\"Species\": \"Class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm        Class\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      "Id               150 non-null int64\n",
      "SepalLengthCm    150 non-null float64\n",
      "SepalWidthCm     150 non-null float64\n",
      "PetalLengthCm    150 non-null float64\n",
      "PetalWidthCm     150 non-null float64\n",
      "Class            150 non-null object\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_iris = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "X = data.drop(['SepalWidthCm','PetalWidthCm','Class','Id'],axis=1).values\n",
    "y = data['Class'].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_previsores = LabelEncoder()\n",
    "y = labelencoder_previsores.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_features = scaler.transform(X)\n",
    "X = pd.DataFrame(scaled_features,columns=df.columns[1:3]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001, activation='identity',\n",
    "                     solver='sgd', verbose=10,  random_state=0,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.47259835\n",
      "Iteration 2, loss = 1.45394536\n",
      "Iteration 3, loss = 1.42787577\n",
      "Iteration 4, loss = 1.39566475\n",
      "Iteration 5, loss = 1.35854645\n",
      "Iteration 6, loss = 1.31769006\n",
      "Iteration 7, loss = 1.27418234\n",
      "Iteration 8, loss = 1.22901531\n",
      "Iteration 9, loss = 1.18307793\n",
      "Iteration 10, loss = 1.13715111\n",
      "Iteration 11, loss = 1.09190530\n",
      "Iteration 12, loss = 1.04790051\n",
      "Iteration 13, loss = 1.00558848\n",
      "Iteration 14, loss = 0.96531696\n",
      "Iteration 15, loss = 0.92733612\n",
      "Iteration 16, loss = 0.89180676\n",
      "Iteration 17, loss = 0.85881014\n",
      "Iteration 18, loss = 0.82835902\n",
      "Iteration 19, loss = 0.80040922\n",
      "Iteration 20, loss = 0.77487134\n",
      "Iteration 21, loss = 0.75162199\n",
      "Iteration 22, loss = 0.73051402\n",
      "Iteration 23, loss = 0.71138563\n",
      "Iteration 24, loss = 0.69406798\n",
      "Iteration 25, loss = 0.67839145\n",
      "Iteration 26, loss = 0.66419041\n",
      "Iteration 27, loss = 0.65130671\n",
      "Iteration 28, loss = 0.63959219\n",
      "Iteration 29, loss = 0.62891007\n",
      "Iteration 30, loss = 0.61913579\n",
      "Iteration 31, loss = 0.61015715\n",
      "Iteration 32, loss = 0.60187409\n",
      "Iteration 33, loss = 0.59419814\n",
      "Iteration 34, loss = 0.58705171\n",
      "Iteration 35, loss = 0.58036721\n",
      "Iteration 36, loss = 0.57408614\n",
      "Iteration 37, loss = 0.56815820\n",
      "Iteration 38, loss = 0.56254033\n",
      "Iteration 39, loss = 0.55719590\n",
      "Iteration 40, loss = 0.55209385\n",
      "Iteration 41, loss = 0.54720798\n",
      "Iteration 42, loss = 0.54251624\n",
      "Iteration 43, loss = 0.53800013\n",
      "Iteration 44, loss = 0.53364417\n",
      "Iteration 45, loss = 0.52943539\n",
      "Iteration 46, loss = 0.52536291\n",
      "Iteration 47, loss = 0.52141760\n",
      "Iteration 48, loss = 0.51759172\n",
      "Iteration 49, loss = 0.51387869\n",
      "Iteration 50, loss = 0.51027284\n",
      "Iteration 51, loss = 0.50676922\n",
      "Iteration 52, loss = 0.50336345\n",
      "Iteration 53, loss = 0.50005158\n",
      "Iteration 54, loss = 0.49682999\n",
      "Iteration 55, loss = 0.49369532\n",
      "Iteration 56, loss = 0.49064439\n",
      "Iteration 57, loss = 0.48767415\n",
      "Iteration 58, loss = 0.48478165\n",
      "Iteration 59, loss = 0.48196405\n",
      "Iteration 60, loss = 0.47921853\n",
      "Iteration 61, loss = 0.47654235\n",
      "Iteration 62, loss = 0.47393281\n",
      "Iteration 63, loss = 0.47138728\n",
      "Iteration 64, loss = 0.46890316\n",
      "Iteration 65, loss = 0.46647793\n",
      "Iteration 66, loss = 0.46410913\n",
      "Iteration 67, loss = 0.46179437\n",
      "Iteration 68, loss = 0.45953133\n",
      "Iteration 69, loss = 0.45731779\n",
      "Iteration 70, loss = 0.45515162\n",
      "Iteration 71, loss = 0.45303076\n",
      "Iteration 72, loss = 0.45095325\n",
      "Iteration 73, loss = 0.44891725\n",
      "Iteration 74, loss = 0.44692099\n",
      "Iteration 75, loss = 0.44496279\n",
      "Iteration 76, loss = 0.44304109\n",
      "Iteration 77, loss = 0.44115438\n",
      "Iteration 78, loss = 0.43930128\n",
      "Iteration 79, loss = 0.43748046\n",
      "Iteration 80, loss = 0.43569068\n",
      "Iteration 81, loss = 0.43393078\n",
      "Iteration 82, loss = 0.43219965\n",
      "Iteration 83, loss = 0.43049626\n",
      "Iteration 84, loss = 0.42881965\n",
      "Iteration 85, loss = 0.42716888\n",
      "Iteration 86, loss = 0.42554309\n",
      "Iteration 87, loss = 0.42394147\n",
      "Iteration 88, loss = 0.42236322\n",
      "Iteration 89, loss = 0.42080762\n",
      "Iteration 90, loss = 0.41927395\n",
      "Iteration 91, loss = 0.41776156\n",
      "Iteration 92, loss = 0.41626979\n",
      "Iteration 93, loss = 0.41479803\n",
      "Iteration 94, loss = 0.41334571\n",
      "Iteration 95, loss = 0.41191226\n",
      "Iteration 96, loss = 0.41049714\n",
      "Iteration 97, loss = 0.40909983\n",
      "Iteration 98, loss = 0.40771983\n",
      "Iteration 99, loss = 0.40635667\n",
      "Iteration 100, loss = 0.40500988\n",
      "Iteration 101, loss = 0.40367902\n",
      "Iteration 102, loss = 0.40236366\n",
      "Iteration 103, loss = 0.40106338\n",
      "Iteration 104, loss = 0.39977779\n",
      "Iteration 105, loss = 0.39850650\n",
      "Iteration 106, loss = 0.39724914\n",
      "Iteration 107, loss = 0.39600536\n",
      "Iteration 108, loss = 0.39477481\n",
      "Iteration 109, loss = 0.39355715\n",
      "Iteration 110, loss = 0.39235207\n",
      "Iteration 111, loss = 0.39115925\n",
      "Iteration 112, loss = 0.38997841\n",
      "Iteration 113, loss = 0.38880924\n",
      "Iteration 114, loss = 0.38765148\n",
      "Iteration 115, loss = 0.38650485\n",
      "Iteration 116, loss = 0.38536909\n",
      "Iteration 117, loss = 0.38424395\n",
      "Iteration 118, loss = 0.38312920\n",
      "Iteration 119, loss = 0.38202459\n",
      "Iteration 120, loss = 0.38092991\n",
      "Iteration 121, loss = 0.37984493\n",
      "Iteration 122, loss = 0.37876944\n",
      "Iteration 123, loss = 0.37770323\n",
      "Iteration 124, loss = 0.37664611\n",
      "Iteration 125, loss = 0.37559789\n",
      "Iteration 126, loss = 0.37455837\n",
      "Iteration 127, loss = 0.37352738\n",
      "Iteration 128, loss = 0.37250475\n",
      "Iteration 129, loss = 0.37149029\n",
      "Iteration 130, loss = 0.37048385\n",
      "Iteration 131, loss = 0.36948527\n",
      "Iteration 132, loss = 0.36849438\n",
      "Iteration 133, loss = 0.36751105\n",
      "Iteration 134, loss = 0.36653511\n",
      "Iteration 135, loss = 0.36556643\n",
      "Iteration 136, loss = 0.36460487\n",
      "Iteration 137, loss = 0.36365029\n",
      "Iteration 138, loss = 0.36270256\n",
      "Iteration 139, loss = 0.36176155\n",
      "Iteration 140, loss = 0.36082713\n",
      "Iteration 141, loss = 0.35989919\n",
      "Iteration 142, loss = 0.35897761\n",
      "Iteration 143, loss = 0.35806226\n",
      "Iteration 144, loss = 0.35715304\n",
      "Iteration 145, loss = 0.35624984\n",
      "Iteration 146, loss = 0.35535255\n",
      "Iteration 147, loss = 0.35446106\n",
      "Iteration 148, loss = 0.35357528\n",
      "Iteration 149, loss = 0.35269511\n",
      "Iteration 150, loss = 0.35182044\n",
      "Iteration 151, loss = 0.35095118\n",
      "Iteration 152, loss = 0.35008725\n",
      "Iteration 153, loss = 0.34922855\n",
      "Iteration 154, loss = 0.34837500\n",
      "Iteration 155, loss = 0.34752650\n",
      "Iteration 156, loss = 0.34668298\n",
      "Iteration 157, loss = 0.34584436\n",
      "Iteration 158, loss = 0.34501054\n",
      "Iteration 159, loss = 0.34418147\n",
      "Iteration 160, loss = 0.34335706\n",
      "Iteration 161, loss = 0.34253723\n",
      "Iteration 162, loss = 0.34172192\n",
      "Iteration 163, loss = 0.34091106\n",
      "Iteration 164, loss = 0.34010457\n",
      "Iteration 165, loss = 0.33930239\n",
      "Iteration 166, loss = 0.33850445\n",
      "Iteration 167, loss = 0.33771068\n",
      "Iteration 168, loss = 0.33692103\n",
      "Iteration 169, loss = 0.33613544\n",
      "Iteration 170, loss = 0.33535384\n",
      "Iteration 171, loss = 0.33457617\n",
      "Iteration 172, loss = 0.33380237\n",
      "Iteration 173, loss = 0.33303240\n",
      "Iteration 174, loss = 0.33226619\n",
      "Iteration 175, loss = 0.33150369\n",
      "Iteration 176, loss = 0.33074485\n",
      "Iteration 177, loss = 0.32998962\n",
      "Iteration 178, loss = 0.32923794\n",
      "Iteration 179, loss = 0.32848977\n",
      "Iteration 180, loss = 0.32774505\n",
      "Iteration 181, loss = 0.32700375\n",
      "Iteration 182, loss = 0.32626581\n",
      "Iteration 183, loss = 0.32553119\n",
      "Iteration 184, loss = 0.32479985\n",
      "Iteration 185, loss = 0.32407174\n",
      "Iteration 186, loss = 0.32334681\n",
      "Iteration 187, loss = 0.32262504\n",
      "Iteration 188, loss = 0.32190637\n",
      "Iteration 189, loss = 0.32119076\n",
      "Iteration 190, loss = 0.32047819\n",
      "Iteration 191, loss = 0.31976860\n",
      "Iteration 192, loss = 0.31906196\n",
      "Iteration 193, loss = 0.31835824\n",
      "Iteration 194, loss = 0.31765740\n",
      "Iteration 195, loss = 0.31695940\n",
      "Iteration 196, loss = 0.31626421\n",
      "Iteration 197, loss = 0.31557179\n",
      "Iteration 198, loss = 0.31488212\n",
      "Iteration 199, loss = 0.31419515\n",
      "Iteration 200, loss = 0.31351087\n",
      "Iteration 201, loss = 0.31282923\n",
      "Iteration 202, loss = 0.31215020\n",
      "Iteration 203, loss = 0.31147376\n",
      "Iteration 204, loss = 0.31079988\n",
      "Iteration 205, loss = 0.31012852\n",
      "Iteration 206, loss = 0.30945967\n",
      "Iteration 207, loss = 0.30879328\n",
      "Iteration 208, loss = 0.30812934\n",
      "Iteration 209, loss = 0.30746782\n",
      "Iteration 210, loss = 0.30680869\n",
      "Iteration 211, loss = 0.30615193\n",
      "Iteration 212, loss = 0.30549750\n",
      "Iteration 213, loss = 0.30484540\n",
      "Iteration 214, loss = 0.30419558\n",
      "Iteration 215, loss = 0.30354804\n",
      "Iteration 216, loss = 0.30290274\n",
      "Iteration 217, loss = 0.30225966\n",
      "Iteration 218, loss = 0.30161879\n",
      "Iteration 219, loss = 0.30098009\n",
      "Iteration 220, loss = 0.30034356\n",
      "Iteration 221, loss = 0.29970915\n",
      "Iteration 222, loss = 0.29907687\n",
      "Iteration 223, loss = 0.29844668\n",
      "Iteration 224, loss = 0.29781857\n",
      "Iteration 225, loss = 0.29719251\n",
      "Iteration 226, loss = 0.29656849\n",
      "Iteration 227, loss = 0.29594649\n",
      "Iteration 228, loss = 0.29532649\n",
      "Iteration 229, loss = 0.29470848\n",
      "Iteration 230, loss = 0.29409243\n",
      "Iteration 231, loss = 0.29347832\n",
      "Iteration 232, loss = 0.29286615\n",
      "Iteration 233, loss = 0.29225590\n",
      "Iteration 234, loss = 0.29164754\n",
      "Iteration 235, loss = 0.29104106\n",
      "Iteration 236, loss = 0.29043645\n",
      "Iteration 237, loss = 0.28983370\n",
      "Iteration 238, loss = 0.28923277\n",
      "Iteration 239, loss = 0.28863367\n",
      "Iteration 240, loss = 0.28803638\n",
      "Iteration 241, loss = 0.28744088\n",
      "Iteration 242, loss = 0.28684716\n",
      "Iteration 243, loss = 0.28625521\n",
      "Iteration 244, loss = 0.28566501\n",
      "Iteration 245, loss = 0.28507654\n",
      "Iteration 246, loss = 0.28448981\n",
      "Iteration 247, loss = 0.28390478\n",
      "Iteration 248, loss = 0.28332146\n",
      "Iteration 249, loss = 0.28273983\n",
      "Iteration 250, loss = 0.28215987\n",
      "Iteration 251, loss = 0.28158159\n",
      "Iteration 252, loss = 0.28100495\n",
      "Iteration 253, loss = 0.28042996\n",
      "Iteration 254, loss = 0.27985660\n",
      "Iteration 255, loss = 0.27928486\n",
      "Iteration 256, loss = 0.27871474\n",
      "Iteration 257, loss = 0.27814621\n",
      "Iteration 258, loss = 0.27757928\n",
      "Iteration 259, loss = 0.27701392\n",
      "Iteration 260, loss = 0.27645014\n",
      "Iteration 261, loss = 0.27588792\n",
      "Iteration 262, loss = 0.27532725\n",
      "Iteration 263, loss = 0.27476812\n",
      "Iteration 264, loss = 0.27421053\n",
      "Iteration 265, loss = 0.27365447\n",
      "Iteration 266, loss = 0.27309992\n",
      "Iteration 267, loss = 0.27254688\n",
      "Iteration 268, loss = 0.27199534\n",
      "Iteration 269, loss = 0.27144529\n",
      "Iteration 270, loss = 0.27089673\n",
      "Iteration 271, loss = 0.27034964\n",
      "Iteration 272, loss = 0.26980402\n",
      "Iteration 273, loss = 0.26925987\n",
      "Iteration 274, loss = 0.26871717\n",
      "Iteration 275, loss = 0.26817592\n",
      "Iteration 276, loss = 0.26763610\n",
      "Iteration 277, loss = 0.26709773\n",
      "Iteration 278, loss = 0.26656077\n",
      "Iteration 279, loss = 0.26602524\n",
      "Iteration 280, loss = 0.26549113\n",
      "Iteration 281, loss = 0.26495842\n",
      "Iteration 282, loss = 0.26442711\n",
      "Iteration 283, loss = 0.26389720\n",
      "Iteration 284, loss = 0.26336868\n",
      "Iteration 285, loss = 0.26284155\n",
      "Iteration 286, loss = 0.26231579\n",
      "Iteration 287, loss = 0.26179140\n",
      "Iteration 288, loss = 0.26126839\n",
      "Iteration 289, loss = 0.26074674\n",
      "Iteration 290, loss = 0.26022644\n",
      "Iteration 291, loss = 0.25970750\n",
      "Iteration 292, loss = 0.25918990\n",
      "Iteration 293, loss = 0.25867365\n",
      "Iteration 294, loss = 0.25815874\n",
      "Iteration 295, loss = 0.25764516\n",
      "Iteration 296, loss = 0.25713291\n",
      "Iteration 297, loss = 0.25662199\n",
      "Iteration 298, loss = 0.25611238\n",
      "Iteration 299, loss = 0.25560410\n",
      "Iteration 300, loss = 0.25509712\n",
      "Iteration 301, loss = 0.25459146\n",
      "Iteration 302, loss = 0.25408710\n",
      "Iteration 303, loss = 0.25358404\n",
      "Iteration 304, loss = 0.25308228\n",
      "Iteration 305, loss = 0.25258181\n",
      "Iteration 306, loss = 0.25208263\n",
      "Iteration 307, loss = 0.25158474\n",
      "Iteration 308, loss = 0.25108814\n",
      "Iteration 309, loss = 0.25059281\n",
      "Iteration 310, loss = 0.25009876\n",
      "Iteration 311, loss = 0.24960598\n",
      "Iteration 312, loss = 0.24911448\n",
      "Iteration 313, loss = 0.24862424\n",
      "Iteration 314, loss = 0.24813527\n",
      "Iteration 315, loss = 0.24764756\n",
      "Iteration 316, loss = 0.24716111\n",
      "Iteration 317, loss = 0.24667592\n",
      "Iteration 318, loss = 0.24619198\n",
      "Iteration 319, loss = 0.24570930\n",
      "Iteration 320, loss = 0.24522786\n",
      "Iteration 321, loss = 0.24474767\n",
      "Iteration 322, loss = 0.24426872\n",
      "Iteration 323, loss = 0.24379102\n",
      "Iteration 324, loss = 0.24331456\n",
      "Iteration 325, loss = 0.24283933\n",
      "Iteration 326, loss = 0.24236534\n",
      "Iteration 327, loss = 0.24189259\n",
      "Iteration 328, loss = 0.24142106\n",
      "Iteration 329, loss = 0.24095077\n",
      "Iteration 330, loss = 0.24048170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 331, loss = 0.24001386\n",
      "Iteration 332, loss = 0.23954724\n",
      "Iteration 333, loss = 0.23908185\n",
      "Iteration 334, loss = 0.23861768\n",
      "Iteration 335, loss = 0.23815472\n",
      "Iteration 336, loss = 0.23769299\n",
      "Iteration 337, loss = 0.23723246\n",
      "Iteration 338, loss = 0.23677316\n",
      "Iteration 339, loss = 0.23631506\n",
      "Iteration 340, loss = 0.23585818\n",
      "Iteration 341, loss = 0.23540251\n",
      "Iteration 342, loss = 0.23494804\n",
      "Iteration 343, loss = 0.23449478\n",
      "Iteration 344, loss = 0.23404273\n",
      "Iteration 345, loss = 0.23359188\n",
      "Iteration 346, loss = 0.23314224\n",
      "Iteration 347, loss = 0.23269380\n",
      "Iteration 348, loss = 0.23224655\n",
      "Iteration 349, loss = 0.23180051\n",
      "Iteration 350, loss = 0.23135567\n",
      "Iteration 351, loss = 0.23091202\n",
      "Iteration 352, loss = 0.23046957\n",
      "Iteration 353, loss = 0.23002832\n",
      "Iteration 354, loss = 0.22958825\n",
      "Iteration 355, loss = 0.22914939\n",
      "Iteration 356, loss = 0.22871171\n",
      "Iteration 357, loss = 0.22827523\n",
      "Iteration 358, loss = 0.22783993\n",
      "Iteration 359, loss = 0.22740583\n",
      "Iteration 360, loss = 0.22697291\n",
      "Iteration 361, loss = 0.22654118\n",
      "Iteration 362, loss = 0.22611064\n",
      "Iteration 363, loss = 0.22568128\n",
      "Iteration 364, loss = 0.22525311\n",
      "Iteration 365, loss = 0.22482612\n",
      "Iteration 366, loss = 0.22440031\n",
      "Iteration 367, loss = 0.22397569\n",
      "Iteration 368, loss = 0.22355225\n",
      "Iteration 369, loss = 0.22312999\n",
      "Iteration 370, loss = 0.22270891\n",
      "Iteration 371, loss = 0.22228901\n",
      "Iteration 372, loss = 0.22187029\n",
      "Iteration 373, loss = 0.22145274\n",
      "Iteration 374, loss = 0.22103637\n",
      "Iteration 375, loss = 0.22062118\n",
      "Iteration 376, loss = 0.22020717\n",
      "Iteration 377, loss = 0.21979433\n",
      "Iteration 378, loss = 0.21938266\n",
      "Iteration 379, loss = 0.21897217\n",
      "Iteration 380, loss = 0.21856285\n",
      "Iteration 381, loss = 0.21815470\n",
      "Iteration 382, loss = 0.21774772\n",
      "Iteration 383, loss = 0.21734192\n",
      "Iteration 384, loss = 0.21693728\n",
      "Iteration 385, loss = 0.21653382\n",
      "Iteration 386, loss = 0.21613152\n",
      "Iteration 387, loss = 0.21573039\n",
      "Iteration 388, loss = 0.21533042\n",
      "Iteration 389, loss = 0.21493163\n",
      "Iteration 390, loss = 0.21453400\n",
      "Iteration 391, loss = 0.21413753\n",
      "Iteration 392, loss = 0.21374223\n",
      "Iteration 393, loss = 0.21334809\n",
      "Iteration 394, loss = 0.21295512\n",
      "Iteration 395, loss = 0.21256331\n",
      "Iteration 396, loss = 0.21217266\n",
      "Iteration 397, loss = 0.21178317\n",
      "Iteration 398, loss = 0.21139484\n",
      "Iteration 399, loss = 0.21100767\n",
      "Iteration 400, loss = 0.21062166\n",
      "Iteration 401, loss = 0.21023680\n",
      "Iteration 402, loss = 0.20985310\n",
      "Iteration 403, loss = 0.20947056\n",
      "Iteration 404, loss = 0.20908918\n",
      "Iteration 405, loss = 0.20870895\n",
      "Iteration 406, loss = 0.20832987\n",
      "Iteration 407, loss = 0.20795195\n",
      "Iteration 408, loss = 0.20757518\n",
      "Iteration 409, loss = 0.20719955\n",
      "Iteration 410, loss = 0.20682508\n",
      "Iteration 411, loss = 0.20645176\n",
      "Iteration 412, loss = 0.20607959\n",
      "Iteration 413, loss = 0.20570857\n",
      "Iteration 414, loss = 0.20533869\n",
      "Iteration 415, loss = 0.20496996\n",
      "Iteration 416, loss = 0.20460237\n",
      "Iteration 417, loss = 0.20423593\n",
      "Iteration 418, loss = 0.20387063\n",
      "Iteration 419, loss = 0.20350648\n",
      "Iteration 420, loss = 0.20314346\n",
      "Iteration 421, loss = 0.20278159\n",
      "Iteration 422, loss = 0.20242085\n",
      "Iteration 423, loss = 0.20206126\n",
      "Iteration 424, loss = 0.20170279\n",
      "Iteration 425, loss = 0.20134547\n",
      "Iteration 426, loss = 0.20098928\n",
      "Iteration 427, loss = 0.20063423\n",
      "Iteration 428, loss = 0.20028030\n",
      "Iteration 429, loss = 0.19992751\n",
      "Iteration 430, loss = 0.19957585\n",
      "Iteration 431, loss = 0.19922532\n",
      "Iteration 432, loss = 0.19887592\n",
      "Iteration 433, loss = 0.19852764\n",
      "Iteration 434, loss = 0.19818049\n",
      "Iteration 435, loss = 0.19783446\n",
      "Iteration 436, loss = 0.19748956\n",
      "Iteration 437, loss = 0.19714578\n",
      "Iteration 438, loss = 0.19680312\n",
      "Iteration 439, loss = 0.19646158\n",
      "Iteration 440, loss = 0.19612116\n",
      "Iteration 441, loss = 0.19578185\n",
      "Iteration 442, loss = 0.19544366\n",
      "Iteration 443, loss = 0.19510658\n",
      "Iteration 444, loss = 0.19477062\n",
      "Iteration 445, loss = 0.19443576\n",
      "Iteration 446, loss = 0.19410202\n",
      "Iteration 447, loss = 0.19376939\n",
      "Iteration 448, loss = 0.19343786\n",
      "Iteration 449, loss = 0.19310743\n",
      "Iteration 450, loss = 0.19277811\n",
      "Iteration 451, loss = 0.19244990\n",
      "Iteration 452, loss = 0.19212278\n",
      "Iteration 453, loss = 0.19179676\n",
      "Iteration 454, loss = 0.19147184\n",
      "Iteration 455, loss = 0.19114802\n",
      "Iteration 456, loss = 0.19082529\n",
      "Iteration 457, loss = 0.19050365\n",
      "Iteration 458, loss = 0.19018311\n",
      "Iteration 459, loss = 0.18986365\n",
      "Iteration 460, loss = 0.18954528\n",
      "Iteration 461, loss = 0.18922800\n",
      "Iteration 462, loss = 0.18891180\n",
      "Iteration 463, loss = 0.18859668\n",
      "Iteration 464, loss = 0.18828265\n",
      "Iteration 465, loss = 0.18796969\n",
      "Iteration 466, loss = 0.18765781\n",
      "Iteration 467, loss = 0.18734700\n",
      "Iteration 468, loss = 0.18703727\n",
      "Iteration 469, loss = 0.18672861\n",
      "Iteration 470, loss = 0.18642102\n",
      "Iteration 471, loss = 0.18611449\n",
      "Iteration 472, loss = 0.18580904\n",
      "Iteration 473, loss = 0.18550464\n",
      "Iteration 474, loss = 0.18520131\n",
      "Iteration 475, loss = 0.18489904\n",
      "Iteration 476, loss = 0.18459782\n",
      "Iteration 477, loss = 0.18429767\n",
      "Iteration 478, loss = 0.18399856\n",
      "Iteration 479, loss = 0.18370051\n",
      "Iteration 480, loss = 0.18340350\n",
      "Iteration 481, loss = 0.18310755\n",
      "Iteration 482, loss = 0.18281264\n",
      "Iteration 483, loss = 0.18251877\n",
      "Iteration 484, loss = 0.18222594\n",
      "Iteration 485, loss = 0.18193416\n",
      "Iteration 486, loss = 0.18164341\n",
      "Iteration 487, loss = 0.18135369\n",
      "Iteration 488, loss = 0.18106501\n",
      "Iteration 489, loss = 0.18077736\n",
      "Iteration 490, loss = 0.18049073\n",
      "Iteration 491, loss = 0.18020513\n",
      "Iteration 492, loss = 0.17992056\n",
      "Iteration 493, loss = 0.17963700\n",
      "Iteration 494, loss = 0.17935447\n",
      "Iteration 495, loss = 0.17907295\n",
      "Iteration 496, loss = 0.17879245\n",
      "Iteration 497, loss = 0.17851295\n",
      "Iteration 498, loss = 0.17823447\n",
      "Iteration 499, loss = 0.17795700\n",
      "Iteration 500, loss = 0.17768053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.42454703\n",
      "Iteration 2, loss = 1.40922918\n",
      "Iteration 3, loss = 1.38778403\n",
      "Iteration 4, loss = 1.36122525\n",
      "Iteration 5, loss = 1.33052932\n",
      "Iteration 6, loss = 1.29661791\n",
      "Iteration 7, loss = 1.26034537\n",
      "Iteration 8, loss = 1.22249030\n",
      "Iteration 9, loss = 1.18375042\n",
      "Iteration 10, loss = 1.14474005\n",
      "Iteration 11, loss = 1.10598979\n",
      "Iteration 12, loss = 1.06794779\n",
      "Iteration 13, loss = 1.03098255\n",
      "Iteration 14, loss = 0.99538688\n",
      "Iteration 15, loss = 0.96138288\n",
      "Iteration 16, loss = 0.92912789\n",
      "Iteration 17, loss = 0.89872106\n",
      "Iteration 18, loss = 0.87021057\n",
      "Iteration 19, loss = 0.84360097\n",
      "Iteration 20, loss = 0.81886078\n",
      "Iteration 21, loss = 0.79592968\n",
      "Iteration 22, loss = 0.77472543\n",
      "Iteration 23, loss = 0.75515006\n",
      "Iteration 24, loss = 0.73709548\n",
      "Iteration 25, loss = 0.72044811\n",
      "Iteration 26, loss = 0.70509283\n",
      "Iteration 27, loss = 0.69091604\n",
      "Iteration 28, loss = 0.67780806\n",
      "Iteration 29, loss = 0.66566483\n",
      "Iteration 30, loss = 0.65438905\n",
      "Iteration 31, loss = 0.64389091\n",
      "Iteration 32, loss = 0.63408836\n",
      "Iteration 33, loss = 0.62490717\n",
      "Iteration 34, loss = 0.61628072\n",
      "Iteration 35, loss = 0.60814968\n",
      "Iteration 36, loss = 0.60046151\n",
      "Iteration 37, loss = 0.59316997\n",
      "Iteration 38, loss = 0.58623456\n",
      "Iteration 39, loss = 0.57961998\n",
      "Iteration 40, loss = 0.57329552\n",
      "Iteration 41, loss = 0.56723459\n",
      "Iteration 42, loss = 0.56141420\n",
      "Iteration 43, loss = 0.55581446\n",
      "Iteration 44, loss = 0.55041822\n",
      "Iteration 45, loss = 0.54521064\n",
      "Iteration 46, loss = 0.54017884\n",
      "Iteration 47, loss = 0.53531166\n",
      "Iteration 48, loss = 0.53059931\n",
      "Iteration 49, loss = 0.52603320\n",
      "Iteration 50, loss = 0.52160569\n",
      "Iteration 51, loss = 0.51730999\n",
      "Iteration 52, loss = 0.51313993\n",
      "Iteration 53, loss = 0.50908991\n",
      "Iteration 54, loss = 0.50515477\n",
      "Iteration 55, loss = 0.50132972\n",
      "Iteration 56, loss = 0.49761025\n",
      "Iteration 57, loss = 0.49399211\n",
      "Iteration 58, loss = 0.49047126\n",
      "Iteration 59, loss = 0.48704383\n",
      "Iteration 60, loss = 0.48370613\n",
      "Iteration 61, loss = 0.48045457\n",
      "Iteration 62, loss = 0.47728572\n",
      "Iteration 63, loss = 0.47419625\n",
      "Iteration 64, loss = 0.47118297\n",
      "Iteration 65, loss = 0.46824279\n",
      "Iteration 66, loss = 0.46537273\n",
      "Iteration 67, loss = 0.46256994\n",
      "Iteration 68, loss = 0.45983166\n",
      "Iteration 69, loss = 0.45715526\n",
      "Iteration 70, loss = 0.45453823\n",
      "Iteration 71, loss = 0.45197815\n",
      "Iteration 72, loss = 0.44947274\n",
      "Iteration 73, loss = 0.44701980\n",
      "Iteration 74, loss = 0.44461726\n",
      "Iteration 75, loss = 0.44226316\n",
      "Iteration 76, loss = 0.43995561\n",
      "Iteration 77, loss = 0.43769286\n",
      "Iteration 78, loss = 0.43547322\n",
      "Iteration 79, loss = 0.43329510\n",
      "Iteration 80, loss = 0.43115702\n",
      "Iteration 81, loss = 0.42905754\n",
      "Iteration 82, loss = 0.42699533\n",
      "Iteration 83, loss = 0.42496910\n",
      "Iteration 84, loss = 0.42297766\n",
      "Iteration 85, loss = 0.42101985\n",
      "Iteration 86, loss = 0.41909460\n",
      "Iteration 87, loss = 0.41720087\n",
      "Iteration 88, loss = 0.41533769\n",
      "Iteration 89, loss = 0.41350410\n",
      "Iteration 90, loss = 0.41169924\n",
      "Iteration 91, loss = 0.40992223\n",
      "Iteration 92, loss = 0.40817228\n",
      "Iteration 93, loss = 0.40644860\n",
      "Iteration 94, loss = 0.40475045\n",
      "Iteration 95, loss = 0.40307710\n",
      "Iteration 96, loss = 0.40142788\n",
      "Iteration 97, loss = 0.39980213\n",
      "Iteration 98, loss = 0.39819921\n",
      "Iteration 99, loss = 0.39661851\n",
      "Iteration 100, loss = 0.39505945\n",
      "Iteration 101, loss = 0.39352145\n",
      "Iteration 102, loss = 0.39200398\n",
      "Iteration 103, loss = 0.39050650\n",
      "Iteration 104, loss = 0.38902852\n",
      "Iteration 105, loss = 0.38756954\n",
      "Iteration 106, loss = 0.38612909\n",
      "Iteration 107, loss = 0.38470671\n",
      "Iteration 108, loss = 0.38330197\n",
      "Iteration 109, loss = 0.38191444\n",
      "Iteration 110, loss = 0.38054371\n",
      "Iteration 111, loss = 0.37918938\n",
      "Iteration 112, loss = 0.37785106\n",
      "Iteration 113, loss = 0.37652840\n",
      "Iteration 114, loss = 0.37522102\n",
      "Iteration 115, loss = 0.37392859\n",
      "Iteration 116, loss = 0.37265076\n",
      "Iteration 117, loss = 0.37138722\n",
      "Iteration 118, loss = 0.37013764\n",
      "Iteration 119, loss = 0.36890172\n",
      "Iteration 120, loss = 0.36767918\n",
      "Iteration 121, loss = 0.36646971\n",
      "Iteration 122, loss = 0.36527305\n",
      "Iteration 123, loss = 0.36408892\n",
      "Iteration 124, loss = 0.36291707\n",
      "Iteration 125, loss = 0.36175724\n",
      "Iteration 126, loss = 0.36060919\n",
      "Iteration 127, loss = 0.35947267\n",
      "Iteration 128, loss = 0.35834746\n",
      "Iteration 129, loss = 0.35723332\n",
      "Iteration 130, loss = 0.35613005\n",
      "Iteration 131, loss = 0.35503743\n",
      "Iteration 132, loss = 0.35395524\n",
      "Iteration 133, loss = 0.35288330\n",
      "Iteration 134, loss = 0.35182140\n",
      "Iteration 135, loss = 0.35076935\n",
      "Iteration 136, loss = 0.34972697\n",
      "Iteration 137, loss = 0.34869407\n",
      "Iteration 138, loss = 0.34767048\n",
      "Iteration 139, loss = 0.34665604\n",
      "Iteration 140, loss = 0.34565056\n",
      "Iteration 141, loss = 0.34465389\n",
      "Iteration 142, loss = 0.34366587\n",
      "Iteration 143, loss = 0.34268635\n",
      "Iteration 144, loss = 0.34171517\n",
      "Iteration 145, loss = 0.34075218\n",
      "Iteration 146, loss = 0.33979725\n",
      "Iteration 147, loss = 0.33885023\n",
      "Iteration 148, loss = 0.33791098\n",
      "Iteration 149, loss = 0.33697937\n",
      "Iteration 150, loss = 0.33605528\n",
      "Iteration 151, loss = 0.33513856\n",
      "Iteration 152, loss = 0.33422911\n",
      "Iteration 153, loss = 0.33332679\n",
      "Iteration 154, loss = 0.33243149\n",
      "Iteration 155, loss = 0.33154310\n",
      "Iteration 156, loss = 0.33066149\n",
      "Iteration 157, loss = 0.32978656\n",
      "Iteration 158, loss = 0.32891821\n",
      "Iteration 159, loss = 0.32805631\n",
      "Iteration 160, loss = 0.32720078\n",
      "Iteration 161, loss = 0.32635150\n",
      "Iteration 162, loss = 0.32550839\n",
      "Iteration 163, loss = 0.32467134\n",
      "Iteration 164, loss = 0.32384026\n",
      "Iteration 165, loss = 0.32301506\n",
      "Iteration 166, loss = 0.32219564\n",
      "Iteration 167, loss = 0.32138191\n",
      "Iteration 168, loss = 0.32057380\n",
      "Iteration 169, loss = 0.31977121\n",
      "Iteration 170, loss = 0.31897406\n",
      "Iteration 171, loss = 0.31818227\n",
      "Iteration 172, loss = 0.31739577\n",
      "Iteration 173, loss = 0.31661446\n",
      "Iteration 174, loss = 0.31583828\n",
      "Iteration 175, loss = 0.31506714\n",
      "Iteration 176, loss = 0.31430099\n",
      "Iteration 177, loss = 0.31353974\n",
      "Iteration 178, loss = 0.31278332\n",
      "Iteration 179, loss = 0.31203166\n",
      "Iteration 180, loss = 0.31128470\n",
      "Iteration 181, loss = 0.31054237\n",
      "Iteration 182, loss = 0.30980461\n",
      "Iteration 183, loss = 0.30907135\n",
      "Iteration 184, loss = 0.30834252\n",
      "Iteration 185, loss = 0.30761808\n",
      "Iteration 186, loss = 0.30689794\n",
      "Iteration 187, loss = 0.30618207\n",
      "Iteration 188, loss = 0.30547039\n",
      "Iteration 189, loss = 0.30476286\n",
      "Iteration 190, loss = 0.30405941\n",
      "Iteration 191, loss = 0.30336000\n",
      "Iteration 192, loss = 0.30266456\n",
      "Iteration 193, loss = 0.30197305\n",
      "Iteration 194, loss = 0.30128542\n",
      "Iteration 195, loss = 0.30060160\n",
      "Iteration 196, loss = 0.29992156\n",
      "Iteration 197, loss = 0.29924525\n",
      "Iteration 198, loss = 0.29857261\n",
      "Iteration 199, loss = 0.29790360\n",
      "Iteration 200, loss = 0.29723817\n",
      "Iteration 201, loss = 0.29657628\n",
      "Iteration 202, loss = 0.29591789\n",
      "Iteration 203, loss = 0.29526294\n",
      "Iteration 204, loss = 0.29461140\n",
      "Iteration 205, loss = 0.29396322\n",
      "Iteration 206, loss = 0.29331837\n",
      "Iteration 207, loss = 0.29267680\n",
      "Iteration 208, loss = 0.29203847\n",
      "Iteration 209, loss = 0.29140334\n",
      "Iteration 210, loss = 0.29077138\n",
      "Iteration 211, loss = 0.29014254\n",
      "Iteration 212, loss = 0.28951680\n",
      "Iteration 213, loss = 0.28889410\n",
      "Iteration 214, loss = 0.28827442\n",
      "Iteration 215, loss = 0.28765773\n",
      "Iteration 216, loss = 0.28704398\n",
      "Iteration 217, loss = 0.28643314\n",
      "Iteration 218, loss = 0.28582518\n",
      "Iteration 219, loss = 0.28522006\n",
      "Iteration 220, loss = 0.28461776\n",
      "Iteration 221, loss = 0.28401825\n",
      "Iteration 222, loss = 0.28342148\n",
      "Iteration 223, loss = 0.28282743\n",
      "Iteration 224, loss = 0.28223607\n",
      "Iteration 225, loss = 0.28164737\n",
      "Iteration 226, loss = 0.28106130\n",
      "Iteration 227, loss = 0.28047783\n",
      "Iteration 228, loss = 0.27989694\n",
      "Iteration 229, loss = 0.27931859\n",
      "Iteration 230, loss = 0.27874276\n",
      "Iteration 231, loss = 0.27816942\n",
      "Iteration 232, loss = 0.27759854\n",
      "Iteration 233, loss = 0.27703011\n",
      "Iteration 234, loss = 0.27646409\n",
      "Iteration 235, loss = 0.27590046\n",
      "Iteration 236, loss = 0.27533919\n",
      "Iteration 237, loss = 0.27478026\n",
      "Iteration 238, loss = 0.27422364\n",
      "Iteration 239, loss = 0.27366932\n",
      "Iteration 240, loss = 0.27311727\n",
      "Iteration 241, loss = 0.27256746\n",
      "Iteration 242, loss = 0.27201988\n",
      "Iteration 243, loss = 0.27147450\n",
      "Iteration 244, loss = 0.27093129\n",
      "Iteration 245, loss = 0.27039025\n",
      "Iteration 246, loss = 0.26985135\n",
      "Iteration 247, loss = 0.26931456\n",
      "Iteration 248, loss = 0.26877987\n",
      "Iteration 249, loss = 0.26824726\n",
      "Iteration 250, loss = 0.26771671\n",
      "Iteration 251, loss = 0.26718819\n",
      "Iteration 252, loss = 0.26666169\n",
      "Iteration 253, loss = 0.26613719\n",
      "Iteration 254, loss = 0.26561468\n",
      "Iteration 255, loss = 0.26509413\n",
      "Iteration 256, loss = 0.26457553\n",
      "Iteration 257, loss = 0.26405885\n",
      "Iteration 258, loss = 0.26354409\n",
      "Iteration 259, loss = 0.26303122\n",
      "Iteration 260, loss = 0.26252023\n",
      "Iteration 261, loss = 0.26201110\n",
      "Iteration 262, loss = 0.26150381\n",
      "Iteration 263, loss = 0.26099836\n",
      "Iteration 264, loss = 0.26049472\n",
      "Iteration 265, loss = 0.25999287\n",
      "Iteration 266, loss = 0.25949281\n",
      "Iteration 267, loss = 0.25899452\n",
      "Iteration 268, loss = 0.25849798\n",
      "Iteration 269, loss = 0.25800319\n",
      "Iteration 270, loss = 0.25751011\n",
      "Iteration 271, loss = 0.25701875\n",
      "Iteration 272, loss = 0.25652908\n",
      "Iteration 273, loss = 0.25604110\n",
      "Iteration 274, loss = 0.25555479\n",
      "Iteration 275, loss = 0.25507013\n",
      "Iteration 276, loss = 0.25458712\n",
      "Iteration 277, loss = 0.25410574\n",
      "Iteration 278, loss = 0.25362598\n",
      "Iteration 279, loss = 0.25314783\n",
      "Iteration 280, loss = 0.25267127\n",
      "Iteration 281, loss = 0.25219629\n",
      "Iteration 282, loss = 0.25172289\n",
      "Iteration 283, loss = 0.25125104\n",
      "Iteration 284, loss = 0.25078074\n",
      "Iteration 285, loss = 0.25031198\n",
      "Iteration 286, loss = 0.24984474\n",
      "Iteration 287, loss = 0.24937901\n",
      "Iteration 288, loss = 0.24891480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 289, loss = 0.24845207\n",
      "Iteration 290, loss = 0.24799083\n",
      "Iteration 291, loss = 0.24753106\n",
      "Iteration 292, loss = 0.24707275\n",
      "Iteration 293, loss = 0.24661589\n",
      "Iteration 294, loss = 0.24616048\n",
      "Iteration 295, loss = 0.24570650\n",
      "Iteration 296, loss = 0.24525395\n",
      "Iteration 297, loss = 0.24480281\n",
      "Iteration 298, loss = 0.24435308\n",
      "Iteration 299, loss = 0.24390474\n",
      "Iteration 300, loss = 0.24345779\n",
      "Iteration 301, loss = 0.24301222\n",
      "Iteration 302, loss = 0.24256802\n",
      "Iteration 303, loss = 0.24212518\n",
      "Iteration 304, loss = 0.24168370\n",
      "Iteration 305, loss = 0.24124356\n",
      "Iteration 306, loss = 0.24080476\n",
      "Iteration 307, loss = 0.24036729\n",
      "Iteration 308, loss = 0.23993113\n",
      "Iteration 309, loss = 0.23949630\n",
      "Iteration 310, loss = 0.23906277\n",
      "Iteration 311, loss = 0.23863054\n",
      "Iteration 312, loss = 0.23819960\n",
      "Iteration 313, loss = 0.23776994\n",
      "Iteration 314, loss = 0.23734156\n",
      "Iteration 315, loss = 0.23691445\n",
      "Iteration 316, loss = 0.23648861\n",
      "Iteration 317, loss = 0.23606402\n",
      "Iteration 318, loss = 0.23564068\n",
      "Iteration 319, loss = 0.23521858\n",
      "Iteration 320, loss = 0.23479772\n",
      "Iteration 321, loss = 0.23437809\n",
      "Iteration 322, loss = 0.23395969\n",
      "Iteration 323, loss = 0.23354250\n",
      "Iteration 324, loss = 0.23312653\n",
      "Iteration 325, loss = 0.23271176\n",
      "Iteration 326, loss = 0.23229818\n",
      "Iteration 327, loss = 0.23188581\n",
      "Iteration 328, loss = 0.23147462\n",
      "Iteration 329, loss = 0.23106461\n",
      "Iteration 330, loss = 0.23065578\n",
      "Iteration 331, loss = 0.23024813\n",
      "Iteration 332, loss = 0.22984163\n",
      "Iteration 333, loss = 0.22943630\n",
      "Iteration 334, loss = 0.22903213\n",
      "Iteration 335, loss = 0.22862910\n",
      "Iteration 336, loss = 0.22822722\n",
      "Iteration 337, loss = 0.22782648\n",
      "Iteration 338, loss = 0.22742688\n",
      "Iteration 339, loss = 0.22702840\n",
      "Iteration 340, loss = 0.22663106\n",
      "Iteration 341, loss = 0.22623483\n",
      "Iteration 342, loss = 0.22583972\n",
      "Iteration 343, loss = 0.22544572\n",
      "Iteration 344, loss = 0.22505283\n",
      "Iteration 345, loss = 0.22466104\n",
      "Iteration 346, loss = 0.22427035\n",
      "Iteration 347, loss = 0.22388075\n",
      "Iteration 348, loss = 0.22349225\n",
      "Iteration 349, loss = 0.22310483\n",
      "Iteration 350, loss = 0.22271849\n",
      "Iteration 351, loss = 0.22233323\n",
      "Iteration 352, loss = 0.22194904\n",
      "Iteration 353, loss = 0.22156592\n",
      "Iteration 354, loss = 0.22118387\n",
      "Iteration 355, loss = 0.22080288\n",
      "Iteration 356, loss = 0.22042295\n",
      "Iteration 357, loss = 0.22004408\n",
      "Iteration 358, loss = 0.21966625\n",
      "Iteration 359, loss = 0.21928947\n",
      "Iteration 360, loss = 0.21891374\n",
      "Iteration 361, loss = 0.21853904\n",
      "Iteration 362, loss = 0.21816539\n",
      "Iteration 363, loss = 0.21779276\n",
      "Iteration 364, loss = 0.21742117\n",
      "Iteration 365, loss = 0.21705060\n",
      "Iteration 366, loss = 0.21668106\n",
      "Iteration 367, loss = 0.21631254\n",
      "Iteration 368, loss = 0.21594503\n",
      "Iteration 369, loss = 0.21557854\n",
      "Iteration 370, loss = 0.21521306\n",
      "Iteration 371, loss = 0.21484858\n",
      "Iteration 372, loss = 0.21448512\n",
      "Iteration 373, loss = 0.21412265\n",
      "Iteration 374, loss = 0.21376118\n",
      "Iteration 375, loss = 0.21340071\n",
      "Iteration 376, loss = 0.21304124\n",
      "Iteration 377, loss = 0.21268275\n",
      "Iteration 378, loss = 0.21232525\n",
      "Iteration 379, loss = 0.21196874\n",
      "Iteration 380, loss = 0.21161321\n",
      "Iteration 381, loss = 0.21125865\n",
      "Iteration 382, loss = 0.21090508\n",
      "Iteration 383, loss = 0.21055248\n",
      "Iteration 384, loss = 0.21020085\n",
      "Iteration 385, loss = 0.20985019\n",
      "Iteration 386, loss = 0.20950050\n",
      "Iteration 387, loss = 0.20915177\n",
      "Iteration 388, loss = 0.20880401\n",
      "Iteration 389, loss = 0.20845720\n",
      "Iteration 390, loss = 0.20811135\n",
      "Iteration 391, loss = 0.20776646\n",
      "Iteration 392, loss = 0.20742252\n",
      "Iteration 393, loss = 0.20707953\n",
      "Iteration 394, loss = 0.20673749\n",
      "Iteration 395, loss = 0.20639639\n",
      "Iteration 396, loss = 0.20605624\n",
      "Iteration 397, loss = 0.20571703\n",
      "Iteration 398, loss = 0.20537875\n",
      "Iteration 399, loss = 0.20504142\n",
      "Iteration 400, loss = 0.20470502\n",
      "Iteration 401, loss = 0.20436955\n",
      "Iteration 402, loss = 0.20403501\n",
      "Iteration 403, loss = 0.20370141\n",
      "Iteration 404, loss = 0.20336873\n",
      "Iteration 405, loss = 0.20303697\n",
      "Iteration 406, loss = 0.20270614\n",
      "Iteration 407, loss = 0.20237622\n",
      "Iteration 408, loss = 0.20204723\n",
      "Iteration 409, loss = 0.20171915\n",
      "Iteration 410, loss = 0.20139199\n",
      "Iteration 411, loss = 0.20106574\n",
      "Iteration 412, loss = 0.20074040\n",
      "Iteration 413, loss = 0.20041598\n",
      "Iteration 414, loss = 0.20009246\n",
      "Iteration 415, loss = 0.19976984\n",
      "Iteration 416, loss = 0.19944813\n",
      "Iteration 417, loss = 0.19912732\n",
      "Iteration 418, loss = 0.19880741\n",
      "Iteration 419, loss = 0.19848840\n",
      "Iteration 420, loss = 0.19817029\n",
      "Iteration 421, loss = 0.19785307\n",
      "Iteration 422, loss = 0.19753675\n",
      "Iteration 423, loss = 0.19722131\n",
      "Iteration 424, loss = 0.19690677\n",
      "Iteration 425, loss = 0.19659311\n",
      "Iteration 426, loss = 0.19628034\n",
      "Iteration 427, loss = 0.19596846\n",
      "Iteration 428, loss = 0.19565746\n",
      "Iteration 429, loss = 0.19534734\n",
      "Iteration 430, loss = 0.19503810\n",
      "Iteration 431, loss = 0.19472974\n",
      "Iteration 432, loss = 0.19442226\n",
      "Iteration 433, loss = 0.19411565\n",
      "Iteration 434, loss = 0.19380991\n",
      "Iteration 435, loss = 0.19350505\n",
      "Iteration 436, loss = 0.19320105\n",
      "Iteration 437, loss = 0.19289793\n",
      "Iteration 438, loss = 0.19259567\n",
      "Iteration 439, loss = 0.19229427\n",
      "Iteration 440, loss = 0.19199375\n",
      "Iteration 441, loss = 0.19169408\n",
      "Iteration 442, loss = 0.19139527\n",
      "Iteration 443, loss = 0.19109733\n",
      "Iteration 444, loss = 0.19080024\n",
      "Iteration 445, loss = 0.19050401\n",
      "Iteration 446, loss = 0.19020863\n",
      "Iteration 447, loss = 0.18991411\n",
      "Iteration 448, loss = 0.18962043\n",
      "Iteration 449, loss = 0.18932761\n",
      "Iteration 450, loss = 0.18903564\n",
      "Iteration 451, loss = 0.18874451\n",
      "Iteration 452, loss = 0.18845423\n",
      "Iteration 453, loss = 0.18816479\n",
      "Iteration 454, loss = 0.18787620\n",
      "Iteration 455, loss = 0.18758845\n",
      "Iteration 456, loss = 0.18730153\n",
      "Iteration 457, loss = 0.18701546\n",
      "Iteration 458, loss = 0.18673022\n",
      "Iteration 459, loss = 0.18644582\n",
      "Iteration 460, loss = 0.18616225\n",
      "Iteration 461, loss = 0.18587952\n",
      "Iteration 462, loss = 0.18559761\n",
      "Iteration 463, loss = 0.18531654\n",
      "Iteration 464, loss = 0.18503629\n",
      "Iteration 465, loss = 0.18475687\n",
      "Iteration 466, loss = 0.18447827\n",
      "Iteration 467, loss = 0.18420050\n",
      "Iteration 468, loss = 0.18392355\n",
      "Iteration 469, loss = 0.18364742\n",
      "Iteration 470, loss = 0.18337211\n",
      "Iteration 471, loss = 0.18309762\n",
      "Iteration 472, loss = 0.18282394\n",
      "Iteration 473, loss = 0.18255108\n",
      "Iteration 474, loss = 0.18227903\n",
      "Iteration 475, loss = 0.18200779\n",
      "Iteration 476, loss = 0.18173737\n",
      "Iteration 477, loss = 0.18146775\n",
      "Iteration 478, loss = 0.18119894\n",
      "Iteration 479, loss = 0.18093094\n",
      "Iteration 480, loss = 0.18066374\n",
      "Iteration 481, loss = 0.18039734\n",
      "Iteration 482, loss = 0.18013175\n",
      "Iteration 483, loss = 0.17986695\n",
      "Iteration 484, loss = 0.17960296\n",
      "Iteration 485, loss = 0.17933976\n",
      "Iteration 486, loss = 0.17907736\n",
      "Iteration 487, loss = 0.17881575\n",
      "Iteration 488, loss = 0.17855493\n",
      "Iteration 489, loss = 0.17829491\n",
      "Iteration 490, loss = 0.17803567\n",
      "Iteration 491, loss = 0.17777723\n",
      "Iteration 492, loss = 0.17751957\n",
      "Iteration 493, loss = 0.17726269\n",
      "Iteration 494, loss = 0.17700660\n",
      "Iteration 495, loss = 0.17675130\n",
      "Iteration 496, loss = 0.17649677\n",
      "Iteration 497, loss = 0.17624302\n",
      "Iteration 498, loss = 0.17599005\n",
      "Iteration 499, loss = 0.17573786\n",
      "Iteration 500, loss = 0.17548644\n",
      "Iteration 1, loss = 1.43752901\n",
      "Iteration 2, loss = 1.42088803\n",
      "Iteration 3, loss = 1.39762838\n",
      "Iteration 4, loss = 1.36888334\n",
      "Iteration 5, loss = 1.33574599\n",
      "Iteration 6, loss = 1.29924726\n",
      "Iteration 7, loss = 1.26034038\n",
      "Iteration 8, loss = 1.21989048\n",
      "Iteration 9, loss = 1.17866854\n",
      "Iteration 10, loss = 1.13734857\n",
      "Iteration 11, loss = 1.09650752\n",
      "Iteration 12, loss = 1.05662728\n",
      "Iteration 13, loss = 1.01809836\n",
      "Iteration 14, loss = 0.98122494\n",
      "Iteration 15, loss = 0.94623112\n",
      "Iteration 16, loss = 0.91326810\n",
      "Iteration 17, loss = 0.88242217\n",
      "Iteration 18, loss = 0.85372325\n",
      "Iteration 19, loss = 0.82715371\n",
      "Iteration 20, loss = 0.80265718\n",
      "Iteration 21, loss = 0.78014710\n",
      "Iteration 22, loss = 0.75951466\n",
      "Iteration 23, loss = 0.74063595\n",
      "Iteration 24, loss = 0.72337825\n",
      "Iteration 25, loss = 0.70760524\n",
      "Iteration 26, loss = 0.69318125\n",
      "Iteration 27, loss = 0.67997456\n",
      "Iteration 28, loss = 0.66785981\n",
      "Iteration 29, loss = 0.65671964\n",
      "Iteration 30, loss = 0.64644574\n",
      "Iteration 31, loss = 0.63693936\n",
      "Iteration 32, loss = 0.62811142\n",
      "Iteration 33, loss = 0.61988230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.61218143\n",
      "Iteration 35, loss = 0.60494676\n",
      "Iteration 36, loss = 0.59812406\n",
      "Iteration 37, loss = 0.59166624\n",
      "Iteration 38, loss = 0.58553263\n",
      "Iteration 39, loss = 0.57968827\n",
      "Iteration 40, loss = 0.57410325\n",
      "Iteration 41, loss = 0.56875203\n",
      "Iteration 42, loss = 0.56361291\n",
      "Iteration 43, loss = 0.55866740\n",
      "Iteration 44, loss = 0.55389984\n",
      "Iteration 45, loss = 0.54929688\n",
      "Iteration 46, loss = 0.54484712\n",
      "Iteration 47, loss = 0.54054080\n",
      "Iteration 48, loss = 0.53636950\n",
      "Iteration 49, loss = 0.53232586\n",
      "Iteration 50, loss = 0.52840341\n",
      "Iteration 51, loss = 0.52459639\n",
      "Iteration 52, loss = 0.52089959\n",
      "Iteration 53, loss = 0.51730823\n",
      "Iteration 54, loss = 0.51381790\n",
      "Iteration 55, loss = 0.51042445\n",
      "Iteration 56, loss = 0.50712394\n",
      "Iteration 57, loss = 0.50391263\n",
      "Iteration 58, loss = 0.50078689\n",
      "Iteration 59, loss = 0.49774325\n",
      "Iteration 60, loss = 0.49477833\n",
      "Iteration 61, loss = 0.49188886\n",
      "Iteration 62, loss = 0.48907168\n",
      "Iteration 63, loss = 0.48632371\n",
      "Iteration 64, loss = 0.48364200\n",
      "Iteration 65, loss = 0.48102368\n",
      "Iteration 66, loss = 0.47846601\n",
      "Iteration 67, loss = 0.47596634\n",
      "Iteration 68, loss = 0.47352216\n",
      "Iteration 69, loss = 0.47113105\n",
      "Iteration 70, loss = 0.46879073\n",
      "Iteration 71, loss = 0.46649900\n",
      "Iteration 72, loss = 0.46425382\n",
      "Iteration 73, loss = 0.46205322\n",
      "Iteration 74, loss = 0.45989538\n",
      "Iteration 75, loss = 0.45777854\n",
      "Iteration 76, loss = 0.45570106\n",
      "Iteration 77, loss = 0.45366142\n",
      "Iteration 78, loss = 0.45165814\n",
      "Iteration 79, loss = 0.44968986\n",
      "Iteration 80, loss = 0.44775530\n",
      "Iteration 81, loss = 0.44585323\n",
      "Iteration 82, loss = 0.44398249\n",
      "Iteration 83, loss = 0.44214202\n",
      "Iteration 84, loss = 0.44033076\n",
      "Iteration 85, loss = 0.43854776\n",
      "Iteration 86, loss = 0.43679207\n",
      "Iteration 87, loss = 0.43506282\n",
      "Iteration 88, loss = 0.43335917\n",
      "Iteration 89, loss = 0.43168032\n",
      "Iteration 90, loss = 0.43002550\n",
      "Iteration 91, loss = 0.42839397\n",
      "Iteration 92, loss = 0.42678504\n",
      "Iteration 93, loss = 0.42519804\n",
      "Iteration 94, loss = 0.42363231\n",
      "Iteration 95, loss = 0.42208725\n",
      "Iteration 96, loss = 0.42056225\n",
      "Iteration 97, loss = 0.41905674\n",
      "Iteration 98, loss = 0.41757018\n",
      "Iteration 99, loss = 0.41610203\n",
      "Iteration 100, loss = 0.41465180\n",
      "Iteration 101, loss = 0.41321898\n",
      "Iteration 102, loss = 0.41180311\n",
      "Iteration 103, loss = 0.41040374\n",
      "Iteration 104, loss = 0.40902044\n",
      "Iteration 105, loss = 0.40765277\n",
      "Iteration 106, loss = 0.40630035\n",
      "Iteration 107, loss = 0.40496278\n",
      "Iteration 108, loss = 0.40363969\n",
      "Iteration 109, loss = 0.40233072\n",
      "Iteration 110, loss = 0.40103552\n",
      "Iteration 111, loss = 0.39975375\n",
      "Iteration 112, loss = 0.39848510\n",
      "Iteration 113, loss = 0.39722926\n",
      "Iteration 114, loss = 0.39598592\n",
      "Iteration 115, loss = 0.39475478\n",
      "Iteration 116, loss = 0.39353559\n",
      "Iteration 117, loss = 0.39232805\n",
      "Iteration 118, loss = 0.39113192\n",
      "Iteration 119, loss = 0.38994693\n",
      "Iteration 120, loss = 0.38877285\n",
      "Iteration 121, loss = 0.38760943\n",
      "Iteration 122, loss = 0.38645644\n",
      "Iteration 123, loss = 0.38531367\n",
      "Iteration 124, loss = 0.38418089\n",
      "Iteration 125, loss = 0.38305790\n",
      "Iteration 126, loss = 0.38194449\n",
      "Iteration 127, loss = 0.38084046\n",
      "Iteration 128, loss = 0.37974563\n",
      "Iteration 129, loss = 0.37865980\n",
      "Iteration 130, loss = 0.37758280\n",
      "Iteration 131, loss = 0.37651445\n",
      "Iteration 132, loss = 0.37545458\n",
      "Iteration 133, loss = 0.37440302\n",
      "Iteration 134, loss = 0.37335960\n",
      "Iteration 135, loss = 0.37232418\n",
      "Iteration 136, loss = 0.37129660\n",
      "Iteration 137, loss = 0.37027671\n",
      "Iteration 138, loss = 0.36926437\n",
      "Iteration 139, loss = 0.36825943\n",
      "Iteration 140, loss = 0.36726175\n",
      "Iteration 141, loss = 0.36627120\n",
      "Iteration 142, loss = 0.36528766\n",
      "Iteration 143, loss = 0.36431099\n",
      "Iteration 144, loss = 0.36334107\n",
      "Iteration 145, loss = 0.36237778\n",
      "Iteration 146, loss = 0.36142100\n",
      "Iteration 147, loss = 0.36047061\n",
      "Iteration 148, loss = 0.35952651\n",
      "Iteration 149, loss = 0.35858859\n",
      "Iteration 150, loss = 0.35765673\n",
      "Iteration 151, loss = 0.35673084\n",
      "Iteration 152, loss = 0.35581082\n",
      "Iteration 153, loss = 0.35489655\n",
      "Iteration 154, loss = 0.35398796\n",
      "Iteration 155, loss = 0.35308494\n",
      "Iteration 156, loss = 0.35218740\n",
      "Iteration 157, loss = 0.35129525\n",
      "Iteration 158, loss = 0.35040841\n",
      "Iteration 159, loss = 0.34952679\n",
      "Iteration 160, loss = 0.34865030\n",
      "Iteration 161, loss = 0.34777887\n",
      "Iteration 162, loss = 0.34691241\n",
      "Iteration 163, loss = 0.34605084\n",
      "Iteration 164, loss = 0.34519409\n",
      "Iteration 165, loss = 0.34434209\n",
      "Iteration 166, loss = 0.34349476\n",
      "Iteration 167, loss = 0.34265204\n",
      "Iteration 168, loss = 0.34181384\n",
      "Iteration 169, loss = 0.34098011\n",
      "Iteration 170, loss = 0.34015077\n",
      "Iteration 171, loss = 0.33932576\n",
      "Iteration 172, loss = 0.33850503\n",
      "Iteration 173, loss = 0.33768850\n",
      "Iteration 174, loss = 0.33687611\n",
      "Iteration 175, loss = 0.33606781\n",
      "Iteration 176, loss = 0.33526354\n",
      "Iteration 177, loss = 0.33446324\n",
      "Iteration 178, loss = 0.33366685\n",
      "Iteration 179, loss = 0.33287432\n",
      "Iteration 180, loss = 0.33208560\n",
      "Iteration 181, loss = 0.33130064\n",
      "Iteration 182, loss = 0.33051937\n",
      "Iteration 183, loss = 0.32974176\n",
      "Iteration 184, loss = 0.32896776\n",
      "Iteration 185, loss = 0.32819731\n",
      "Iteration 186, loss = 0.32743037\n",
      "Iteration 187, loss = 0.32666690\n",
      "Iteration 188, loss = 0.32590684\n",
      "Iteration 189, loss = 0.32515016\n",
      "Iteration 190, loss = 0.32439680\n",
      "Iteration 191, loss = 0.32364674\n",
      "Iteration 192, loss = 0.32289993\n",
      "Iteration 193, loss = 0.32215632\n",
      "Iteration 194, loss = 0.32141588\n",
      "Iteration 195, loss = 0.32067857\n",
      "Iteration 196, loss = 0.31994434\n",
      "Iteration 197, loss = 0.31921318\n",
      "Iteration 198, loss = 0.31848502\n",
      "Iteration 199, loss = 0.31775985\n",
      "Iteration 200, loss = 0.31703763\n",
      "Iteration 201, loss = 0.31631831\n",
      "Iteration 202, loss = 0.31560188\n",
      "Iteration 203, loss = 0.31488828\n",
      "Iteration 204, loss = 0.31417750\n",
      "Iteration 205, loss = 0.31346950\n",
      "Iteration 206, loss = 0.31276425\n",
      "Iteration 207, loss = 0.31206172\n",
      "Iteration 208, loss = 0.31136188\n",
      "Iteration 209, loss = 0.31066470\n",
      "Iteration 210, loss = 0.30997015\n",
      "Iteration 211, loss = 0.30927820\n",
      "Iteration 212, loss = 0.30858883\n",
      "Iteration 213, loss = 0.30790201\n",
      "Iteration 214, loss = 0.30721771\n",
      "Iteration 215, loss = 0.30653591\n",
      "Iteration 216, loss = 0.30585658\n",
      "Iteration 217, loss = 0.30517969\n",
      "Iteration 218, loss = 0.30450523\n",
      "Iteration 219, loss = 0.30383317\n",
      "Iteration 220, loss = 0.30316348\n",
      "Iteration 221, loss = 0.30249614\n",
      "Iteration 222, loss = 0.30183113\n",
      "Iteration 223, loss = 0.30116843\n",
      "Iteration 224, loss = 0.30050801\n",
      "Iteration 225, loss = 0.29984986\n",
      "Iteration 226, loss = 0.29919396\n",
      "Iteration 227, loss = 0.29854027\n",
      "Iteration 228, loss = 0.29788879\n",
      "Iteration 229, loss = 0.29723950\n",
      "Iteration 230, loss = 0.29659236\n",
      "Iteration 231, loss = 0.29594738\n",
      "Iteration 232, loss = 0.29530452\n",
      "Iteration 233, loss = 0.29466377\n",
      "Iteration 234, loss = 0.29402512\n",
      "Iteration 235, loss = 0.29338854\n",
      "Iteration 236, loss = 0.29275401\n",
      "Iteration 237, loss = 0.29212153\n",
      "Iteration 238, loss = 0.29149107\n",
      "Iteration 239, loss = 0.29086262\n",
      "Iteration 240, loss = 0.29023617\n",
      "Iteration 241, loss = 0.28961169\n",
      "Iteration 242, loss = 0.28898917\n",
      "Iteration 243, loss = 0.28836861\n",
      "Iteration 244, loss = 0.28774997\n",
      "Iteration 245, loss = 0.28713326\n",
      "Iteration 246, loss = 0.28651845\n",
      "Iteration 247, loss = 0.28590554\n",
      "Iteration 248, loss = 0.28529450\n",
      "Iteration 249, loss = 0.28468533\n",
      "Iteration 250, loss = 0.28407801\n",
      "Iteration 251, loss = 0.28347254\n",
      "Iteration 252, loss = 0.28286889\n",
      "Iteration 253, loss = 0.28226706\n",
      "Iteration 254, loss = 0.28166703\n",
      "Iteration 255, loss = 0.28106880\n",
      "Iteration 256, loss = 0.28047234\n",
      "Iteration 257, loss = 0.27987766\n",
      "Iteration 258, loss = 0.27928474\n",
      "Iteration 259, loss = 0.27869357\n",
      "Iteration 260, loss = 0.27810414\n",
      "Iteration 261, loss = 0.27751644\n",
      "Iteration 262, loss = 0.27693045\n",
      "Iteration 263, loss = 0.27634618\n",
      "Iteration 264, loss = 0.27576361\n",
      "Iteration 265, loss = 0.27518272\n",
      "Iteration 266, loss = 0.27460352\n",
      "Iteration 267, loss = 0.27402599\n",
      "Iteration 268, loss = 0.27345013\n",
      "Iteration 269, loss = 0.27287592\n",
      "Iteration 270, loss = 0.27230336\n",
      "Iteration 271, loss = 0.27173244\n",
      "Iteration 272, loss = 0.27116315\n",
      "Iteration 273, loss = 0.27059548\n",
      "Iteration 274, loss = 0.27002942\n",
      "Iteration 275, loss = 0.26946498\n",
      "Iteration 276, loss = 0.26890213\n",
      "Iteration 277, loss = 0.26834088\n",
      "Iteration 278, loss = 0.26778122\n",
      "Iteration 279, loss = 0.26722313\n",
      "Iteration 280, loss = 0.26666662\n",
      "Iteration 281, loss = 0.26611168\n",
      "Iteration 282, loss = 0.26555829\n",
      "Iteration 283, loss = 0.26500646\n",
      "Iteration 284, loss = 0.26445618\n",
      "Iteration 285, loss = 0.26390744\n",
      "Iteration 286, loss = 0.26336023\n",
      "Iteration 287, loss = 0.26281455\n",
      "Iteration 288, loss = 0.26227040\n",
      "Iteration 289, loss = 0.26172777\n",
      "Iteration 290, loss = 0.26118665\n",
      "Iteration 291, loss = 0.26064704\n",
      "Iteration 292, loss = 0.26010893\n",
      "Iteration 293, loss = 0.25957232\n",
      "Iteration 294, loss = 0.25903721\n",
      "Iteration 295, loss = 0.25850358\n",
      "Iteration 296, loss = 0.25797144\n",
      "Iteration 297, loss = 0.25744077\n",
      "Iteration 298, loss = 0.25691158\n",
      "Iteration 299, loss = 0.25638387\n",
      "Iteration 300, loss = 0.25585762\n",
      "Iteration 301, loss = 0.25533283\n",
      "Iteration 302, loss = 0.25480950\n",
      "Iteration 303, loss = 0.25428762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 304, loss = 0.25376720\n",
      "Iteration 305, loss = 0.25324822\n",
      "Iteration 306, loss = 0.25273068\n",
      "Iteration 307, loss = 0.25221459\n",
      "Iteration 308, loss = 0.25169993\n",
      "Iteration 309, loss = 0.25118670\n",
      "Iteration 310, loss = 0.25067490\n",
      "Iteration 311, loss = 0.25016453\n",
      "Iteration 312, loss = 0.24965558\n",
      "Iteration 313, loss = 0.24914805\n",
      "Iteration 314, loss = 0.24864193\n",
      "Iteration 315, loss = 0.24813723\n",
      "Iteration 316, loss = 0.24763395\n",
      "Iteration 317, loss = 0.24713206\n",
      "Iteration 318, loss = 0.24663159\n",
      "Iteration 319, loss = 0.24613252\n",
      "Iteration 320, loss = 0.24563484\n",
      "Iteration 321, loss = 0.24513856\n",
      "Iteration 322, loss = 0.24464368\n",
      "Iteration 323, loss = 0.24415019\n",
      "Iteration 324, loss = 0.24365809\n",
      "Iteration 325, loss = 0.24316738\n",
      "Iteration 326, loss = 0.24267805\n",
      "Iteration 327, loss = 0.24219011\n",
      "Iteration 328, loss = 0.24170355\n",
      "Iteration 329, loss = 0.24121836\n",
      "Iteration 330, loss = 0.24073455\n",
      "Iteration 331, loss = 0.24025212\n",
      "Iteration 332, loss = 0.23977106\n",
      "Iteration 333, loss = 0.23929137\n",
      "Iteration 334, loss = 0.23881305\n",
      "Iteration 335, loss = 0.23833609\n",
      "Iteration 336, loss = 0.23786050\n",
      "Iteration 337, loss = 0.23738627\n",
      "Iteration 338, loss = 0.23691341\n",
      "Iteration 339, loss = 0.23644190\n",
      "Iteration 340, loss = 0.23597175\n",
      "Iteration 341, loss = 0.23550296\n",
      "Iteration 342, loss = 0.23503552\n",
      "Iteration 343, loss = 0.23456943\n",
      "Iteration 344, loss = 0.23410470\n",
      "Iteration 345, loss = 0.23364131\n",
      "Iteration 346, loss = 0.23317928\n",
      "Iteration 347, loss = 0.23271859\n",
      "Iteration 348, loss = 0.23225924\n",
      "Iteration 349, loss = 0.23180124\n",
      "Iteration 350, loss = 0.23134458\n",
      "Iteration 351, loss = 0.23088927\n",
      "Iteration 352, loss = 0.23043529\n",
      "Iteration 353, loss = 0.22998265\n",
      "Iteration 354, loss = 0.22953135\n",
      "Iteration 355, loss = 0.22908138\n",
      "Iteration 356, loss = 0.22863275\n",
      "Iteration 357, loss = 0.22818544\n",
      "Iteration 358, loss = 0.22773948\n",
      "Iteration 359, loss = 0.22729484\n",
      "Iteration 360, loss = 0.22685153\n",
      "Iteration 361, loss = 0.22640955\n",
      "Iteration 362, loss = 0.22596889\n",
      "Iteration 363, loss = 0.22552956\n",
      "Iteration 364, loss = 0.22509156\n",
      "Iteration 365, loss = 0.22465487\n",
      "Iteration 366, loss = 0.22421951\n",
      "Iteration 367, loss = 0.22378547\n",
      "Iteration 368, loss = 0.22335275\n",
      "Iteration 369, loss = 0.22292135\n",
      "Iteration 370, loss = 0.22249126\n",
      "Iteration 371, loss = 0.22206249\n",
      "Iteration 372, loss = 0.22163503\n",
      "Iteration 373, loss = 0.22120889\n",
      "Iteration 374, loss = 0.22078406\n",
      "Iteration 375, loss = 0.22036054\n",
      "Iteration 376, loss = 0.21993833\n",
      "Iteration 377, loss = 0.21951743\n",
      "Iteration 378, loss = 0.21909783\n",
      "Iteration 379, loss = 0.21867954\n",
      "Iteration 380, loss = 0.21826256\n",
      "Iteration 381, loss = 0.21784688\n",
      "Iteration 382, loss = 0.21743250\n",
      "Iteration 383, loss = 0.21701943\n",
      "Iteration 384, loss = 0.21660765\n",
      "Iteration 385, loss = 0.21619717\n",
      "Iteration 386, loss = 0.21578799\n",
      "Iteration 387, loss = 0.21538011\n",
      "Iteration 388, loss = 0.21497352\n",
      "Iteration 389, loss = 0.21456823\n",
      "Iteration 390, loss = 0.21416423\n",
      "Iteration 391, loss = 0.21376152\n",
      "Iteration 392, loss = 0.21336010\n",
      "Iteration 393, loss = 0.21295996\n",
      "Iteration 394, loss = 0.21256112\n",
      "Iteration 395, loss = 0.21216356\n",
      "Iteration 396, loss = 0.21176729\n",
      "Iteration 397, loss = 0.21137230\n",
      "Iteration 398, loss = 0.21097859\n",
      "Iteration 399, loss = 0.21058617\n",
      "Iteration 400, loss = 0.21019502\n",
      "Iteration 401, loss = 0.20980515\n",
      "Iteration 402, loss = 0.20941656\n",
      "Iteration 403, loss = 0.20902924\n",
      "Iteration 404, loss = 0.20864320\n",
      "Iteration 405, loss = 0.20825843\n",
      "Iteration 406, loss = 0.20787493\n",
      "Iteration 407, loss = 0.20749269\n",
      "Iteration 408, loss = 0.20711173\n",
      "Iteration 409, loss = 0.20673203\n",
      "Iteration 410, loss = 0.20635360\n",
      "Iteration 411, loss = 0.20597643\n",
      "Iteration 412, loss = 0.20560052\n",
      "Iteration 413, loss = 0.20522588\n",
      "Iteration 414, loss = 0.20485249\n",
      "Iteration 415, loss = 0.20448035\n",
      "Iteration 416, loss = 0.20410948\n",
      "Iteration 417, loss = 0.20373985\n",
      "Iteration 418, loss = 0.20337148\n",
      "Iteration 419, loss = 0.20300436\n",
      "Iteration 420, loss = 0.20263849\n",
      "Iteration 421, loss = 0.20227386\n",
      "Iteration 422, loss = 0.20191048\n",
      "Iteration 423, loss = 0.20154834\n",
      "Iteration 424, loss = 0.20118745\n",
      "Iteration 425, loss = 0.20082779\n",
      "Iteration 426, loss = 0.20046937\n",
      "Iteration 427, loss = 0.20011219\n",
      "Iteration 428, loss = 0.19975624\n",
      "Iteration 429, loss = 0.19940152\n",
      "Iteration 430, loss = 0.19904803\n",
      "Iteration 431, loss = 0.19869577\n",
      "Iteration 432, loss = 0.19834474\n",
      "Iteration 433, loss = 0.19799493\n",
      "Iteration 434, loss = 0.19764635\n",
      "Iteration 435, loss = 0.19729898\n",
      "Iteration 436, loss = 0.19695284\n",
      "Iteration 437, loss = 0.19660791\n",
      "Iteration 438, loss = 0.19626419\n",
      "Iteration 439, loss = 0.19592169\n",
      "Iteration 440, loss = 0.19558040\n",
      "Iteration 441, loss = 0.19524031\n",
      "Iteration 442, loss = 0.19490143\n",
      "Iteration 443, loss = 0.19456376\n",
      "Iteration 444, loss = 0.19422729\n",
      "Iteration 445, loss = 0.19389201\n",
      "Iteration 446, loss = 0.19355793\n",
      "Iteration 447, loss = 0.19322505\n",
      "Iteration 448, loss = 0.19289336\n",
      "Iteration 449, loss = 0.19256286\n",
      "Iteration 450, loss = 0.19223355\n",
      "Iteration 451, loss = 0.19190543\n",
      "Iteration 452, loss = 0.19157848\n",
      "Iteration 453, loss = 0.19125272\n",
      "Iteration 454, loss = 0.19092814\n",
      "Iteration 455, loss = 0.19060473\n",
      "Iteration 456, loss = 0.19028250\n",
      "Iteration 457, loss = 0.18996144\n",
      "Iteration 458, loss = 0.18964155\n",
      "Iteration 459, loss = 0.18932283\n",
      "Iteration 460, loss = 0.18900526\n",
      "Iteration 461, loss = 0.18868886\n",
      "Iteration 462, loss = 0.18837362\n",
      "Iteration 463, loss = 0.18805954\n",
      "Iteration 464, loss = 0.18774661\n",
      "Iteration 465, loss = 0.18743483\n",
      "Iteration 466, loss = 0.18712420\n",
      "Iteration 467, loss = 0.18681472\n",
      "Iteration 468, loss = 0.18650638\n",
      "Iteration 469, loss = 0.18619918\n",
      "Iteration 470, loss = 0.18589311\n",
      "Iteration 471, loss = 0.18558819\n",
      "Iteration 472, loss = 0.18528440\n",
      "Iteration 473, loss = 0.18498173\n",
      "Iteration 474, loss = 0.18468020\n",
      "Iteration 475, loss = 0.18437979\n",
      "Iteration 476, loss = 0.18408050\n",
      "Iteration 477, loss = 0.18378233\n",
      "Iteration 478, loss = 0.18348527\n",
      "Iteration 479, loss = 0.18318933\n",
      "Iteration 480, loss = 0.18289450\n",
      "Iteration 481, loss = 0.18260078\n",
      "Iteration 482, loss = 0.18230817\n",
      "Iteration 483, loss = 0.18201665\n",
      "Iteration 484, loss = 0.18172624\n",
      "Iteration 485, loss = 0.18143692\n",
      "Iteration 486, loss = 0.18114869\n",
      "Iteration 487, loss = 0.18086156\n",
      "Iteration 488, loss = 0.18057551\n",
      "Iteration 489, loss = 0.18029055\n",
      "Iteration 490, loss = 0.18000667\n",
      "Iteration 491, loss = 0.17972387\n",
      "Iteration 492, loss = 0.17944215\n",
      "Iteration 493, loss = 0.17916149\n",
      "Iteration 494, loss = 0.17888191\n",
      "Iteration 495, loss = 0.17860340\n",
      "Iteration 496, loss = 0.17832595\n",
      "Iteration 497, loss = 0.17804955\n",
      "Iteration 498, loss = 0.17777422\n",
      "Iteration 499, loss = 0.17749994\n",
      "Iteration 500, loss = 0.17722672\n",
      "Iteration 1, loss = 1.43094140\n",
      "Iteration 2, loss = 1.41490844\n",
      "Iteration 3, loss = 1.39248114\n",
      "Iteration 4, loss = 1.36473695\n",
      "Iteration 5, loss = 1.33271501\n",
      "Iteration 6, loss = 1.29739643\n",
      "Iteration 7, loss = 1.25969017\n",
      "Iteration 8, loss = 1.22042352\n",
      "Iteration 9, loss = 1.18033624\n",
      "Iteration 10, loss = 1.14007767\n",
      "Iteration 11, loss = 1.10020615\n",
      "Iteration 12, loss = 1.06119041\n",
      "Iteration 13, loss = 1.02341253\n",
      "Iteration 14, loss = 0.98717229\n",
      "Iteration 15, loss = 0.95269277\n",
      "Iteration 16, loss = 0.92012696\n",
      "Iteration 17, loss = 0.88956523\n",
      "Iteration 18, loss = 0.86104341\n",
      "Iteration 19, loss = 0.83455119\n",
      "Iteration 20, loss = 0.81004057\n",
      "Iteration 21, loss = 0.78743400\n",
      "Iteration 22, loss = 0.76663201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.74752007\n",
      "Iteration 24, loss = 0.72997459\n",
      "Iteration 25, loss = 0.71386797\n",
      "Iteration 26, loss = 0.69907264\n",
      "Iteration 27, loss = 0.68546430\n",
      "Iteration 28, loss = 0.67292423\n",
      "Iteration 29, loss = 0.66134094\n",
      "Iteration 30, loss = 0.65061119\n",
      "Iteration 31, loss = 0.64064057\n",
      "Iteration 32, loss = 0.63134362\n",
      "Iteration 33, loss = 0.62264370\n",
      "Iteration 34, loss = 0.61447266\n",
      "Iteration 35, loss = 0.60677032\n",
      "Iteration 36, loss = 0.59948393\n",
      "Iteration 37, loss = 0.59256748\n",
      "Iteration 38, loss = 0.58598106\n",
      "Iteration 39, loss = 0.57969023\n",
      "Iteration 40, loss = 0.57366539\n",
      "Iteration 41, loss = 0.56788115\n",
      "Iteration 42, loss = 0.56231583\n",
      "Iteration 43, loss = 0.55695091\n",
      "Iteration 44, loss = 0.55177061\n",
      "Iteration 45, loss = 0.54676144\n",
      "Iteration 46, loss = 0.54191187\n",
      "Iteration 47, loss = 0.53721199\n",
      "Iteration 48, loss = 0.53265324\n",
      "Iteration 49, loss = 0.52822817\n",
      "Iteration 50, loss = 0.52393023\n",
      "Iteration 51, loss = 0.51975360\n",
      "Iteration 52, loss = 0.51569306\n",
      "Iteration 53, loss = 0.51174384\n",
      "Iteration 54, loss = 0.50790153\n",
      "Iteration 55, loss = 0.50416204\n",
      "Iteration 56, loss = 0.50052148\n",
      "Iteration 57, loss = 0.49697618\n",
      "Iteration 58, loss = 0.49352258\n",
      "Iteration 59, loss = 0.49015725\n",
      "Iteration 60, loss = 0.48687688\n",
      "Iteration 61, loss = 0.48367824\n",
      "Iteration 62, loss = 0.48055819\n",
      "Iteration 63, loss = 0.47751366\n",
      "Iteration 64, loss = 0.47454169\n",
      "Iteration 65, loss = 0.47163937\n",
      "Iteration 66, loss = 0.46880391\n",
      "Iteration 67, loss = 0.46603259\n",
      "Iteration 68, loss = 0.46332280\n",
      "Iteration 69, loss = 0.46067200\n",
      "Iteration 70, loss = 0.45807777\n",
      "Iteration 71, loss = 0.45553781\n",
      "Iteration 72, loss = 0.45304987\n",
      "Iteration 73, loss = 0.45061186\n",
      "Iteration 74, loss = 0.44822176\n",
      "Iteration 75, loss = 0.44587764\n",
      "Iteration 76, loss = 0.44357771\n",
      "Iteration 77, loss = 0.44132023\n",
      "Iteration 78, loss = 0.43910358\n",
      "Iteration 79, loss = 0.43692623\n",
      "Iteration 80, loss = 0.43478672\n",
      "Iteration 81, loss = 0.43268367\n",
      "Iteration 82, loss = 0.43061579\n",
      "Iteration 83, loss = 0.42858186\n",
      "Iteration 84, loss = 0.42658070\n",
      "Iteration 85, loss = 0.42461123\n",
      "Iteration 86, loss = 0.42267241\n",
      "Iteration 87, loss = 0.42076325\n",
      "Iteration 88, loss = 0.41888281\n",
      "Iteration 89, loss = 0.41703020\n",
      "Iteration 90, loss = 0.41520458\n",
      "Iteration 91, loss = 0.41340514\n",
      "Iteration 92, loss = 0.41163112\n",
      "Iteration 93, loss = 0.40988176\n",
      "Iteration 94, loss = 0.40815638\n",
      "Iteration 95, loss = 0.40645429\n",
      "Iteration 96, loss = 0.40477485\n",
      "Iteration 97, loss = 0.40311743\n",
      "Iteration 98, loss = 0.40148144\n",
      "Iteration 99, loss = 0.39986631\n",
      "Iteration 100, loss = 0.39827147\n",
      "Iteration 101, loss = 0.39669639\n",
      "Iteration 102, loss = 0.39514057\n",
      "Iteration 103, loss = 0.39360351\n",
      "Iteration 104, loss = 0.39208472\n",
      "Iteration 105, loss = 0.39058374\n",
      "Iteration 106, loss = 0.38910013\n",
      "Iteration 107, loss = 0.38763346\n",
      "Iteration 108, loss = 0.38618330\n",
      "Iteration 109, loss = 0.38474927\n",
      "Iteration 110, loss = 0.38333096\n",
      "Iteration 111, loss = 0.38192800\n",
      "Iteration 112, loss = 0.38054004\n",
      "Iteration 113, loss = 0.37916670\n",
      "Iteration 114, loss = 0.37780766\n",
      "Iteration 115, loss = 0.37646258\n",
      "Iteration 116, loss = 0.37513115\n",
      "Iteration 117, loss = 0.37381306\n",
      "Iteration 118, loss = 0.37250800\n",
      "Iteration 119, loss = 0.37121568\n",
      "Iteration 120, loss = 0.36993583\n",
      "Iteration 121, loss = 0.36866818\n",
      "Iteration 122, loss = 0.36741245\n",
      "Iteration 123, loss = 0.36616839\n",
      "Iteration 124, loss = 0.36493575\n",
      "Iteration 125, loss = 0.36371430\n",
      "Iteration 126, loss = 0.36250379\n",
      "Iteration 127, loss = 0.36130401\n",
      "Iteration 128, loss = 0.36011472\n",
      "Iteration 129, loss = 0.35893571\n",
      "Iteration 130, loss = 0.35776679\n",
      "Iteration 131, loss = 0.35660773\n",
      "Iteration 132, loss = 0.35545835\n",
      "Iteration 133, loss = 0.35431846\n",
      "Iteration 134, loss = 0.35318786\n",
      "Iteration 135, loss = 0.35206638\n",
      "Iteration 136, loss = 0.35095384\n",
      "Iteration 137, loss = 0.34985006\n",
      "Iteration 138, loss = 0.34875489\n",
      "Iteration 139, loss = 0.34766816\n",
      "Iteration 140, loss = 0.34658970\n",
      "Iteration 141, loss = 0.34551937\n",
      "Iteration 142, loss = 0.34445701\n",
      "Iteration 143, loss = 0.34340248\n",
      "Iteration 144, loss = 0.34235564\n",
      "Iteration 145, loss = 0.34131634\n",
      "Iteration 146, loss = 0.34028445\n",
      "Iteration 147, loss = 0.33925983\n",
      "Iteration 148, loss = 0.33824236\n",
      "Iteration 149, loss = 0.33723192\n",
      "Iteration 150, loss = 0.33622837\n",
      "Iteration 151, loss = 0.33523160\n",
      "Iteration 152, loss = 0.33424149\n",
      "Iteration 153, loss = 0.33325792\n",
      "Iteration 154, loss = 0.33228079\n",
      "Iteration 155, loss = 0.33130999\n",
      "Iteration 156, loss = 0.33034541\n",
      "Iteration 157, loss = 0.32938694\n",
      "Iteration 158, loss = 0.32843449\n",
      "Iteration 159, loss = 0.32748795\n",
      "Iteration 160, loss = 0.32654724\n",
      "Iteration 161, loss = 0.32561224\n",
      "Iteration 162, loss = 0.32468288\n",
      "Iteration 163, loss = 0.32375906\n",
      "Iteration 164, loss = 0.32284069\n",
      "Iteration 165, loss = 0.32192769\n",
      "Iteration 166, loss = 0.32101997\n",
      "Iteration 167, loss = 0.32011746\n",
      "Iteration 168, loss = 0.31922006\n",
      "Iteration 169, loss = 0.31832770\n",
      "Iteration 170, loss = 0.31744031\n",
      "Iteration 171, loss = 0.31655780\n",
      "Iteration 172, loss = 0.31568011\n",
      "Iteration 173, loss = 0.31480716\n",
      "Iteration 174, loss = 0.31393889\n",
      "Iteration 175, loss = 0.31307522\n",
      "Iteration 176, loss = 0.31221608\n",
      "Iteration 177, loss = 0.31136141\n",
      "Iteration 178, loss = 0.31051115\n",
      "Iteration 179, loss = 0.30966522\n",
      "Iteration 180, loss = 0.30882358\n",
      "Iteration 181, loss = 0.30798616\n",
      "Iteration 182, loss = 0.30715289\n",
      "Iteration 183, loss = 0.30632373\n",
      "Iteration 184, loss = 0.30549861\n",
      "Iteration 185, loss = 0.30467748\n",
      "Iteration 186, loss = 0.30386029\n",
      "Iteration 187, loss = 0.30304697\n",
      "Iteration 188, loss = 0.30223748\n",
      "Iteration 189, loss = 0.30143178\n",
      "Iteration 190, loss = 0.30062979\n",
      "Iteration 191, loss = 0.29983149\n",
      "Iteration 192, loss = 0.29903682\n",
      "Iteration 193, loss = 0.29824573\n",
      "Iteration 194, loss = 0.29745817\n",
      "Iteration 195, loss = 0.29667411\n",
      "Iteration 196, loss = 0.29589350\n",
      "Iteration 197, loss = 0.29511628\n",
      "Iteration 198, loss = 0.29434244\n",
      "Iteration 199, loss = 0.29357191\n",
      "Iteration 200, loss = 0.29280466\n",
      "Iteration 201, loss = 0.29204064\n",
      "Iteration 202, loss = 0.29127983\n",
      "Iteration 203, loss = 0.29052218\n",
      "Iteration 204, loss = 0.28976765\n",
      "Iteration 205, loss = 0.28901622\n",
      "Iteration 206, loss = 0.28826783\n",
      "Iteration 207, loss = 0.28752245\n",
      "Iteration 208, loss = 0.28678006\n",
      "Iteration 209, loss = 0.28604062\n",
      "Iteration 210, loss = 0.28530408\n",
      "Iteration 211, loss = 0.28457043\n",
      "Iteration 212, loss = 0.28383963\n",
      "Iteration 213, loss = 0.28311165\n",
      "Iteration 214, loss = 0.28238645\n",
      "Iteration 215, loss = 0.28166401\n",
      "Iteration 216, loss = 0.28094430\n",
      "Iteration 217, loss = 0.28022728\n",
      "Iteration 218, loss = 0.27951293\n",
      "Iteration 219, loss = 0.27880123\n",
      "Iteration 220, loss = 0.27809214\n",
      "Iteration 221, loss = 0.27738564\n",
      "Iteration 222, loss = 0.27668170\n",
      "Iteration 223, loss = 0.27598030\n",
      "Iteration 224, loss = 0.27528141\n",
      "Iteration 225, loss = 0.27458500\n",
      "Iteration 226, loss = 0.27389106\n",
      "Iteration 227, loss = 0.27319955\n",
      "Iteration 228, loss = 0.27251046\n",
      "Iteration 229, loss = 0.27182377\n",
      "Iteration 230, loss = 0.27113944\n",
      "Iteration 231, loss = 0.27045746\n",
      "Iteration 232, loss = 0.26977781\n",
      "Iteration 233, loss = 0.26910047\n",
      "Iteration 234, loss = 0.26842541\n",
      "Iteration 235, loss = 0.26775261\n",
      "Iteration 236, loss = 0.26708206\n",
      "Iteration 237, loss = 0.26641374\n",
      "Iteration 238, loss = 0.26574763\n",
      "Iteration 239, loss = 0.26508370\n",
      "Iteration 240, loss = 0.26442194\n",
      "Iteration 241, loss = 0.26376234\n",
      "Iteration 242, loss = 0.26310487\n",
      "Iteration 243, loss = 0.26244951\n",
      "Iteration 244, loss = 0.26179626\n",
      "Iteration 245, loss = 0.26114509\n",
      "Iteration 246, loss = 0.26049600\n",
      "Iteration 247, loss = 0.25984895\n",
      "Iteration 248, loss = 0.25920394\n",
      "Iteration 249, loss = 0.25856095\n",
      "Iteration 250, loss = 0.25791997\n",
      "Iteration 251, loss = 0.25728098\n",
      "Iteration 252, loss = 0.25664397\n",
      "Iteration 253, loss = 0.25600893\n",
      "Iteration 254, loss = 0.25537583\n",
      "Iteration 255, loss = 0.25474467\n",
      "Iteration 256, loss = 0.25411544\n",
      "Iteration 257, loss = 0.25348811\n",
      "Iteration 258, loss = 0.25286269\n",
      "Iteration 259, loss = 0.25223915\n",
      "Iteration 260, loss = 0.25161748\n",
      "Iteration 261, loss = 0.25099768\n",
      "Iteration 262, loss = 0.25037973\n",
      "Iteration 263, loss = 0.24976361\n",
      "Iteration 264, loss = 0.24914933\n",
      "Iteration 265, loss = 0.24853686\n",
      "Iteration 266, loss = 0.24792620\n",
      "Iteration 267, loss = 0.24731733\n",
      "Iteration 268, loss = 0.24671026\n",
      "Iteration 269, loss = 0.24610495\n",
      "Iteration 270, loss = 0.24550142\n",
      "Iteration 271, loss = 0.24489964\n",
      "Iteration 272, loss = 0.24429961\n",
      "Iteration 273, loss = 0.24370131\n",
      "Iteration 274, loss = 0.24310475\n",
      "Iteration 275, loss = 0.24250991\n",
      "Iteration 276, loss = 0.24191678\n",
      "Iteration 277, loss = 0.24132535\n",
      "Iteration 278, loss = 0.24073562\n",
      "Iteration 279, loss = 0.24014758\n",
      "Iteration 280, loss = 0.23956122\n",
      "Iteration 281, loss = 0.23897653\n",
      "Iteration 282, loss = 0.23839350\n",
      "Iteration 283, loss = 0.23781213\n",
      "Iteration 284, loss = 0.23723242\n",
      "Iteration 285, loss = 0.23665434\n",
      "Iteration 286, loss = 0.23607791\n",
      "Iteration 287, loss = 0.23550310\n",
      "Iteration 288, loss = 0.23492992\n",
      "Iteration 289, loss = 0.23435835\n",
      "Iteration 290, loss = 0.23378839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 291, loss = 0.23322004\n",
      "Iteration 292, loss = 0.23265329\n",
      "Iteration 293, loss = 0.23208813\n",
      "Iteration 294, loss = 0.23152456\n",
      "Iteration 295, loss = 0.23096257\n",
      "Iteration 296, loss = 0.23040216\n",
      "Iteration 297, loss = 0.22984332\n",
      "Iteration 298, loss = 0.22928604\n",
      "Iteration 299, loss = 0.22873033\n",
      "Iteration 300, loss = 0.22817617\n",
      "Iteration 301, loss = 0.22762356\n",
      "Iteration 302, loss = 0.22707250\n",
      "Iteration 303, loss = 0.22652299\n",
      "Iteration 304, loss = 0.22597501\n",
      "Iteration 305, loss = 0.22542856\n",
      "Iteration 306, loss = 0.22488364\n",
      "Iteration 307, loss = 0.22434025\n",
      "Iteration 308, loss = 0.22379838\n",
      "Iteration 309, loss = 0.22325803\n",
      "Iteration 310, loss = 0.22271919\n",
      "Iteration 311, loss = 0.22218186\n",
      "Iteration 312, loss = 0.22164603\n",
      "Iteration 313, loss = 0.22111171\n",
      "Iteration 314, loss = 0.22057889\n",
      "Iteration 315, loss = 0.22004756\n",
      "Iteration 316, loss = 0.21951772\n",
      "Iteration 317, loss = 0.21898938\n",
      "Iteration 318, loss = 0.21846252\n",
      "Iteration 319, loss = 0.21793714\n",
      "Iteration 320, loss = 0.21741324\n",
      "Iteration 321, loss = 0.21689082\n",
      "Iteration 322, loss = 0.21636987\n",
      "Iteration 323, loss = 0.21585039\n",
      "Iteration 324, loss = 0.21533239\n",
      "Iteration 325, loss = 0.21481584\n",
      "Iteration 326, loss = 0.21430077\n",
      "Iteration 327, loss = 0.21378715\n",
      "Iteration 328, loss = 0.21327499\n",
      "Iteration 329, loss = 0.21276429\n",
      "Iteration 330, loss = 0.21225504\n",
      "Iteration 331, loss = 0.21174724\n",
      "Iteration 332, loss = 0.21124089\n",
      "Iteration 333, loss = 0.21073599\n",
      "Iteration 334, loss = 0.21023253\n",
      "Iteration 335, loss = 0.20973052\n",
      "Iteration 336, loss = 0.20922995\n",
      "Iteration 337, loss = 0.20873081\n",
      "Iteration 338, loss = 0.20823312\n",
      "Iteration 339, loss = 0.20773686\n",
      "Iteration 340, loss = 0.20724203\n",
      "Iteration 341, loss = 0.20674863\n",
      "Iteration 342, loss = 0.20625666\n",
      "Iteration 343, loss = 0.20576612\n",
      "Iteration 344, loss = 0.20527701\n",
      "Iteration 345, loss = 0.20478932\n",
      "Iteration 346, loss = 0.20430306\n",
      "Iteration 347, loss = 0.20381821\n",
      "Iteration 348, loss = 0.20333479\n",
      "Iteration 349, loss = 0.20285278\n",
      "Iteration 350, loss = 0.20237220\n",
      "Iteration 351, loss = 0.20189302\n",
      "Iteration 352, loss = 0.20141526\n",
      "Iteration 353, loss = 0.20093892\n",
      "Iteration 354, loss = 0.20046398\n",
      "Iteration 355, loss = 0.19999046\n",
      "Iteration 356, loss = 0.19951834\n",
      "Iteration 357, loss = 0.19904763\n",
      "Iteration 358, loss = 0.19857832\n",
      "Iteration 359, loss = 0.19811043\n",
      "Iteration 360, loss = 0.19764393\n",
      "Iteration 361, loss = 0.19717884\n",
      "Iteration 362, loss = 0.19671514\n",
      "Iteration 363, loss = 0.19625285\n",
      "Iteration 364, loss = 0.19579195\n",
      "Iteration 365, loss = 0.19533245\n",
      "Iteration 366, loss = 0.19487435\n",
      "Iteration 367, loss = 0.19441764\n",
      "Iteration 368, loss = 0.19396233\n",
      "Iteration 369, loss = 0.19350841\n",
      "Iteration 370, loss = 0.19305588\n",
      "Iteration 371, loss = 0.19260474\n",
      "Iteration 372, loss = 0.19215498\n",
      "Iteration 373, loss = 0.19170662\n",
      "Iteration 374, loss = 0.19125964\n",
      "Iteration 375, loss = 0.19081405\n",
      "Iteration 376, loss = 0.19036984\n",
      "Iteration 377, loss = 0.18992702\n",
      "Iteration 378, loss = 0.18948558\n",
      "Iteration 379, loss = 0.18904551\n",
      "Iteration 380, loss = 0.18860683\n",
      "Iteration 381, loss = 0.18816953\n",
      "Iteration 382, loss = 0.18773360\n",
      "Iteration 383, loss = 0.18729905\n",
      "Iteration 384, loss = 0.18686588\n",
      "Iteration 385, loss = 0.18643408\n",
      "Iteration 386, loss = 0.18600365\n",
      "Iteration 387, loss = 0.18557459\n",
      "Iteration 388, loss = 0.18514690\n",
      "Iteration 389, loss = 0.18472059\n",
      "Iteration 390, loss = 0.18429564\n",
      "Iteration 391, loss = 0.18387205\n",
      "Iteration 392, loss = 0.18344983\n",
      "Iteration 393, loss = 0.18302898\n",
      "Iteration 394, loss = 0.18260949\n",
      "Iteration 395, loss = 0.18219136\n",
      "Iteration 396, loss = 0.18177459\n",
      "Iteration 397, loss = 0.18135918\n",
      "Iteration 398, loss = 0.18094513\n",
      "Iteration 399, loss = 0.18053243\n",
      "Iteration 400, loss = 0.18012109\n",
      "Iteration 401, loss = 0.17971111\n",
      "Iteration 402, loss = 0.17930247\n",
      "Iteration 403, loss = 0.17889519\n",
      "Iteration 404, loss = 0.17848925\n",
      "Iteration 405, loss = 0.17808467\n",
      "Iteration 406, loss = 0.17768143\n",
      "Iteration 407, loss = 0.17727953\n",
      "Iteration 408, loss = 0.17687898\n",
      "Iteration 409, loss = 0.17647978\n",
      "Iteration 410, loss = 0.17608191\n",
      "Iteration 411, loss = 0.17568538\n",
      "Iteration 412, loss = 0.17529019\n",
      "Iteration 413, loss = 0.17489634\n",
      "Iteration 414, loss = 0.17450382\n",
      "Iteration 415, loss = 0.17411263\n",
      "Iteration 416, loss = 0.17372278\n",
      "Iteration 417, loss = 0.17333425\n",
      "Iteration 418, loss = 0.17294705\n",
      "Iteration 419, loss = 0.17256118\n",
      "Iteration 420, loss = 0.17217663\n",
      "Iteration 421, loss = 0.17179341\n",
      "Iteration 422, loss = 0.17141151\n",
      "Iteration 423, loss = 0.17103092\n",
      "Iteration 424, loss = 0.17065166\n",
      "Iteration 425, loss = 0.17027371\n",
      "Iteration 426, loss = 0.16989707\n",
      "Iteration 427, loss = 0.16952175\n",
      "Iteration 428, loss = 0.16914773\n",
      "Iteration 429, loss = 0.16877503\n",
      "Iteration 430, loss = 0.16840363\n",
      "Iteration 431, loss = 0.16803353\n",
      "Iteration 432, loss = 0.16766474\n",
      "Iteration 433, loss = 0.16729725\n",
      "Iteration 434, loss = 0.16693105\n",
      "Iteration 435, loss = 0.16656615\n",
      "Iteration 436, loss = 0.16620255\n",
      "Iteration 437, loss = 0.16584024\n",
      "Iteration 438, loss = 0.16547922\n",
      "Iteration 439, loss = 0.16511948\n",
      "Iteration 440, loss = 0.16476103\n",
      "Iteration 441, loss = 0.16440387\n",
      "Iteration 442, loss = 0.16404798\n",
      "Iteration 443, loss = 0.16369338\n",
      "Iteration 444, loss = 0.16334005\n",
      "Iteration 445, loss = 0.16298800\n",
      "Iteration 446, loss = 0.16263722\n",
      "Iteration 447, loss = 0.16228770\n",
      "Iteration 448, loss = 0.16193946\n",
      "Iteration 449, loss = 0.16159248\n",
      "Iteration 450, loss = 0.16124676\n",
      "Iteration 451, loss = 0.16090231\n",
      "Iteration 452, loss = 0.16055911\n",
      "Iteration 453, loss = 0.16021717\n",
      "Iteration 454, loss = 0.15987648\n",
      "Iteration 455, loss = 0.15953704\n",
      "Iteration 456, loss = 0.15919885\n",
      "Iteration 457, loss = 0.15886190\n",
      "Iteration 458, loss = 0.15852620\n",
      "Iteration 459, loss = 0.15819173\n",
      "Iteration 460, loss = 0.15785851\n",
      "Iteration 461, loss = 0.15752652\n",
      "Iteration 462, loss = 0.15719576\n",
      "Iteration 463, loss = 0.15686623\n",
      "Iteration 464, loss = 0.15653792\n",
      "Iteration 465, loss = 0.15621085\n",
      "Iteration 466, loss = 0.15588499\n",
      "Iteration 467, loss = 0.15556035\n",
      "Iteration 468, loss = 0.15523693\n",
      "Iteration 469, loss = 0.15491472\n",
      "Iteration 470, loss = 0.15459372\n",
      "Iteration 471, loss = 0.15427392\n",
      "Iteration 472, loss = 0.15395534\n",
      "Iteration 473, loss = 0.15363795\n",
      "Iteration 474, loss = 0.15332176\n",
      "Iteration 475, loss = 0.15300677\n",
      "Iteration 476, loss = 0.15269297\n",
      "Iteration 477, loss = 0.15238036\n",
      "Iteration 478, loss = 0.15206894\n",
      "Iteration 479, loss = 0.15175870\n",
      "Iteration 480, loss = 0.15144964\n",
      "Iteration 481, loss = 0.15114176\n",
      "Iteration 482, loss = 0.15083505\n",
      "Iteration 483, loss = 0.15052952\n",
      "Iteration 484, loss = 0.15022515\n",
      "Iteration 485, loss = 0.14992195\n",
      "Iteration 486, loss = 0.14961992\n",
      "Iteration 487, loss = 0.14931904\n",
      "Iteration 488, loss = 0.14901932\n",
      "Iteration 489, loss = 0.14872075\n",
      "Iteration 490, loss = 0.14842333\n",
      "Iteration 491, loss = 0.14812706\n",
      "Iteration 492, loss = 0.14783193\n",
      "Iteration 493, loss = 0.14753794\n",
      "Iteration 494, loss = 0.14724509\n",
      "Iteration 495, loss = 0.14695337\n",
      "Iteration 496, loss = 0.14666278\n",
      "Iteration 497, loss = 0.14637332\n",
      "Iteration 498, loss = 0.14608498\n",
      "Iteration 499, loss = 0.14579777\n",
      "Iteration 500, loss = 0.14551167\n",
      "Iteration 1, loss = 1.42962563\n",
      "Iteration 2, loss = 1.41377261\n",
      "Iteration 3, loss = 1.39159963\n",
      "Iteration 4, loss = 1.36417392\n",
      "Iteration 5, loss = 1.33252441\n",
      "Iteration 6, loss = 1.29762192\n",
      "Iteration 7, loss = 1.26036508\n",
      "Iteration 8, loss = 1.22157091\n",
      "Iteration 9, loss = 1.18196924\n",
      "Iteration 10, loss = 1.14220009\n",
      "Iteration 11, loss = 1.10281345\n",
      "Iteration 12, loss = 1.06427097\n",
      "Iteration 13, loss = 1.02694925\n",
      "Iteration 14, loss = 0.99114430\n",
      "Iteration 15, loss = 0.95707715\n",
      "Iteration 16, loss = 0.92490038\n",
      "Iteration 17, loss = 0.89470532\n",
      "Iteration 18, loss = 0.86652986\n",
      "Iteration 19, loss = 0.84036653\n",
      "Iteration 20, loss = 0.81617056\n",
      "Iteration 21, loss = 0.79386776\n",
      "Iteration 22, loss = 0.77336194\n",
      "Iteration 23, loss = 0.75454158\n",
      "Iteration 24, loss = 0.73728569\n",
      "Iteration 25, loss = 0.72146884\n",
      "Iteration 26, loss = 0.70696517\n",
      "Iteration 27, loss = 0.69365165\n",
      "Iteration 28, loss = 0.68141041\n",
      "Iteration 29, loss = 0.67013043\n",
      "Iteration 30, loss = 0.65970870\n",
      "Iteration 31, loss = 0.65005070\n",
      "Iteration 32, loss = 0.64107074\n",
      "Iteration 33, loss = 0.63269177\n",
      "Iteration 34, loss = 0.62484514\n",
      "Iteration 35, loss = 0.61747009\n",
      "Iteration 36, loss = 0.61051324\n",
      "Iteration 37, loss = 0.60392797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.59767375\n",
      "Iteration 39, loss = 0.59171554\n",
      "Iteration 40, loss = 0.58602313\n",
      "Iteration 41, loss = 0.58057062\n",
      "Iteration 42, loss = 0.57533580\n",
      "Iteration 43, loss = 0.57029967\n",
      "Iteration 44, loss = 0.56544601\n",
      "Iteration 45, loss = 0.56076092\n",
      "Iteration 46, loss = 0.55623249\n",
      "Iteration 47, loss = 0.55185047\n",
      "Iteration 48, loss = 0.54760597\n",
      "Iteration 49, loss = 0.54349123\n",
      "Iteration 50, loss = 0.53949944\n",
      "Iteration 51, loss = 0.53562451\n",
      "Iteration 52, loss = 0.53186098\n",
      "Iteration 53, loss = 0.52820387\n",
      "Iteration 54, loss = 0.52464857\n",
      "Iteration 55, loss = 0.52119080\n",
      "Iteration 56, loss = 0.51782653\n",
      "Iteration 57, loss = 0.51455192\n",
      "Iteration 58, loss = 0.51136332\n",
      "Iteration 59, loss = 0.50825720\n",
      "Iteration 60, loss = 0.50523018\n",
      "Iteration 61, loss = 0.50227896\n",
      "Iteration 62, loss = 0.49940038\n",
      "Iteration 63, loss = 0.49659137\n",
      "Iteration 64, loss = 0.49384898\n",
      "Iteration 65, loss = 0.49117034\n",
      "Iteration 66, loss = 0.48855271\n",
      "Iteration 67, loss = 0.48599344\n",
      "Iteration 68, loss = 0.48349001\n",
      "Iteration 69, loss = 0.48104000\n",
      "Iteration 70, loss = 0.47864110\n",
      "Iteration 71, loss = 0.47629111\n",
      "Iteration 72, loss = 0.47398796\n",
      "Iteration 73, loss = 0.47172966\n",
      "Iteration 74, loss = 0.46951436\n",
      "Iteration 75, loss = 0.46734028\n",
      "Iteration 76, loss = 0.46520575\n",
      "Iteration 77, loss = 0.46310921\n",
      "Iteration 78, loss = 0.46104916\n",
      "Iteration 79, loss = 0.45902422\n",
      "Iteration 80, loss = 0.45703306\n",
      "Iteration 81, loss = 0.45507444\n",
      "Iteration 82, loss = 0.45314718\n",
      "Iteration 83, loss = 0.45125017\n",
      "Iteration 84, loss = 0.44938237\n",
      "Iteration 85, loss = 0.44754276\n",
      "Iteration 86, loss = 0.44573042\n",
      "Iteration 87, loss = 0.44394443\n",
      "Iteration 88, loss = 0.44218396\n",
      "Iteration 89, loss = 0.44044817\n",
      "Iteration 90, loss = 0.43873629\n",
      "Iteration 91, loss = 0.43704759\n",
      "Iteration 92, loss = 0.43538135\n",
      "Iteration 93, loss = 0.43373689\n",
      "Iteration 94, loss = 0.43211355\n",
      "Iteration 95, loss = 0.43051072\n",
      "Iteration 96, loss = 0.42892779\n",
      "Iteration 97, loss = 0.42736419\n",
      "Iteration 98, loss = 0.42581935\n",
      "Iteration 99, loss = 0.42429276\n",
      "Iteration 100, loss = 0.42278389\n",
      "Iteration 101, loss = 0.42129225\n",
      "Iteration 102, loss = 0.41981736\n",
      "Iteration 103, loss = 0.41835878\n",
      "Iteration 104, loss = 0.41691606\n",
      "Iteration 105, loss = 0.41548878\n",
      "Iteration 106, loss = 0.41407652\n",
      "Iteration 107, loss = 0.41267890\n",
      "Iteration 108, loss = 0.41129555\n",
      "Iteration 109, loss = 0.40992609\n",
      "Iteration 110, loss = 0.40857017\n",
      "Iteration 111, loss = 0.40722746\n",
      "Iteration 112, loss = 0.40589762\n",
      "Iteration 113, loss = 0.40458036\n",
      "Iteration 114, loss = 0.40327535\n",
      "Iteration 115, loss = 0.40198231\n",
      "Iteration 116, loss = 0.40070096\n",
      "Iteration 117, loss = 0.39943102\n",
      "Iteration 118, loss = 0.39817223\n",
      "Iteration 119, loss = 0.39692433\n",
      "Iteration 120, loss = 0.39568707\n",
      "Iteration 121, loss = 0.39446022\n",
      "Iteration 122, loss = 0.39324355\n",
      "Iteration 123, loss = 0.39203682\n",
      "Iteration 124, loss = 0.39083983\n",
      "Iteration 125, loss = 0.38965235\n",
      "Iteration 126, loss = 0.38847420\n",
      "Iteration 127, loss = 0.38730516\n",
      "Iteration 128, loss = 0.38614505\n",
      "Iteration 129, loss = 0.38499368\n",
      "Iteration 130, loss = 0.38385087\n",
      "Iteration 131, loss = 0.38271644\n",
      "Iteration 132, loss = 0.38159023\n",
      "Iteration 133, loss = 0.38047206\n",
      "Iteration 134, loss = 0.37936177\n",
      "Iteration 135, loss = 0.37825921\n",
      "Iteration 136, loss = 0.37716422\n",
      "Iteration 137, loss = 0.37607666\n",
      "Iteration 138, loss = 0.37499639\n",
      "Iteration 139, loss = 0.37392325\n",
      "Iteration 140, loss = 0.37285712\n",
      "Iteration 141, loss = 0.37179785\n",
      "Iteration 142, loss = 0.37074534\n",
      "Iteration 143, loss = 0.36969943\n",
      "Iteration 144, loss = 0.36866002\n",
      "Iteration 145, loss = 0.36762699\n",
      "Iteration 146, loss = 0.36660021\n",
      "Iteration 147, loss = 0.36557957\n",
      "Iteration 148, loss = 0.36456497\n",
      "Iteration 149, loss = 0.36355630\n",
      "Iteration 150, loss = 0.36255345\n",
      "Iteration 151, loss = 0.36155631\n",
      "Iteration 152, loss = 0.36056480\n",
      "Iteration 153, loss = 0.35957881\n",
      "Iteration 154, loss = 0.35859825\n",
      "Iteration 155, loss = 0.35762302\n",
      "Iteration 156, loss = 0.35665304\n",
      "Iteration 157, loss = 0.35568822\n",
      "Iteration 158, loss = 0.35472847\n",
      "Iteration 159, loss = 0.35377371\n",
      "Iteration 160, loss = 0.35282385\n",
      "Iteration 161, loss = 0.35187883\n",
      "Iteration 162, loss = 0.35093855\n",
      "Iteration 163, loss = 0.35000295\n",
      "Iteration 164, loss = 0.34907195\n",
      "Iteration 165, loss = 0.34814548\n",
      "Iteration 166, loss = 0.34722347\n",
      "Iteration 167, loss = 0.34630584\n",
      "Iteration 168, loss = 0.34539254\n",
      "Iteration 169, loss = 0.34448350\n",
      "Iteration 170, loss = 0.34357864\n",
      "Iteration 171, loss = 0.34267792\n",
      "Iteration 172, loss = 0.34178127\n",
      "Iteration 173, loss = 0.34088863\n",
      "Iteration 174, loss = 0.33999993\n",
      "Iteration 175, loss = 0.33911514\n",
      "Iteration 176, loss = 0.33823418\n",
      "Iteration 177, loss = 0.33735700\n",
      "Iteration 178, loss = 0.33648356\n",
      "Iteration 179, loss = 0.33561379\n",
      "Iteration 180, loss = 0.33474765\n",
      "Iteration 181, loss = 0.33388510\n",
      "Iteration 182, loss = 0.33302607\n",
      "Iteration 183, loss = 0.33217053\n",
      "Iteration 184, loss = 0.33131842\n",
      "Iteration 185, loss = 0.33046971\n",
      "Iteration 186, loss = 0.32962434\n",
      "Iteration 187, loss = 0.32878228\n",
      "Iteration 188, loss = 0.32794348\n",
      "Iteration 189, loss = 0.32710790\n",
      "Iteration 190, loss = 0.32627550\n",
      "Iteration 191, loss = 0.32544625\n",
      "Iteration 192, loss = 0.32462009\n",
      "Iteration 193, loss = 0.32379700\n",
      "Iteration 194, loss = 0.32297694\n",
      "Iteration 195, loss = 0.32215988\n",
      "Iteration 196, loss = 0.32134577\n",
      "Iteration 197, loss = 0.32053458\n",
      "Iteration 198, loss = 0.31972628\n",
      "Iteration 199, loss = 0.31892084\n",
      "Iteration 200, loss = 0.31811822\n",
      "Iteration 201, loss = 0.31731840\n",
      "Iteration 202, loss = 0.31652133\n",
      "Iteration 203, loss = 0.31572700\n",
      "Iteration 204, loss = 0.31493538\n",
      "Iteration 205, loss = 0.31414642\n",
      "Iteration 206, loss = 0.31336011\n",
      "Iteration 207, loss = 0.31257642\n",
      "Iteration 208, loss = 0.31179532\n",
      "Iteration 209, loss = 0.31101679\n",
      "Iteration 210, loss = 0.31024080\n",
      "Iteration 211, loss = 0.30946732\n",
      "Iteration 212, loss = 0.30869633\n",
      "Iteration 213, loss = 0.30792781\n",
      "Iteration 214, loss = 0.30716173\n",
      "Iteration 215, loss = 0.30639807\n",
      "Iteration 216, loss = 0.30563681\n",
      "Iteration 217, loss = 0.30487792\n",
      "Iteration 218, loss = 0.30412139\n",
      "Iteration 219, loss = 0.30336720\n",
      "Iteration 220, loss = 0.30261531\n",
      "Iteration 221, loss = 0.30186572\n",
      "Iteration 222, loss = 0.30111841\n",
      "Iteration 223, loss = 0.30037334\n",
      "Iteration 224, loss = 0.29963052\n",
      "Iteration 225, loss = 0.29888991\n",
      "Iteration 226, loss = 0.29815151\n",
      "Iteration 227, loss = 0.29741528\n",
      "Iteration 228, loss = 0.29668123\n",
      "Iteration 229, loss = 0.29594932\n",
      "Iteration 230, loss = 0.29521955\n",
      "Iteration 231, loss = 0.29449190\n",
      "Iteration 232, loss = 0.29376634\n",
      "Iteration 233, loss = 0.29304288\n",
      "Iteration 234, loss = 0.29232149\n",
      "Iteration 235, loss = 0.29160216\n",
      "Iteration 236, loss = 0.29088487\n",
      "Iteration 237, loss = 0.29016962\n",
      "Iteration 238, loss = 0.28945638\n",
      "Iteration 239, loss = 0.28874515\n",
      "Iteration 240, loss = 0.28803591\n",
      "Iteration 241, loss = 0.28732865\n",
      "Iteration 242, loss = 0.28662336\n",
      "Iteration 243, loss = 0.28592003\n",
      "Iteration 244, loss = 0.28521864\n",
      "Iteration 245, loss = 0.28451919\n",
      "Iteration 246, loss = 0.28382165\n",
      "Iteration 247, loss = 0.28312604\n",
      "Iteration 248, loss = 0.28243232\n",
      "Iteration 249, loss = 0.28174050\n",
      "Iteration 250, loss = 0.28105056\n",
      "Iteration 251, loss = 0.28036249\n",
      "Iteration 252, loss = 0.27967629\n",
      "Iteration 253, loss = 0.27899194\n",
      "Iteration 254, loss = 0.27830944\n",
      "Iteration 255, loss = 0.27762877\n",
      "Iteration 256, loss = 0.27694994\n",
      "Iteration 257, loss = 0.27627292\n",
      "Iteration 258, loss = 0.27559772\n",
      "Iteration 259, loss = 0.27492433\n",
      "Iteration 260, loss = 0.27425273\n",
      "Iteration 261, loss = 0.27358293\n",
      "Iteration 262, loss = 0.27291490\n",
      "Iteration 263, loss = 0.27224866\n",
      "Iteration 264, loss = 0.27158419\n",
      "Iteration 265, loss = 0.27092148\n",
      "Iteration 266, loss = 0.27026053\n",
      "Iteration 267, loss = 0.26960133\n",
      "Iteration 268, loss = 0.26894388\n",
      "Iteration 269, loss = 0.26828816\n",
      "Iteration 270, loss = 0.26763419\n",
      "Iteration 271, loss = 0.26698194\n",
      "Iteration 272, loss = 0.26633141\n",
      "Iteration 273, loss = 0.26568261\n",
      "Iteration 274, loss = 0.26503552\n",
      "Iteration 275, loss = 0.26439014\n",
      "Iteration 276, loss = 0.26374646\n",
      "Iteration 277, loss = 0.26310449\n",
      "Iteration 278, loss = 0.26246421\n",
      "Iteration 279, loss = 0.26182562\n",
      "Iteration 280, loss = 0.26118872\n",
      "Iteration 281, loss = 0.26055351\n",
      "Iteration 282, loss = 0.25991997\n",
      "Iteration 283, loss = 0.25928812\n",
      "Iteration 284, loss = 0.25865793\n",
      "Iteration 285, loss = 0.25802942\n",
      "Iteration 286, loss = 0.25740257\n",
      "Iteration 287, loss = 0.25677738\n",
      "Iteration 288, loss = 0.25615386\n",
      "Iteration 289, loss = 0.25553199\n",
      "Iteration 290, loss = 0.25491178\n",
      "Iteration 291, loss = 0.25429322\n",
      "Iteration 292, loss = 0.25367631\n",
      "Iteration 293, loss = 0.25306105\n",
      "Iteration 294, loss = 0.25244743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 295, loss = 0.25183545\n",
      "Iteration 296, loss = 0.25122511\n",
      "Iteration 297, loss = 0.25061641\n",
      "Iteration 298, loss = 0.25000934\n",
      "Iteration 299, loss = 0.24940391\n",
      "Iteration 300, loss = 0.24880011\n",
      "Iteration 301, loss = 0.24819794\n",
      "Iteration 302, loss = 0.24759740\n",
      "Iteration 303, loss = 0.24699848\n",
      "Iteration 304, loss = 0.24640119\n",
      "Iteration 305, loss = 0.24580552\n",
      "Iteration 306, loss = 0.24521147\n",
      "Iteration 307, loss = 0.24461904\n",
      "Iteration 308, loss = 0.24402823\n",
      "Iteration 309, loss = 0.24343904\n",
      "Iteration 310, loss = 0.24285147\n",
      "Iteration 311, loss = 0.24226550\n",
      "Iteration 312, loss = 0.24168116\n",
      "Iteration 313, loss = 0.24109842\n",
      "Iteration 314, loss = 0.24051730\n",
      "Iteration 315, loss = 0.23993778\n",
      "Iteration 316, loss = 0.23935988\n",
      "Iteration 317, loss = 0.23878358\n",
      "Iteration 318, loss = 0.23820890\n",
      "Iteration 319, loss = 0.23763582\n",
      "Iteration 320, loss = 0.23706434\n",
      "Iteration 321, loss = 0.23649447\n",
      "Iteration 322, loss = 0.23592621\n",
      "Iteration 323, loss = 0.23535955\n",
      "Iteration 324, loss = 0.23479449\n",
      "Iteration 325, loss = 0.23423103\n",
      "Iteration 326, loss = 0.23366918\n",
      "Iteration 327, loss = 0.23310892\n",
      "Iteration 328, loss = 0.23255027\n",
      "Iteration 329, loss = 0.23199322\n",
      "Iteration 330, loss = 0.23143776\n",
      "Iteration 331, loss = 0.23088391\n",
      "Iteration 332, loss = 0.23033165\n",
      "Iteration 333, loss = 0.22978099\n",
      "Iteration 334, loss = 0.22923193\n",
      "Iteration 335, loss = 0.22868447\n",
      "Iteration 336, loss = 0.22813860\n",
      "Iteration 337, loss = 0.22759432\n",
      "Iteration 338, loss = 0.22705165\n",
      "Iteration 339, loss = 0.22651056\n",
      "Iteration 340, loss = 0.22597107\n",
      "Iteration 341, loss = 0.22543318\n",
      "Iteration 342, loss = 0.22489688\n",
      "Iteration 343, loss = 0.22436217\n",
      "Iteration 344, loss = 0.22382905\n",
      "Iteration 345, loss = 0.22329753\n",
      "Iteration 346, loss = 0.22276760\n",
      "Iteration 347, loss = 0.22223926\n",
      "Iteration 348, loss = 0.22171251\n",
      "Iteration 349, loss = 0.22118735\n",
      "Iteration 350, loss = 0.22066378\n",
      "Iteration 351, loss = 0.22014180\n",
      "Iteration 352, loss = 0.21962140\n",
      "Iteration 353, loss = 0.21910260\n",
      "Iteration 354, loss = 0.21858538\n",
      "Iteration 355, loss = 0.21806975\n",
      "Iteration 356, loss = 0.21755571\n",
      "Iteration 357, loss = 0.21704325\n",
      "Iteration 358, loss = 0.21653238\n",
      "Iteration 359, loss = 0.21602309\n",
      "Iteration 360, loss = 0.21551539\n",
      "Iteration 361, loss = 0.21500927\n",
      "Iteration 362, loss = 0.21450473\n",
      "Iteration 363, loss = 0.21400177\n",
      "Iteration 364, loss = 0.21350040\n",
      "Iteration 365, loss = 0.21300060\n",
      "Iteration 366, loss = 0.21250239\n",
      "Iteration 367, loss = 0.21200575\n",
      "Iteration 368, loss = 0.21151069\n",
      "Iteration 369, loss = 0.21101721\n",
      "Iteration 370, loss = 0.21052531\n",
      "Iteration 371, loss = 0.21003498\n",
      "Iteration 372, loss = 0.20954623\n",
      "Iteration 373, loss = 0.20905904\n",
      "Iteration 374, loss = 0.20857344\n",
      "Iteration 375, loss = 0.20808940\n",
      "Iteration 376, loss = 0.20760693\n",
      "Iteration 377, loss = 0.20712604\n",
      "Iteration 378, loss = 0.20664671\n",
      "Iteration 379, loss = 0.20616895\n",
      "Iteration 380, loss = 0.20569275\n",
      "Iteration 381, loss = 0.20521812\n",
      "Iteration 382, loss = 0.20474506\n",
      "Iteration 383, loss = 0.20427355\n",
      "Iteration 384, loss = 0.20380361\n",
      "Iteration 385, loss = 0.20333523\n",
      "Iteration 386, loss = 0.20286840\n",
      "Iteration 387, loss = 0.20240313\n",
      "Iteration 388, loss = 0.20193942\n",
      "Iteration 389, loss = 0.20147726\n",
      "Iteration 390, loss = 0.20101666\n",
      "Iteration 391, loss = 0.20055760\n",
      "Iteration 392, loss = 0.20010010\n",
      "Iteration 393, loss = 0.19964414\n",
      "Iteration 394, loss = 0.19918973\n",
      "Iteration 395, loss = 0.19873686\n",
      "Iteration 396, loss = 0.19828554\n",
      "Iteration 397, loss = 0.19783576\n",
      "Iteration 398, loss = 0.19738752\n",
      "Iteration 399, loss = 0.19694081\n",
      "Iteration 400, loss = 0.19649564\n",
      "Iteration 401, loss = 0.19605201\n",
      "Iteration 402, loss = 0.19560990\n",
      "Iteration 403, loss = 0.19516933\n",
      "Iteration 404, loss = 0.19473028\n",
      "Iteration 405, loss = 0.19429276\n",
      "Iteration 406, loss = 0.19385676\n",
      "Iteration 407, loss = 0.19342229\n",
      "Iteration 408, loss = 0.19298933\n",
      "Iteration 409, loss = 0.19255789\n",
      "Iteration 410, loss = 0.19212797\n",
      "Iteration 411, loss = 0.19169956\n",
      "Iteration 412, loss = 0.19127266\n",
      "Iteration 413, loss = 0.19084727\n",
      "Iteration 414, loss = 0.19042338\n",
      "Iteration 415, loss = 0.19000100\n",
      "Iteration 416, loss = 0.18958011\n",
      "Iteration 417, loss = 0.18916073\n",
      "Iteration 418, loss = 0.18874284\n",
      "Iteration 419, loss = 0.18832645\n",
      "Iteration 420, loss = 0.18791154\n",
      "Iteration 421, loss = 0.18749813\n",
      "Iteration 422, loss = 0.18708620\n",
      "Iteration 423, loss = 0.18667575\n",
      "Iteration 424, loss = 0.18626679\n",
      "Iteration 425, loss = 0.18585930\n",
      "Iteration 426, loss = 0.18545328\n",
      "Iteration 427, loss = 0.18504874\n",
      "Iteration 428, loss = 0.18464566\n",
      "Iteration 429, loss = 0.18424406\n",
      "Iteration 430, loss = 0.18384391\n",
      "Iteration 431, loss = 0.18344523\n",
      "Iteration 432, loss = 0.18304800\n",
      "Iteration 433, loss = 0.18265222\n",
      "Iteration 434, loss = 0.18225790\n",
      "Iteration 435, loss = 0.18186503\n",
      "Iteration 436, loss = 0.18147360\n",
      "Iteration 437, loss = 0.18108361\n",
      "Iteration 438, loss = 0.18069506\n",
      "Iteration 439, loss = 0.18030795\n",
      "Iteration 440, loss = 0.17992227\n",
      "Iteration 441, loss = 0.17953801\n",
      "Iteration 442, loss = 0.17915518\n",
      "Iteration 443, loss = 0.17877378\n",
      "Iteration 444, loss = 0.17839379\n",
      "Iteration 445, loss = 0.17801522\n",
      "Iteration 446, loss = 0.17763806\n",
      "Iteration 447, loss = 0.17726231\n",
      "Iteration 448, loss = 0.17688797\n",
      "Iteration 449, loss = 0.17651503\n",
      "Iteration 450, loss = 0.17614348\n",
      "Iteration 451, loss = 0.17577333\n",
      "Iteration 452, loss = 0.17540458\n",
      "Iteration 453, loss = 0.17503720\n",
      "Iteration 454, loss = 0.17467122\n",
      "Iteration 455, loss = 0.17430661\n",
      "Iteration 456, loss = 0.17394338\n",
      "Iteration 457, loss = 0.17358152\n",
      "Iteration 458, loss = 0.17322104\n",
      "Iteration 459, loss = 0.17286191\n",
      "Iteration 460, loss = 0.17250415\n",
      "Iteration 461, loss = 0.17214775\n",
      "Iteration 462, loss = 0.17179270\n",
      "Iteration 463, loss = 0.17143901\n",
      "Iteration 464, loss = 0.17108666\n",
      "Iteration 465, loss = 0.17073565\n",
      "Iteration 466, loss = 0.17038598\n",
      "Iteration 467, loss = 0.17003765\n",
      "Iteration 468, loss = 0.16969064\n",
      "Iteration 469, loss = 0.16934497\n",
      "Iteration 470, loss = 0.16900062\n",
      "Iteration 471, loss = 0.16865758\n",
      "Iteration 472, loss = 0.16831587\n",
      "Iteration 473, loss = 0.16797546\n",
      "Iteration 474, loss = 0.16763636\n",
      "Iteration 475, loss = 0.16729857\n",
      "Iteration 476, loss = 0.16696207\n",
      "Iteration 477, loss = 0.16662687\n",
      "Iteration 478, loss = 0.16629296\n",
      "Iteration 479, loss = 0.16596034\n",
      "Iteration 480, loss = 0.16562900\n",
      "Iteration 481, loss = 0.16529894\n",
      "Iteration 482, loss = 0.16497015\n",
      "Iteration 483, loss = 0.16464264\n",
      "Iteration 484, loss = 0.16431639\n",
      "Iteration 485, loss = 0.16399141\n",
      "Iteration 486, loss = 0.16366768\n",
      "Iteration 487, loss = 0.16334520\n",
      "Iteration 488, loss = 0.16302398\n",
      "Iteration 489, loss = 0.16270400\n",
      "Iteration 490, loss = 0.16238527\n",
      "Iteration 491, loss = 0.16206777\n",
      "Iteration 492, loss = 0.16175150\n",
      "Iteration 493, loss = 0.16143647\n",
      "Iteration 494, loss = 0.16112265\n",
      "Iteration 495, loss = 0.16081006\n",
      "Iteration 496, loss = 0.16049868\n",
      "Iteration 497, loss = 0.16018852\n",
      "Iteration 498, loss = 0.15987956\n",
      "Iteration 499, loss = 0.15957181\n",
      "Iteration 500, loss = 0.15926526\n",
      "Iteration 1, loss = 1.43560514\n",
      "Iteration 2, loss = 1.41904800\n",
      "Iteration 3, loss = 1.39591177\n",
      "Iteration 4, loss = 1.36732918\n",
      "Iteration 5, loss = 1.33439275\n",
      "Iteration 6, loss = 1.29813231\n",
      "Iteration 7, loss = 1.25949900\n",
      "Iteration 8, loss = 1.21935457\n",
      "Iteration 9, loss = 1.17846508\n",
      "Iteration 10, loss = 1.13749820\n",
      "Iteration 11, loss = 1.09702325\n",
      "Iteration 12, loss = 1.05751358\n",
      "Iteration 13, loss = 1.01935074\n",
      "Iteration 14, loss = 0.98283006\n",
      "Iteration 15, loss = 0.94816740\n",
      "Iteration 16, loss = 0.91550682\n",
      "Iteration 17, loss = 0.88492890\n",
      "Iteration 18, loss = 0.85645947\n",
      "Iteration 19, loss = 0.83007845\n",
      "Iteration 20, loss = 0.80572865\n",
      "Iteration 21, loss = 0.78332405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.76275756\n",
      "Iteration 23, loss = 0.74390787\n",
      "Iteration 24, loss = 0.72664549\n",
      "Iteration 25, loss = 0.71083770\n",
      "Iteration 26, loss = 0.69635262\n",
      "Iteration 27, loss = 0.68306231\n",
      "Iteration 28, loss = 0.67084509\n",
      "Iteration 29, loss = 0.65958710\n",
      "Iteration 30, loss = 0.64918325\n",
      "Iteration 31, loss = 0.63953776\n",
      "Iteration 32, loss = 0.63056420\n",
      "Iteration 33, loss = 0.62218534\n",
      "Iteration 34, loss = 0.61433269\n",
      "Iteration 35, loss = 0.60694601\n",
      "Iteration 36, loss = 0.59997262\n",
      "Iteration 37, loss = 0.59336674\n",
      "Iteration 38, loss = 0.58708880\n",
      "Iteration 39, loss = 0.58110471\n",
      "Iteration 40, loss = 0.57538525\n",
      "Iteration 41, loss = 0.56990540\n",
      "Iteration 42, loss = 0.56464378\n",
      "Iteration 43, loss = 0.55958216\n",
      "Iteration 44, loss = 0.55470492\n",
      "Iteration 45, loss = 0.54999868\n",
      "Iteration 46, loss = 0.54545192\n",
      "Iteration 47, loss = 0.54105467\n",
      "Iteration 48, loss = 0.53679820\n",
      "Iteration 49, loss = 0.53267481\n",
      "Iteration 50, loss = 0.52867764\n",
      "Iteration 51, loss = 0.52480051\n",
      "Iteration 52, loss = 0.52103776\n",
      "Iteration 53, loss = 0.51738417\n",
      "Iteration 54, loss = 0.51383486\n",
      "Iteration 55, loss = 0.51038524\n",
      "Iteration 56, loss = 0.50703094\n",
      "Iteration 57, loss = 0.50376781\n",
      "Iteration 58, loss = 0.50059184\n",
      "Iteration 59, loss = 0.49749922\n",
      "Iteration 60, loss = 0.49448624\n",
      "Iteration 61, loss = 0.49154936\n",
      "Iteration 62, loss = 0.48868517\n",
      "Iteration 63, loss = 0.48589039\n",
      "Iteration 64, loss = 0.48316189\n",
      "Iteration 65, loss = 0.48049666\n",
      "Iteration 66, loss = 0.47789186\n",
      "Iteration 67, loss = 0.47534476\n",
      "Iteration 68, loss = 0.47285278\n",
      "Iteration 69, loss = 0.47041348\n",
      "Iteration 70, loss = 0.46802456\n",
      "Iteration 71, loss = 0.46568385\n",
      "Iteration 72, loss = 0.46338929\n",
      "Iteration 73, loss = 0.46113896\n",
      "Iteration 74, loss = 0.45893105\n",
      "Iteration 75, loss = 0.45676386\n",
      "Iteration 76, loss = 0.45463579\n",
      "Iteration 77, loss = 0.45254534\n",
      "Iteration 78, loss = 0.45049109\n",
      "Iteration 79, loss = 0.44847172\n",
      "Iteration 80, loss = 0.44648596\n",
      "Iteration 81, loss = 0.44453263\n",
      "Iteration 82, loss = 0.44261061\n",
      "Iteration 83, loss = 0.44071883\n",
      "Iteration 84, loss = 0.43885627\n",
      "Iteration 85, loss = 0.43702198\n",
      "Iteration 86, loss = 0.43521504\n",
      "Iteration 87, loss = 0.43343457\n",
      "Iteration 88, loss = 0.43167973\n",
      "Iteration 89, loss = 0.42994972\n",
      "Iteration 90, loss = 0.42824378\n",
      "Iteration 91, loss = 0.42656115\n",
      "Iteration 92, loss = 0.42490114\n",
      "Iteration 93, loss = 0.42326306\n",
      "Iteration 94, loss = 0.42164626\n",
      "Iteration 95, loss = 0.42005011\n",
      "Iteration 96, loss = 0.41847400\n",
      "Iteration 97, loss = 0.41691735\n",
      "Iteration 98, loss = 0.41537961\n",
      "Iteration 99, loss = 0.41386023\n",
      "Iteration 100, loss = 0.41235869\n",
      "Iteration 101, loss = 0.41087451\n",
      "Iteration 102, loss = 0.40940720\n",
      "Iteration 103, loss = 0.40795630\n",
      "Iteration 104, loss = 0.40652137\n",
      "Iteration 105, loss = 0.40510199\n",
      "Iteration 106, loss = 0.40369774\n",
      "Iteration 107, loss = 0.40230824\n",
      "Iteration 108, loss = 0.40093310\n",
      "Iteration 109, loss = 0.39957197\n",
      "Iteration 110, loss = 0.39822449\n",
      "Iteration 111, loss = 0.39689033\n",
      "Iteration 112, loss = 0.39556916\n",
      "Iteration 113, loss = 0.39426066\n",
      "Iteration 114, loss = 0.39296455\n",
      "Iteration 115, loss = 0.39168051\n",
      "Iteration 116, loss = 0.39040829\n",
      "Iteration 117, loss = 0.38914760\n",
      "Iteration 118, loss = 0.38789818\n",
      "Iteration 119, loss = 0.38665977\n",
      "Iteration 120, loss = 0.38543214\n",
      "Iteration 121, loss = 0.38421505\n",
      "Iteration 122, loss = 0.38300826\n",
      "Iteration 123, loss = 0.38181155\n",
      "Iteration 124, loss = 0.38062471\n",
      "Iteration 125, loss = 0.37944753\n",
      "Iteration 126, loss = 0.37827980\n",
      "Iteration 127, loss = 0.37712134\n",
      "Iteration 128, loss = 0.37597194\n",
      "Iteration 129, loss = 0.37483142\n",
      "Iteration 130, loss = 0.37369960\n",
      "Iteration 131, loss = 0.37257631\n",
      "Iteration 132, loss = 0.37146138\n",
      "Iteration 133, loss = 0.37035465\n",
      "Iteration 134, loss = 0.36925594\n",
      "Iteration 135, loss = 0.36816511\n",
      "Iteration 136, loss = 0.36708201\n",
      "Iteration 137, loss = 0.36600648\n",
      "Iteration 138, loss = 0.36493839\n",
      "Iteration 139, loss = 0.36387760\n",
      "Iteration 140, loss = 0.36282396\n",
      "Iteration 141, loss = 0.36177735\n",
      "Iteration 142, loss = 0.36073764\n",
      "Iteration 143, loss = 0.35970470\n",
      "Iteration 144, loss = 0.35867842\n",
      "Iteration 145, loss = 0.35765867\n",
      "Iteration 146, loss = 0.35664534\n",
      "Iteration 147, loss = 0.35563832\n",
      "Iteration 148, loss = 0.35463749\n",
      "Iteration 149, loss = 0.35364275\n",
      "Iteration 150, loss = 0.35265400\n",
      "Iteration 151, loss = 0.35167113\n",
      "Iteration 152, loss = 0.35069405\n",
      "Iteration 153, loss = 0.34972266\n",
      "Iteration 154, loss = 0.34875687\n",
      "Iteration 155, loss = 0.34779658\n",
      "Iteration 156, loss = 0.34684171\n",
      "Iteration 157, loss = 0.34589216\n",
      "Iteration 158, loss = 0.34494786\n",
      "Iteration 159, loss = 0.34400872\n",
      "Iteration 160, loss = 0.34307466\n",
      "Iteration 161, loss = 0.34214560\n",
      "Iteration 162, loss = 0.34122147\n",
      "Iteration 163, loss = 0.34030218\n",
      "Iteration 164, loss = 0.33938767\n",
      "Iteration 165, loss = 0.33847787\n",
      "Iteration 166, loss = 0.33757270\n",
      "Iteration 167, loss = 0.33667210\n",
      "Iteration 168, loss = 0.33577599\n",
      "Iteration 169, loss = 0.33488433\n",
      "Iteration 170, loss = 0.33399703\n",
      "Iteration 171, loss = 0.33311404\n",
      "Iteration 172, loss = 0.33223531\n",
      "Iteration 173, loss = 0.33136076\n",
      "Iteration 174, loss = 0.33049035\n",
      "Iteration 175, loss = 0.32962401\n",
      "Iteration 176, loss = 0.32876169\n",
      "Iteration 177, loss = 0.32790334\n",
      "Iteration 178, loss = 0.32704890\n",
      "Iteration 179, loss = 0.32619833\n",
      "Iteration 180, loss = 0.32535157\n",
      "Iteration 181, loss = 0.32450857\n",
      "Iteration 182, loss = 0.32366929\n",
      "Iteration 183, loss = 0.32283368\n",
      "Iteration 184, loss = 0.32200169\n",
      "Iteration 185, loss = 0.32117328\n",
      "Iteration 186, loss = 0.32034841\n",
      "Iteration 187, loss = 0.31952702\n",
      "Iteration 188, loss = 0.31870909\n",
      "Iteration 189, loss = 0.31789457\n",
      "Iteration 190, loss = 0.31708341\n",
      "Iteration 191, loss = 0.31627559\n",
      "Iteration 192, loss = 0.31547105\n",
      "Iteration 193, loss = 0.31466977\n",
      "Iteration 194, loss = 0.31387171\n",
      "Iteration 195, loss = 0.31307683\n",
      "Iteration 196, loss = 0.31228510\n",
      "Iteration 197, loss = 0.31149649\n",
      "Iteration 198, loss = 0.31071095\n",
      "Iteration 199, loss = 0.30992846\n",
      "Iteration 200, loss = 0.30914898\n",
      "Iteration 201, loss = 0.30837249\n",
      "Iteration 202, loss = 0.30759895\n",
      "Iteration 203, loss = 0.30682834\n",
      "Iteration 204, loss = 0.30606062\n",
      "Iteration 205, loss = 0.30529576\n",
      "Iteration 206, loss = 0.30453374\n",
      "Iteration 207, loss = 0.30377454\n",
      "Iteration 208, loss = 0.30301812\n",
      "Iteration 209, loss = 0.30226445\n",
      "Iteration 210, loss = 0.30151352\n",
      "Iteration 211, loss = 0.30076529\n",
      "Iteration 212, loss = 0.30001975\n",
      "Iteration 213, loss = 0.29927687\n",
      "Iteration 214, loss = 0.29853662\n",
      "Iteration 215, loss = 0.29779898\n",
      "Iteration 216, loss = 0.29706394\n",
      "Iteration 217, loss = 0.29633146\n",
      "Iteration 218, loss = 0.29560153\n",
      "Iteration 219, loss = 0.29487412\n",
      "Iteration 220, loss = 0.29414923\n",
      "Iteration 221, loss = 0.29342681\n",
      "Iteration 222, loss = 0.29270687\n",
      "Iteration 223, loss = 0.29198937\n",
      "Iteration 224, loss = 0.29127430\n",
      "Iteration 225, loss = 0.29056163\n",
      "Iteration 226, loss = 0.28985136\n",
      "Iteration 227, loss = 0.28914347\n",
      "Iteration 228, loss = 0.28843793\n",
      "Iteration 229, loss = 0.28773474\n",
      "Iteration 230, loss = 0.28703387\n",
      "Iteration 231, loss = 0.28633530\n",
      "Iteration 232, loss = 0.28563903\n",
      "Iteration 233, loss = 0.28494504\n",
      "Iteration 234, loss = 0.28425331\n",
      "Iteration 235, loss = 0.28356383\n",
      "Iteration 236, loss = 0.28287659\n",
      "Iteration 237, loss = 0.28219156\n",
      "Iteration 238, loss = 0.28150874\n",
      "Iteration 239, loss = 0.28082811\n",
      "Iteration 240, loss = 0.28014967\n",
      "Iteration 241, loss = 0.27947339\n",
      "Iteration 242, loss = 0.27879927\n",
      "Iteration 243, loss = 0.27812729\n",
      "Iteration 244, loss = 0.27745744\n",
      "Iteration 245, loss = 0.27678971\n",
      "Iteration 246, loss = 0.27612409\n",
      "Iteration 247, loss = 0.27546057\n",
      "Iteration 248, loss = 0.27479914\n",
      "Iteration 249, loss = 0.27413978\n",
      "Iteration 250, loss = 0.27348248\n",
      "Iteration 251, loss = 0.27282725\n",
      "Iteration 252, loss = 0.27217406\n",
      "Iteration 253, loss = 0.27152290\n",
      "Iteration 254, loss = 0.27087378\n",
      "Iteration 255, loss = 0.27022667\n",
      "Iteration 256, loss = 0.26958157\n",
      "Iteration 257, loss = 0.26893847\n",
      "Iteration 258, loss = 0.26829736\n",
      "Iteration 259, loss = 0.26765824\n",
      "Iteration 260, loss = 0.26702109\n",
      "Iteration 261, loss = 0.26638591\n",
      "Iteration 262, loss = 0.26575269\n",
      "Iteration 263, loss = 0.26512143\n",
      "Iteration 264, loss = 0.26449211\n",
      "Iteration 265, loss = 0.26386472\n",
      "Iteration 266, loss = 0.26323927\n",
      "Iteration 267, loss = 0.26261574\n",
      "Iteration 268, loss = 0.26199413\n",
      "Iteration 269, loss = 0.26137444\n",
      "Iteration 270, loss = 0.26075664\n",
      "Iteration 271, loss = 0.26014075\n",
      "Iteration 272, loss = 0.25952674\n",
      "Iteration 273, loss = 0.25891463\n",
      "Iteration 274, loss = 0.25830439\n",
      "Iteration 275, loss = 0.25769603\n",
      "Iteration 276, loss = 0.25708954\n",
      "Iteration 277, loss = 0.25648492\n",
      "Iteration 278, loss = 0.25588215\n",
      "Iteration 279, loss = 0.25528124\n",
      "Iteration 280, loss = 0.25468218\n",
      "Iteration 281, loss = 0.25408496\n",
      "Iteration 282, loss = 0.25348959\n",
      "Iteration 283, loss = 0.25289604\n",
      "Iteration 284, loss = 0.25230433\n",
      "Iteration 285, loss = 0.25171444\n",
      "Iteration 286, loss = 0.25112638\n",
      "Iteration 287, loss = 0.25054014\n",
      "Iteration 288, loss = 0.24995570\n",
      "Iteration 289, loss = 0.24937308\n",
      "Iteration 290, loss = 0.24879226\n",
      "Iteration 291, loss = 0.24821325\n",
      "Iteration 292, loss = 0.24763603\n",
      "Iteration 293, loss = 0.24706061\n",
      "Iteration 294, loss = 0.24648698\n",
      "Iteration 295, loss = 0.24591514\n",
      "Iteration 296, loss = 0.24534508\n",
      "Iteration 297, loss = 0.24477680\n",
      "Iteration 298, loss = 0.24421030\n",
      "Iteration 299, loss = 0.24364558\n",
      "Iteration 300, loss = 0.24308262\n",
      "Iteration 301, loss = 0.24252144\n",
      "Iteration 302, loss = 0.24196202\n",
      "Iteration 303, loss = 0.24140436\n",
      "Iteration 304, loss = 0.24084846\n",
      "Iteration 305, loss = 0.24029432\n",
      "Iteration 306, loss = 0.23974193\n",
      "Iteration 307, loss = 0.23919130\n",
      "Iteration 308, loss = 0.23864241\n",
      "Iteration 309, loss = 0.23809527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 310, loss = 0.23754988\n",
      "Iteration 311, loss = 0.23700622\n",
      "Iteration 312, loss = 0.23646431\n",
      "Iteration 313, loss = 0.23592413\n",
      "Iteration 314, loss = 0.23538568\n",
      "Iteration 315, loss = 0.23484897\n",
      "Iteration 316, loss = 0.23431398\n",
      "Iteration 317, loss = 0.23378073\n",
      "Iteration 318, loss = 0.23324919\n",
      "Iteration 319, loss = 0.23271938\n",
      "Iteration 320, loss = 0.23219129\n",
      "Iteration 321, loss = 0.23166492\n",
      "Iteration 322, loss = 0.23114027\n",
      "Iteration 323, loss = 0.23061733\n",
      "Iteration 324, loss = 0.23009610\n",
      "Iteration 325, loss = 0.22957658\n",
      "Iteration 326, loss = 0.22905877\n",
      "Iteration 327, loss = 0.22854267\n",
      "Iteration 328, loss = 0.22802827\n",
      "Iteration 329, loss = 0.22751557\n",
      "Iteration 330, loss = 0.22700457\n",
      "Iteration 331, loss = 0.22649527\n",
      "Iteration 332, loss = 0.22598767\n",
      "Iteration 333, loss = 0.22548176\n",
      "Iteration 334, loss = 0.22497755\n",
      "Iteration 335, loss = 0.22447502\n",
      "Iteration 336, loss = 0.22397418\n",
      "Iteration 337, loss = 0.22347503\n",
      "Iteration 338, loss = 0.22297757\n",
      "Iteration 339, loss = 0.22248178\n",
      "Iteration 340, loss = 0.22198768\n",
      "Iteration 341, loss = 0.22149526\n",
      "Iteration 342, loss = 0.22100451\n",
      "Iteration 343, loss = 0.22051544\n",
      "Iteration 344, loss = 0.22002804\n",
      "Iteration 345, loss = 0.21954231\n",
      "Iteration 346, loss = 0.21905826\n",
      "Iteration 347, loss = 0.21857587\n",
      "Iteration 348, loss = 0.21809514\n",
      "Iteration 349, loss = 0.21761608\n",
      "Iteration 350, loss = 0.21713869\n",
      "Iteration 351, loss = 0.21666295\n",
      "Iteration 352, loss = 0.21618887\n",
      "Iteration 353, loss = 0.21571644\n",
      "Iteration 354, loss = 0.21524567\n",
      "Iteration 355, loss = 0.21477655\n",
      "Iteration 356, loss = 0.21430908\n",
      "Iteration 357, loss = 0.21384326\n",
      "Iteration 358, loss = 0.21337908\n",
      "Iteration 359, loss = 0.21291655\n",
      "Iteration 360, loss = 0.21245566\n",
      "Iteration 361, loss = 0.21199641\n",
      "Iteration 362, loss = 0.21153879\n",
      "Iteration 363, loss = 0.21108281\n",
      "Iteration 364, loss = 0.21062847\n",
      "Iteration 365, loss = 0.21017575\n",
      "Iteration 366, loss = 0.20972467\n",
      "Iteration 367, loss = 0.20927521\n",
      "Iteration 368, loss = 0.20882737\n",
      "Iteration 369, loss = 0.20838116\n",
      "Iteration 370, loss = 0.20793656\n",
      "Iteration 371, loss = 0.20749359\n",
      "Iteration 372, loss = 0.20705222\n",
      "Iteration 373, loss = 0.20661247\n",
      "Iteration 374, loss = 0.20617434\n",
      "Iteration 375, loss = 0.20573780\n",
      "Iteration 376, loss = 0.20530288\n",
      "Iteration 377, loss = 0.20486956\n",
      "Iteration 378, loss = 0.20443783\n",
      "Iteration 379, loss = 0.20400771\n",
      "Iteration 380, loss = 0.20357918\n",
      "Iteration 381, loss = 0.20315225\n",
      "Iteration 382, loss = 0.20272690\n",
      "Iteration 383, loss = 0.20230315\n",
      "Iteration 384, loss = 0.20188097\n",
      "Iteration 385, loss = 0.20146038\n",
      "Iteration 386, loss = 0.20104137\n",
      "Iteration 387, loss = 0.20062394\n",
      "Iteration 388, loss = 0.20020808\n",
      "Iteration 389, loss = 0.19979380\n",
      "Iteration 390, loss = 0.19938108\n",
      "Iteration 391, loss = 0.19896993\n",
      "Iteration 392, loss = 0.19856034\n",
      "Iteration 393, loss = 0.19815231\n",
      "Iteration 394, loss = 0.19774584\n",
      "Iteration 395, loss = 0.19734092\n",
      "Iteration 396, loss = 0.19693756\n",
      "Iteration 397, loss = 0.19653574\n",
      "Iteration 398, loss = 0.19613547\n",
      "Iteration 399, loss = 0.19573674\n",
      "Iteration 400, loss = 0.19533955\n",
      "Iteration 401, loss = 0.19494390\n",
      "Iteration 402, loss = 0.19454978\n",
      "Iteration 403, loss = 0.19415719\n",
      "Iteration 404, loss = 0.19376613\n",
      "Iteration 405, loss = 0.19337659\n",
      "Iteration 406, loss = 0.19298857\n",
      "Iteration 407, loss = 0.19260207\n",
      "Iteration 408, loss = 0.19221708\n",
      "Iteration 409, loss = 0.19183360\n",
      "Iteration 410, loss = 0.19145163\n",
      "Iteration 411, loss = 0.19107117\n",
      "Iteration 412, loss = 0.19069220\n",
      "Iteration 413, loss = 0.19031473\n",
      "Iteration 414, loss = 0.18993876\n",
      "Iteration 415, loss = 0.18956427\n",
      "Iteration 416, loss = 0.18919127\n",
      "Iteration 417, loss = 0.18881976\n",
      "Iteration 418, loss = 0.18844972\n",
      "Iteration 419, loss = 0.18808116\n",
      "Iteration 420, loss = 0.18771408\n",
      "Iteration 421, loss = 0.18734846\n",
      "Iteration 422, loss = 0.18698430\n",
      "Iteration 423, loss = 0.18662161\n",
      "Iteration 424, loss = 0.18626037\n",
      "Iteration 425, loss = 0.18590059\n",
      "Iteration 426, loss = 0.18554226\n",
      "Iteration 427, loss = 0.18518538\n",
      "Iteration 428, loss = 0.18482994\n",
      "Iteration 429, loss = 0.18447593\n",
      "Iteration 430, loss = 0.18412337\n",
      "Iteration 431, loss = 0.18377223\n",
      "Iteration 432, loss = 0.18342252\n",
      "Iteration 433, loss = 0.18307423\n",
      "Iteration 434, loss = 0.18272737\n",
      "Iteration 435, loss = 0.18238192\n",
      "Iteration 436, loss = 0.18203788\n",
      "Iteration 437, loss = 0.18169525\n",
      "Iteration 438, loss = 0.18135402\n",
      "Iteration 439, loss = 0.18101419\n",
      "Iteration 440, loss = 0.18067576\n",
      "Iteration 441, loss = 0.18033872\n",
      "Iteration 442, loss = 0.18000306\n",
      "Iteration 443, loss = 0.17966879\n",
      "Iteration 444, loss = 0.17933590\n",
      "Iteration 445, loss = 0.17900438\n",
      "Iteration 446, loss = 0.17867424\n",
      "Iteration 447, loss = 0.17834546\n",
      "Iteration 448, loss = 0.17801804\n",
      "Iteration 449, loss = 0.17769199\n",
      "Iteration 450, loss = 0.17736728\n",
      "Iteration 451, loss = 0.17704393\n",
      "Iteration 452, loss = 0.17672192\n",
      "Iteration 453, loss = 0.17640126\n",
      "Iteration 454, loss = 0.17608193\n",
      "Iteration 455, loss = 0.17576393\n",
      "Iteration 456, loss = 0.17544726\n",
      "Iteration 457, loss = 0.17513192\n",
      "Iteration 458, loss = 0.17481790\n",
      "Iteration 459, loss = 0.17450519\n",
      "Iteration 460, loss = 0.17419380\n",
      "Iteration 461, loss = 0.17388371\n",
      "Iteration 462, loss = 0.17357492\n",
      "Iteration 463, loss = 0.17326744\n",
      "Iteration 464, loss = 0.17296124\n",
      "Iteration 465, loss = 0.17265634\n",
      "Iteration 466, loss = 0.17235272\n",
      "Iteration 467, loss = 0.17205038\n",
      "Iteration 468, loss = 0.17174932\n",
      "Iteration 469, loss = 0.17144953\n",
      "Iteration 470, loss = 0.17115101\n",
      "Iteration 471, loss = 0.17085375\n",
      "Iteration 472, loss = 0.17055775\n",
      "Iteration 473, loss = 0.17026300\n",
      "Iteration 474, loss = 0.16996950\n",
      "Iteration 475, loss = 0.16967725\n",
      "Iteration 476, loss = 0.16938624\n",
      "Iteration 477, loss = 0.16909647\n",
      "Iteration 478, loss = 0.16880792\n",
      "Iteration 479, loss = 0.16852061\n",
      "Iteration 480, loss = 0.16823451\n",
      "Iteration 481, loss = 0.16794964\n",
      "Iteration 482, loss = 0.16766598\n",
      "Iteration 483, loss = 0.16738352\n",
      "Iteration 484, loss = 0.16710228\n",
      "Iteration 485, loss = 0.16682223\n",
      "Iteration 486, loss = 0.16654338\n",
      "Iteration 487, loss = 0.16626572\n",
      "Iteration 488, loss = 0.16598924\n",
      "Iteration 489, loss = 0.16571395\n",
      "Iteration 490, loss = 0.16543984\n",
      "Iteration 491, loss = 0.16516690\n",
      "Iteration 492, loss = 0.16489513\n",
      "Iteration 493, loss = 0.16462452\n",
      "Iteration 494, loss = 0.16435507\n",
      "Iteration 495, loss = 0.16408677\n",
      "Iteration 496, loss = 0.16381963\n",
      "Iteration 497, loss = 0.16355363\n",
      "Iteration 498, loss = 0.16328878\n",
      "Iteration 499, loss = 0.16302506\n",
      "Iteration 500, loss = 0.16276247\n",
      "0.9533333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_dt = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_dt.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.95      0.97        20\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      " Neural Network model accuracy(in %): 97.77777777777777\n",
      "[[13  0  0]\n",
      " [ 0 19  1]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import metrics \n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\" Neural Network model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision regions in 2D\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "#Plotting decision regions\n",
    "plot_decision_regions(X, y, clf=classifier_dt, legend=2)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.title('Decision Tree classifier on Iris')\n",
    "plt.legend(target_names_iris)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
