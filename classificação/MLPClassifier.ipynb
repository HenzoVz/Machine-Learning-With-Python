{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Iris.csv')\n",
    "data = data.rename(index=str, columns={\"Species\": \"Class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm        Class\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      "Id               150 non-null int64\n",
      "SepalLengthCm    150 non-null float64\n",
      "SepalWidthCm     150 non-null float64\n",
      "PetalLengthCm    150 non-null float64\n",
      "PetalWidthCm     150 non-null float64\n",
      "Class            150 non-null object\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['SepalWidthCm','PetalWidthCm','Class','Id'],axis=1).values\n",
    "y = data['Class'].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_previsores = LabelEncoder()\n",
    "y = labelencoder_previsores.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X,columns=data.columns[1:3]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001, activation='identity',\n",
    "                     solver='sgd', verbose=10,  random_state=0,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.45637721\n",
      "Iteration 2, loss = 1.43881748\n",
      "Iteration 3, loss = 1.41426972\n",
      "Iteration 4, loss = 1.38392857\n",
      "Iteration 5, loss = 1.34894908\n",
      "Iteration 6, loss = 1.31042439\n",
      "Iteration 7, loss = 1.26936961\n",
      "Iteration 8, loss = 1.22671045\n",
      "Iteration 9, loss = 1.18327592\n",
      "Iteration 10, loss = 1.13979400\n",
      "Iteration 11, loss = 1.09688998\n",
      "Iteration 12, loss = 1.05508690\n",
      "Iteration 13, loss = 1.01480804\n",
      "Iteration 14, loss = 0.97638112\n",
      "Iteration 15, loss = 0.94004436\n",
      "Iteration 16, loss = 0.90595400\n",
      "Iteration 17, loss = 0.87419319\n",
      "Iteration 18, loss = 0.84478183\n",
      "Iteration 19, loss = 0.81768701\n",
      "Iteration 20, loss = 0.79283346\n",
      "Iteration 21, loss = 0.77011371\n",
      "Iteration 22, loss = 0.74939739\n",
      "Iteration 23, loss = 0.73053952\n",
      "Iteration 24, loss = 0.71338762\n",
      "Iteration 25, loss = 0.69778741\n",
      "Iteration 26, loss = 0.68358743\n",
      "Iteration 27, loss = 0.67064233\n",
      "Iteration 28, loss = 0.65881527\n",
      "Iteration 29, loss = 0.64797941\n",
      "Iteration 30, loss = 0.63801874\n",
      "Iteration 31, loss = 0.62882837\n",
      "Iteration 32, loss = 0.62031436\n",
      "Iteration 33, loss = 0.61239336\n",
      "Iteration 34, loss = 0.60499197\n",
      "Iteration 35, loss = 0.59804600\n",
      "Iteration 36, loss = 0.59149969\n",
      "Iteration 37, loss = 0.58530488\n",
      "Iteration 38, loss = 0.57942018\n",
      "Iteration 39, loss = 0.57381023\n",
      "Iteration 40, loss = 0.56844491\n",
      "Iteration 41, loss = 0.56329869\n",
      "Iteration 42, loss = 0.55834998\n",
      "Iteration 43, loss = 0.55358058\n",
      "Iteration 44, loss = 0.54897515\n",
      "Iteration 45, loss = 0.54452079\n",
      "Iteration 46, loss = 0.54020662\n",
      "Iteration 47, loss = 0.53602342\n",
      "Iteration 48, loss = 0.53196336\n",
      "Iteration 49, loss = 0.52801974\n",
      "Iteration 50, loss = 0.52418676\n",
      "Iteration 51, loss = 0.52045933\n",
      "Iteration 52, loss = 0.51683292\n",
      "Iteration 53, loss = 0.51330346\n",
      "Iteration 54, loss = 0.50986722\n",
      "Iteration 55, loss = 0.50652072\n",
      "Iteration 56, loss = 0.50326067\n",
      "Iteration 57, loss = 0.50008394\n",
      "Iteration 58, loss = 0.49698751\n",
      "Iteration 59, loss = 0.49396844\n",
      "Iteration 60, loss = 0.49102387\n",
      "Iteration 61, loss = 0.48815099\n",
      "Iteration 62, loss = 0.48534706\n",
      "Iteration 63, loss = 0.48260937\n",
      "Iteration 64, loss = 0.47993530\n",
      "Iteration 65, loss = 0.47732227\n",
      "Iteration 66, loss = 0.47476777\n",
      "Iteration 67, loss = 0.47226936\n",
      "Iteration 68, loss = 0.46982469\n",
      "Iteration 69, loss = 0.46743147\n",
      "Iteration 70, loss = 0.46508751\n",
      "Iteration 71, loss = 0.46279072\n",
      "Iteration 72, loss = 0.46053907\n",
      "Iteration 73, loss = 0.45833066\n",
      "Iteration 74, loss = 0.45616367\n",
      "Iteration 75, loss = 0.45403635\n",
      "Iteration 76, loss = 0.45194709\n",
      "Iteration 77, loss = 0.44989432\n",
      "Iteration 78, loss = 0.44787659\n",
      "Iteration 79, loss = 0.44589252\n",
      "Iteration 80, loss = 0.44394081\n",
      "Iteration 81, loss = 0.44202025\n",
      "Iteration 82, loss = 0.44012968\n",
      "Iteration 83, loss = 0.43826801\n",
      "Iteration 84, loss = 0.43643424\n",
      "Iteration 85, loss = 0.43462738\n",
      "Iteration 86, loss = 0.43284654\n",
      "Iteration 87, loss = 0.43109085\n",
      "Iteration 88, loss = 0.42935949\n",
      "Iteration 89, loss = 0.42765170\n",
      "Iteration 90, loss = 0.42596674\n",
      "Iteration 91, loss = 0.42430391\n",
      "Iteration 92, loss = 0.42266255\n",
      "Iteration 93, loss = 0.42104202\n",
      "Iteration 94, loss = 0.41944172\n",
      "Iteration 95, loss = 0.41786106\n",
      "Iteration 96, loss = 0.41629949\n",
      "Iteration 97, loss = 0.41475648\n",
      "Iteration 98, loss = 0.41323151\n",
      "Iteration 99, loss = 0.41172409\n",
      "Iteration 100, loss = 0.41023374\n",
      "Iteration 101, loss = 0.40876002\n",
      "Iteration 102, loss = 0.40730248\n",
      "Iteration 103, loss = 0.40586070\n",
      "Iteration 104, loss = 0.40443428\n",
      "Iteration 105, loss = 0.40302281\n",
      "Iteration 106, loss = 0.40162592\n",
      "Iteration 107, loss = 0.40024325\n",
      "Iteration 108, loss = 0.39887444\n",
      "Iteration 109, loss = 0.39751916\n",
      "Iteration 110, loss = 0.39617707\n",
      "Iteration 111, loss = 0.39484786\n",
      "Iteration 112, loss = 0.39353122\n",
      "Iteration 113, loss = 0.39222686\n",
      "Iteration 114, loss = 0.39093450\n",
      "Iteration 115, loss = 0.38965385\n",
      "Iteration 116, loss = 0.38838466\n",
      "Iteration 117, loss = 0.38712667\n",
      "Iteration 118, loss = 0.38587962\n",
      "Iteration 119, loss = 0.38464328\n",
      "Iteration 120, loss = 0.38341742\n",
      "Iteration 121, loss = 0.38220180\n",
      "Iteration 122, loss = 0.38099622\n",
      "Iteration 123, loss = 0.37980047\n",
      "Iteration 124, loss = 0.37861433\n",
      "Iteration 125, loss = 0.37743761\n",
      "Iteration 126, loss = 0.37627012\n",
      "Iteration 127, loss = 0.37511167\n",
      "Iteration 128, loss = 0.37396209\n",
      "Iteration 129, loss = 0.37282119\n",
      "Iteration 130, loss = 0.37168881\n",
      "Iteration 131, loss = 0.37056477\n",
      "Iteration 132, loss = 0.36944894\n",
      "Iteration 133, loss = 0.36834113\n",
      "Iteration 134, loss = 0.36724121\n",
      "Iteration 135, loss = 0.36614903\n",
      "Iteration 136, loss = 0.36506443\n",
      "Iteration 137, loss = 0.36398730\n",
      "Iteration 138, loss = 0.36291748\n",
      "Iteration 139, loss = 0.36185484\n",
      "Iteration 140, loss = 0.36079926\n",
      "Iteration 141, loss = 0.35975062\n",
      "Iteration 142, loss = 0.35870878\n",
      "Iteration 143, loss = 0.35767364\n",
      "Iteration 144, loss = 0.35664507\n",
      "Iteration 145, loss = 0.35562296\n",
      "Iteration 146, loss = 0.35460721\n",
      "Iteration 147, loss = 0.35359771\n",
      "Iteration 148, loss = 0.35259434\n",
      "Iteration 149, loss = 0.35159702\n",
      "Iteration 150, loss = 0.35060564\n",
      "Iteration 151, loss = 0.34962011\n",
      "Iteration 152, loss = 0.34864033\n",
      "Iteration 153, loss = 0.34766621\n",
      "Iteration 154, loss = 0.34669765\n",
      "Iteration 155, loss = 0.34573458\n",
      "Iteration 156, loss = 0.34477691\n",
      "Iteration 157, loss = 0.34382455\n",
      "Iteration 158, loss = 0.34287742\n",
      "Iteration 159, loss = 0.34193545\n",
      "Iteration 160, loss = 0.34099855\n",
      "Iteration 161, loss = 0.34006665\n",
      "Iteration 162, loss = 0.33913968\n",
      "Iteration 163, loss = 0.33821756\n",
      "Iteration 164, loss = 0.33730022\n",
      "Iteration 165, loss = 0.33638760\n",
      "Iteration 166, loss = 0.33547963\n",
      "Iteration 167, loss = 0.33457624\n",
      "Iteration 168, loss = 0.33367737\n",
      "Iteration 169, loss = 0.33278295\n",
      "Iteration 170, loss = 0.33189293\n",
      "Iteration 171, loss = 0.33100724\n",
      "Iteration 172, loss = 0.33012582\n",
      "Iteration 173, loss = 0.32924863\n",
      "Iteration 174, loss = 0.32837559\n",
      "Iteration 175, loss = 0.32750666\n",
      "Iteration 176, loss = 0.32664179\n",
      "Iteration 177, loss = 0.32578092\n",
      "Iteration 178, loss = 0.32492399\n",
      "Iteration 179, loss = 0.32407097\n",
      "Iteration 180, loss = 0.32322180\n",
      "Iteration 181, loss = 0.32237643\n",
      "Iteration 182, loss = 0.32153482\n",
      "Iteration 183, loss = 0.32069691\n",
      "Iteration 184, loss = 0.31986268\n",
      "Iteration 185, loss = 0.31903206\n",
      "Iteration 186, loss = 0.31820503\n",
      "Iteration 187, loss = 0.31738153\n",
      "Iteration 188, loss = 0.31656152\n",
      "Iteration 189, loss = 0.31574497\n",
      "Iteration 190, loss = 0.31493184\n",
      "Iteration 191, loss = 0.31412208\n",
      "Iteration 192, loss = 0.31331566\n",
      "Iteration 193, loss = 0.31251255\n",
      "Iteration 194, loss = 0.31171270\n",
      "Iteration 195, loss = 0.31091608\n",
      "Iteration 196, loss = 0.31012266\n",
      "Iteration 197, loss = 0.30933241\n",
      "Iteration 198, loss = 0.30854528\n",
      "Iteration 199, loss = 0.30776125\n",
      "Iteration 200, loss = 0.30698028\n",
      "Iteration 201, loss = 0.30620235\n",
      "Iteration 202, loss = 0.30542742\n",
      "Iteration 203, loss = 0.30465546\n",
      "Iteration 204, loss = 0.30388645\n",
      "Iteration 205, loss = 0.30312035\n",
      "Iteration 206, loss = 0.30235715\n",
      "Iteration 207, loss = 0.30159680\n",
      "Iteration 208, loss = 0.30083928\n",
      "Iteration 209, loss = 0.30008458\n",
      "Iteration 210, loss = 0.29933265\n",
      "Iteration 211, loss = 0.29858348\n",
      "Iteration 212, loss = 0.29783704\n",
      "Iteration 213, loss = 0.29709331\n",
      "Iteration 214, loss = 0.29635226\n",
      "Iteration 215, loss = 0.29561387\n",
      "Iteration 216, loss = 0.29487812\n",
      "Iteration 217, loss = 0.29414499\n",
      "Iteration 218, loss = 0.29341445\n",
      "Iteration 219, loss = 0.29268648\n",
      "Iteration 220, loss = 0.29196107\n",
      "Iteration 221, loss = 0.29123818\n",
      "Iteration 222, loss = 0.29051781\n",
      "Iteration 223, loss = 0.28979992\n",
      "Iteration 224, loss = 0.28908451\n",
      "Iteration 225, loss = 0.28837156\n",
      "Iteration 226, loss = 0.28766103\n",
      "Iteration 227, loss = 0.28695293\n",
      "Iteration 228, loss = 0.28624723\n",
      "Iteration 229, loss = 0.28554390\n",
      "Iteration 230, loss = 0.28484295\n",
      "Iteration 231, loss = 0.28414434\n",
      "Iteration 232, loss = 0.28344806\n",
      "Iteration 233, loss = 0.28275411\n",
      "Iteration 234, loss = 0.28206245\n",
      "Iteration 235, loss = 0.28137308\n",
      "Iteration 236, loss = 0.28068598\n",
      "Iteration 237, loss = 0.28000114\n",
      "Iteration 238, loss = 0.27931854\n",
      "Iteration 239, loss = 0.27863817\n",
      "Iteration 240, loss = 0.27796002\n",
      "Iteration 241, loss = 0.27728406\n",
      "Iteration 242, loss = 0.27661030\n",
      "Iteration 243, loss = 0.27593871\n",
      "Iteration 244, loss = 0.27526929\n",
      "Iteration 245, loss = 0.27460201\n",
      "Iteration 246, loss = 0.27393688\n",
      "Iteration 247, loss = 0.27327387\n",
      "Iteration 248, loss = 0.27261299\n",
      "Iteration 249, loss = 0.27195420\n",
      "Iteration 250, loss = 0.27129751\n",
      "Iteration 251, loss = 0.27064291\n",
      "Iteration 252, loss = 0.26999037\n",
      "Iteration 253, loss = 0.26933990\n",
      "Iteration 254, loss = 0.26869148\n",
      "Iteration 255, loss = 0.26804511\n",
      "Iteration 256, loss = 0.26740077\n",
      "Iteration 257, loss = 0.26675845\n",
      "Iteration 258, loss = 0.26611815\n",
      "Iteration 259, loss = 0.26547985\n",
      "Iteration 260, loss = 0.26484355\n",
      "Iteration 261, loss = 0.26420924\n",
      "Iteration 262, loss = 0.26357691\n",
      "Iteration 263, loss = 0.26294655\n",
      "Iteration 264, loss = 0.26231816\n",
      "Iteration 265, loss = 0.26169172\n",
      "Iteration 266, loss = 0.26106723\n",
      "Iteration 267, loss = 0.26044468\n",
      "Iteration 268, loss = 0.25982407\n",
      "Iteration 269, loss = 0.25920538\n",
      "Iteration 270, loss = 0.25858862\n",
      "Iteration 271, loss = 0.25797376\n",
      "Iteration 272, loss = 0.25736081\n",
      "Iteration 273, loss = 0.25674977\n",
      "Iteration 274, loss = 0.25614061\n",
      "Iteration 275, loss = 0.25553334\n",
      "Iteration 276, loss = 0.25492796\n",
      "Iteration 277, loss = 0.25432445\n",
      "Iteration 278, loss = 0.25372280\n",
      "Iteration 279, loss = 0.25312303\n",
      "Iteration 280, loss = 0.25252511\n",
      "Iteration 281, loss = 0.25192904\n",
      "Iteration 282, loss = 0.25133482\n",
      "Iteration 283, loss = 0.25074245\n",
      "Iteration 284, loss = 0.25015191\n",
      "Iteration 285, loss = 0.24956320\n",
      "Iteration 286, loss = 0.24897632\n",
      "Iteration 287, loss = 0.24839127\n",
      "Iteration 288, loss = 0.24780803\n",
      "Iteration 289, loss = 0.24722661\n",
      "Iteration 290, loss = 0.24664699\n",
      "Iteration 291, loss = 0.24606918\n",
      "Iteration 292, loss = 0.24549317\n",
      "Iteration 293, loss = 0.24491896\n",
      "Iteration 294, loss = 0.24434654\n",
      "Iteration 295, loss = 0.24377592\n",
      "Iteration 296, loss = 0.24320707\n",
      "Iteration 297, loss = 0.24264001\n",
      "Iteration 298, loss = 0.24207472\n",
      "Iteration 299, loss = 0.24151121\n",
      "Iteration 300, loss = 0.24094947\n",
      "Iteration 301, loss = 0.24038950\n",
      "Iteration 302, loss = 0.23983129\n",
      "Iteration 303, loss = 0.23927484\n",
      "Iteration 304, loss = 0.23872015\n",
      "Iteration 305, loss = 0.23816721\n",
      "Iteration 306, loss = 0.23761602\n",
      "Iteration 307, loss = 0.23706659\n",
      "Iteration 308, loss = 0.23651889\n",
      "Iteration 309, loss = 0.23597294\n",
      "Iteration 310, loss = 0.23542873\n",
      "Iteration 311, loss = 0.23488625\n",
      "Iteration 312, loss = 0.23434551\n",
      "Iteration 313, loss = 0.23380650\n",
      "Iteration 314, loss = 0.23326922\n",
      "Iteration 315, loss = 0.23273366\n",
      "Iteration 316, loss = 0.23219982\n",
      "Iteration 317, loss = 0.23166771\n",
      "Iteration 318, loss = 0.23113732\n",
      "Iteration 319, loss = 0.23060864\n",
      "Iteration 320, loss = 0.23008167\n",
      "Iteration 321, loss = 0.22955641\n",
      "Iteration 322, loss = 0.22903287\n",
      "Iteration 323, loss = 0.22851103\n",
      "Iteration 324, loss = 0.22799089\n",
      "Iteration 325, loss = 0.22747246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 326, loss = 0.22695572\n",
      "Iteration 327, loss = 0.22644069\n",
      "Iteration 328, loss = 0.22592734\n",
      "Iteration 329, loss = 0.22541570\n",
      "Iteration 330, loss = 0.22490574\n",
      "Iteration 331, loss = 0.22439747\n",
      "Iteration 332, loss = 0.22389089\n",
      "Iteration 333, loss = 0.22338599\n",
      "Iteration 334, loss = 0.22288278\n",
      "Iteration 335, loss = 0.22238125\n",
      "Iteration 336, loss = 0.22188139\n",
      "Iteration 337, loss = 0.22138322\n",
      "Iteration 338, loss = 0.22088671\n",
      "Iteration 339, loss = 0.22039188\n",
      "Iteration 340, loss = 0.21989873\n",
      "Iteration 341, loss = 0.21940724\n",
      "Iteration 342, loss = 0.21891741\n",
      "Iteration 343, loss = 0.21842926\n",
      "Iteration 344, loss = 0.21794276\n",
      "Iteration 345, loss = 0.21745793\n",
      "Iteration 346, loss = 0.21697476\n",
      "Iteration 347, loss = 0.21649324\n",
      "Iteration 348, loss = 0.21601338\n",
      "Iteration 349, loss = 0.21553518\n",
      "Iteration 350, loss = 0.21505862\n",
      "Iteration 351, loss = 0.21458372\n",
      "Iteration 352, loss = 0.21411046\n",
      "Iteration 353, loss = 0.21363885\n",
      "Iteration 354, loss = 0.21316888\n",
      "Iteration 355, loss = 0.21270056\n",
      "Iteration 356, loss = 0.21223387\n",
      "Iteration 357, loss = 0.21176883\n",
      "Iteration 358, loss = 0.21130542\n",
      "Iteration 359, loss = 0.21084364\n",
      "Iteration 360, loss = 0.21038350\n",
      "Iteration 361, loss = 0.20992498\n",
      "Iteration 362, loss = 0.20946810\n",
      "Iteration 363, loss = 0.20901284\n",
      "Iteration 364, loss = 0.20855920\n",
      "Iteration 365, loss = 0.20810719\n",
      "Iteration 366, loss = 0.20765679\n",
      "Iteration 367, loss = 0.20720802\n",
      "Iteration 368, loss = 0.20676086\n",
      "Iteration 369, loss = 0.20631531\n",
      "Iteration 370, loss = 0.20587137\n",
      "Iteration 371, loss = 0.20542905\n",
      "Iteration 372, loss = 0.20498833\n",
      "Iteration 373, loss = 0.20454922\n",
      "Iteration 374, loss = 0.20411171\n",
      "Iteration 375, loss = 0.20367580\n",
      "Iteration 376, loss = 0.20324148\n",
      "Iteration 377, loss = 0.20280877\n",
      "Iteration 378, loss = 0.20237765\n",
      "Iteration 379, loss = 0.20194812\n",
      "Iteration 380, loss = 0.20152017\n",
      "Iteration 381, loss = 0.20109382\n",
      "Iteration 382, loss = 0.20066905\n",
      "Iteration 383, loss = 0.20024586\n",
      "Iteration 384, loss = 0.19982425\n",
      "Iteration 385, loss = 0.19940422\n",
      "Iteration 386, loss = 0.19898577\n",
      "Iteration 387, loss = 0.19856888\n",
      "Iteration 388, loss = 0.19815357\n",
      "Iteration 389, loss = 0.19773982\n",
      "Iteration 390, loss = 0.19732764\n",
      "Iteration 391, loss = 0.19691701\n",
      "Iteration 392, loss = 0.19650795\n",
      "Iteration 393, loss = 0.19610045\n",
      "Iteration 394, loss = 0.19569450\n",
      "Iteration 395, loss = 0.19529010\n",
      "Iteration 396, loss = 0.19488725\n",
      "Iteration 397, loss = 0.19448594\n",
      "Iteration 398, loss = 0.19408618\n",
      "Iteration 399, loss = 0.19368796\n",
      "Iteration 400, loss = 0.19329127\n",
      "Iteration 401, loss = 0.19289612\n",
      "Iteration 402, loss = 0.19250251\n",
      "Iteration 403, loss = 0.19211042\n",
      "Iteration 404, loss = 0.19171985\n",
      "Iteration 405, loss = 0.19133081\n",
      "Iteration 406, loss = 0.19094329\n",
      "Iteration 407, loss = 0.19055729\n",
      "Iteration 408, loss = 0.19017280\n",
      "Iteration 409, loss = 0.18978982\n",
      "Iteration 410, loss = 0.18940835\n",
      "Iteration 411, loss = 0.18902839\n",
      "Iteration 412, loss = 0.18864993\n",
      "Iteration 413, loss = 0.18827296\n",
      "Iteration 414, loss = 0.18789749\n",
      "Iteration 415, loss = 0.18752351\n",
      "Iteration 416, loss = 0.18715102\n",
      "Iteration 417, loss = 0.18678002\n",
      "Iteration 418, loss = 0.18641050\n",
      "Iteration 419, loss = 0.18604245\n",
      "Iteration 420, loss = 0.18567589\n",
      "Iteration 421, loss = 0.18531079\n",
      "Iteration 422, loss = 0.18494717\n",
      "Iteration 423, loss = 0.18458501\n",
      "Iteration 424, loss = 0.18422431\n",
      "Iteration 425, loss = 0.18386507\n",
      "Iteration 426, loss = 0.18350728\n",
      "Iteration 427, loss = 0.18315095\n",
      "Iteration 428, loss = 0.18279606\n",
      "Iteration 429, loss = 0.18244262\n",
      "Iteration 430, loss = 0.18209062\n",
      "Iteration 431, loss = 0.18174005\n",
      "Iteration 432, loss = 0.18139092\n",
      "Iteration 433, loss = 0.18104322\n",
      "Iteration 434, loss = 0.18069695\n",
      "Iteration 435, loss = 0.18035210\n",
      "Iteration 436, loss = 0.18000866\n",
      "Iteration 437, loss = 0.17966664\n",
      "Iteration 438, loss = 0.17932604\n",
      "Iteration 439, loss = 0.17898684\n",
      "Iteration 440, loss = 0.17864904\n",
      "Iteration 441, loss = 0.17831265\n",
      "Iteration 442, loss = 0.17797765\n",
      "Iteration 443, loss = 0.17764404\n",
      "Iteration 444, loss = 0.17731182\n",
      "Iteration 445, loss = 0.17698099\n",
      "Iteration 446, loss = 0.17665153\n",
      "Iteration 447, loss = 0.17632345\n",
      "Iteration 448, loss = 0.17599675\n",
      "Iteration 449, loss = 0.17567141\n",
      "Iteration 450, loss = 0.17534744\n",
      "Iteration 451, loss = 0.17502482\n",
      "Iteration 452, loss = 0.17470357\n",
      "Iteration 453, loss = 0.17438367\n",
      "Iteration 454, loss = 0.17406511\n",
      "Iteration 455, loss = 0.17374790\n",
      "Iteration 456, loss = 0.17343203\n",
      "Iteration 457, loss = 0.17311750\n",
      "Iteration 458, loss = 0.17280429\n",
      "Iteration 459, loss = 0.17249242\n",
      "Iteration 460, loss = 0.17218187\n",
      "Iteration 461, loss = 0.17187264\n",
      "Iteration 462, loss = 0.17156472\n",
      "Iteration 463, loss = 0.17125812\n",
      "Iteration 464, loss = 0.17095282\n",
      "Iteration 465, loss = 0.17064882\n",
      "Iteration 466, loss = 0.17034613\n",
      "Iteration 467, loss = 0.17004473\n",
      "Iteration 468, loss = 0.16974461\n",
      "Iteration 469, loss = 0.16944579\n",
      "Iteration 470, loss = 0.16914824\n",
      "Iteration 471, loss = 0.16885197\n",
      "Iteration 472, loss = 0.16855698\n",
      "Iteration 473, loss = 0.16826325\n",
      "Iteration 474, loss = 0.16797079\n",
      "Iteration 475, loss = 0.16767959\n",
      "Iteration 476, loss = 0.16738964\n",
      "Iteration 477, loss = 0.16710094\n",
      "Iteration 478, loss = 0.16681349\n",
      "Iteration 479, loss = 0.16652729\n",
      "Iteration 480, loss = 0.16624232\n",
      "Iteration 481, loss = 0.16595858\n",
      "Iteration 482, loss = 0.16567608\n",
      "Iteration 483, loss = 0.16539480\n",
      "Iteration 484, loss = 0.16511474\n",
      "Iteration 485, loss = 0.16483589\n",
      "Iteration 486, loss = 0.16455826\n",
      "Iteration 487, loss = 0.16428184\n",
      "Iteration 488, loss = 0.16400662\n",
      "Iteration 489, loss = 0.16373259\n",
      "Iteration 490, loss = 0.16345976\n",
      "Iteration 491, loss = 0.16318812\n",
      "Iteration 492, loss = 0.16291767\n",
      "Iteration 493, loss = 0.16264840\n",
      "Iteration 494, loss = 0.16238030\n",
      "Iteration 495, loss = 0.16211337\n",
      "Iteration 496, loss = 0.16184761\n",
      "Iteration 497, loss = 0.16158302\n",
      "Iteration 498, loss = 0.16131958\n",
      "Iteration 499, loss = 0.16105730\n",
      "Iteration 500, loss = 0.16079617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.42988439\n",
      "Iteration 2, loss = 1.41396150\n",
      "Iteration 3, loss = 1.39168749\n",
      "Iteration 4, loss = 1.36413171\n",
      "Iteration 5, loss = 1.33232538\n",
      "Iteration 6, loss = 1.29724188\n",
      "Iteration 7, loss = 1.25978278\n",
      "Iteration 8, loss = 1.22076836\n",
      "Iteration 9, loss = 1.18093181\n",
      "Iteration 10, loss = 1.14091639\n",
      "Iteration 11, loss = 1.10127499\n",
      "Iteration 12, loss = 1.06247152\n",
      "Iteration 13, loss = 1.02488409\n",
      "Iteration 14, loss = 0.98880938\n",
      "Iteration 15, loss = 0.95446835\n",
      "Iteration 16, loss = 0.92201289\n",
      "Iteration 17, loss = 0.89153322\n",
      "Iteration 18, loss = 0.86306598\n",
      "Iteration 19, loss = 0.83660242\n",
      "Iteration 20, loss = 0.81209677\n",
      "Iteration 21, loss = 0.78947415\n",
      "Iteration 22, loss = 0.76863805\n",
      "Iteration 23, loss = 0.74947702\n",
      "Iteration 24, loss = 0.73187054\n",
      "Iteration 25, loss = 0.71569393\n",
      "Iteration 26, loss = 0.70082234\n",
      "Iteration 27, loss = 0.68713392\n",
      "Iteration 28, loss = 0.67451208\n",
      "Iteration 29, loss = 0.66284718\n",
      "Iteration 30, loss = 0.65203756\n",
      "Iteration 31, loss = 0.64199006\n",
      "Iteration 32, loss = 0.63262025\n",
      "Iteration 33, loss = 0.62385230\n",
      "Iteration 34, loss = 0.61561868\n",
      "Iteration 35, loss = 0.60785964\n",
      "Iteration 36, loss = 0.60052272\n",
      "Iteration 37, loss = 0.59356209\n",
      "Iteration 38, loss = 0.58693793\n",
      "Iteration 39, loss = 0.58061577\n",
      "Iteration 40, loss = 0.57456594\n",
      "Iteration 41, loss = 0.56876293\n",
      "Iteration 42, loss = 0.56318488\n",
      "Iteration 43, loss = 0.55781306\n",
      "Iteration 44, loss = 0.55263145\n",
      "Iteration 45, loss = 0.54762632\n",
      "Iteration 46, loss = 0.54278586\n",
      "Iteration 47, loss = 0.53809989\n",
      "Iteration 48, loss = 0.53355956\n",
      "Iteration 49, loss = 0.52915716\n",
      "Iteration 50, loss = 0.52488585\n",
      "Iteration 51, loss = 0.52073953\n",
      "Iteration 52, loss = 0.51671273\n",
      "Iteration 53, loss = 0.51280040\n",
      "Iteration 54, loss = 0.50899793\n",
      "Iteration 55, loss = 0.50530096\n",
      "Iteration 56, loss = 0.50170542\n",
      "Iteration 57, loss = 0.49820741\n",
      "Iteration 58, loss = 0.49480319\n",
      "Iteration 59, loss = 0.49148917\n",
      "Iteration 60, loss = 0.48826188\n",
      "Iteration 61, loss = 0.48511796\n",
      "Iteration 62, loss = 0.48205412\n",
      "Iteration 63, loss = 0.47906721\n",
      "Iteration 64, loss = 0.47615415\n",
      "Iteration 65, loss = 0.47331197\n",
      "Iteration 66, loss = 0.47053779\n",
      "Iteration 67, loss = 0.46782883\n",
      "Iteration 68, loss = 0.46518242\n",
      "Iteration 69, loss = 0.46259599\n",
      "Iteration 70, loss = 0.46006710\n",
      "Iteration 71, loss = 0.45759338\n",
      "Iteration 72, loss = 0.45517259\n",
      "Iteration 73, loss = 0.45280259\n",
      "Iteration 74, loss = 0.45048136\n",
      "Iteration 75, loss = 0.44820697\n",
      "Iteration 76, loss = 0.44597758\n",
      "Iteration 77, loss = 0.44379148\n",
      "Iteration 78, loss = 0.44164702\n",
      "Iteration 79, loss = 0.43954265\n",
      "Iteration 80, loss = 0.43747691\n",
      "Iteration 81, loss = 0.43544842\n",
      "Iteration 82, loss = 0.43345587\n",
      "Iteration 83, loss = 0.43149802\n",
      "Iteration 84, loss = 0.42957369\n",
      "Iteration 85, loss = 0.42768179\n",
      "Iteration 86, loss = 0.42582124\n",
      "Iteration 87, loss = 0.42399106\n",
      "Iteration 88, loss = 0.42219028\n",
      "Iteration 89, loss = 0.42041801\n",
      "Iteration 90, loss = 0.41867338\n",
      "Iteration 91, loss = 0.41695556\n",
      "Iteration 92, loss = 0.41526377\n",
      "Iteration 93, loss = 0.41359725\n",
      "Iteration 94, loss = 0.41195528\n",
      "Iteration 95, loss = 0.41033717\n",
      "Iteration 96, loss = 0.40874224\n",
      "Iteration 97, loss = 0.40716987\n",
      "Iteration 98, loss = 0.40561944\n",
      "Iteration 99, loss = 0.40409034\n",
      "Iteration 100, loss = 0.40258203\n",
      "Iteration 101, loss = 0.40109393\n",
      "Iteration 102, loss = 0.39962553\n",
      "Iteration 103, loss = 0.39817631\n",
      "Iteration 104, loss = 0.39674578\n",
      "Iteration 105, loss = 0.39533347\n",
      "Iteration 106, loss = 0.39393891\n",
      "Iteration 107, loss = 0.39256166\n",
      "Iteration 108, loss = 0.39120129\n",
      "Iteration 109, loss = 0.38985738\n",
      "Iteration 110, loss = 0.38852955\n",
      "Iteration 111, loss = 0.38721739\n",
      "Iteration 112, loss = 0.38592053\n",
      "Iteration 113, loss = 0.38463862\n",
      "Iteration 114, loss = 0.38337130\n",
      "Iteration 115, loss = 0.38211823\n",
      "Iteration 116, loss = 0.38087908\n",
      "Iteration 117, loss = 0.37965354\n",
      "Iteration 118, loss = 0.37844130\n",
      "Iteration 119, loss = 0.37724206\n",
      "Iteration 120, loss = 0.37605554\n",
      "Iteration 121, loss = 0.37488144\n",
      "Iteration 122, loss = 0.37371950\n",
      "Iteration 123, loss = 0.37256946\n",
      "Iteration 124, loss = 0.37143106\n",
      "Iteration 125, loss = 0.37030406\n",
      "Iteration 126, loss = 0.36918820\n",
      "Iteration 127, loss = 0.36808327\n",
      "Iteration 128, loss = 0.36698903\n",
      "Iteration 129, loss = 0.36590525\n",
      "Iteration 130, loss = 0.36483174\n",
      "Iteration 131, loss = 0.36376827\n",
      "Iteration 132, loss = 0.36271464\n",
      "Iteration 133, loss = 0.36167067\n",
      "Iteration 134, loss = 0.36063615\n",
      "Iteration 135, loss = 0.35961089\n",
      "Iteration 136, loss = 0.35859473\n",
      "Iteration 137, loss = 0.35758748\n",
      "Iteration 138, loss = 0.35658896\n",
      "Iteration 139, loss = 0.35559901\n",
      "Iteration 140, loss = 0.35461748\n",
      "Iteration 141, loss = 0.35364418\n",
      "Iteration 142, loss = 0.35267899\n",
      "Iteration 143, loss = 0.35172173\n",
      "Iteration 144, loss = 0.35077226\n",
      "Iteration 145, loss = 0.34983045\n",
      "Iteration 146, loss = 0.34889614\n",
      "Iteration 147, loss = 0.34796921\n",
      "Iteration 148, loss = 0.34704952\n",
      "Iteration 149, loss = 0.34613694\n",
      "Iteration 150, loss = 0.34523134\n",
      "Iteration 151, loss = 0.34433260\n",
      "Iteration 152, loss = 0.34344059\n",
      "Iteration 153, loss = 0.34255521\n",
      "Iteration 154, loss = 0.34167633\n",
      "Iteration 155, loss = 0.34080384\n",
      "Iteration 156, loss = 0.33993763\n",
      "Iteration 157, loss = 0.33907760\n",
      "Iteration 158, loss = 0.33822364\n",
      "Iteration 159, loss = 0.33737565\n",
      "Iteration 160, loss = 0.33653352\n",
      "Iteration 161, loss = 0.33569716\n",
      "Iteration 162, loss = 0.33486648\n",
      "Iteration 163, loss = 0.33404138\n",
      "Iteration 164, loss = 0.33322176\n",
      "Iteration 165, loss = 0.33240755\n",
      "Iteration 166, loss = 0.33159865\n",
      "Iteration 167, loss = 0.33079497\n",
      "Iteration 168, loss = 0.32999644\n",
      "Iteration 169, loss = 0.32920297\n",
      "Iteration 170, loss = 0.32841448\n",
      "Iteration 171, loss = 0.32763090\n",
      "Iteration 172, loss = 0.32685214\n",
      "Iteration 173, loss = 0.32607814\n",
      "Iteration 174, loss = 0.32530882\n",
      "Iteration 175, loss = 0.32454410\n",
      "Iteration 176, loss = 0.32378392\n",
      "Iteration 177, loss = 0.32302821\n",
      "Iteration 178, loss = 0.32227690\n",
      "Iteration 179, loss = 0.32152992\n",
      "Iteration 180, loss = 0.32078722\n",
      "Iteration 181, loss = 0.32004872\n",
      "Iteration 182, loss = 0.31931437\n",
      "Iteration 183, loss = 0.31858410\n",
      "Iteration 184, loss = 0.31785786\n",
      "Iteration 185, loss = 0.31713559\n",
      "Iteration 186, loss = 0.31641723\n",
      "Iteration 187, loss = 0.31570273\n",
      "Iteration 188, loss = 0.31499202\n",
      "Iteration 189, loss = 0.31428506\n",
      "Iteration 190, loss = 0.31358180\n",
      "Iteration 191, loss = 0.31288218\n",
      "Iteration 192, loss = 0.31218615\n",
      "Iteration 193, loss = 0.31149366\n",
      "Iteration 194, loss = 0.31080467\n",
      "Iteration 195, loss = 0.31011913\n",
      "Iteration 196, loss = 0.30943698\n",
      "Iteration 197, loss = 0.30875819\n",
      "Iteration 198, loss = 0.30808271\n",
      "Iteration 199, loss = 0.30741050\n",
      "Iteration 200, loss = 0.30674151\n",
      "Iteration 201, loss = 0.30607571\n",
      "Iteration 202, loss = 0.30541304\n",
      "Iteration 203, loss = 0.30475347\n",
      "Iteration 204, loss = 0.30409696\n",
      "Iteration 205, loss = 0.30344347\n",
      "Iteration 206, loss = 0.30279296\n",
      "Iteration 207, loss = 0.30214540\n",
      "Iteration 208, loss = 0.30150074\n",
      "Iteration 209, loss = 0.30085896\n",
      "Iteration 210, loss = 0.30022001\n",
      "Iteration 211, loss = 0.29958386\n",
      "Iteration 212, loss = 0.29895048\n",
      "Iteration 213, loss = 0.29831984\n",
      "Iteration 214, loss = 0.29769189\n",
      "Iteration 215, loss = 0.29706661\n",
      "Iteration 216, loss = 0.29644397\n",
      "Iteration 217, loss = 0.29582393\n",
      "Iteration 218, loss = 0.29520647\n",
      "Iteration 219, loss = 0.29459156\n",
      "Iteration 220, loss = 0.29397916\n",
      "Iteration 221, loss = 0.29336924\n",
      "Iteration 222, loss = 0.29276179\n",
      "Iteration 223, loss = 0.29215676\n",
      "Iteration 224, loss = 0.29155414\n",
      "Iteration 225, loss = 0.29095390\n",
      "Iteration 226, loss = 0.29035600\n",
      "Iteration 227, loss = 0.28976043\n",
      "Iteration 228, loss = 0.28916716\n",
      "Iteration 229, loss = 0.28857616\n",
      "Iteration 230, loss = 0.28798742\n",
      "Iteration 231, loss = 0.28740090\n",
      "Iteration 232, loss = 0.28681658\n",
      "Iteration 233, loss = 0.28623444\n",
      "Iteration 234, loss = 0.28565445\n",
      "Iteration 235, loss = 0.28507660\n",
      "Iteration 236, loss = 0.28450086\n",
      "Iteration 237, loss = 0.28392722\n",
      "Iteration 238, loss = 0.28335564\n",
      "Iteration 239, loss = 0.28278611\n",
      "Iteration 240, loss = 0.28221861\n",
      "Iteration 241, loss = 0.28165312\n",
      "Iteration 242, loss = 0.28108961\n",
      "Iteration 243, loss = 0.28052808\n",
      "Iteration 244, loss = 0.27996849\n",
      "Iteration 245, loss = 0.27941084\n",
      "Iteration 246, loss = 0.27885510\n",
      "Iteration 247, loss = 0.27830125\n",
      "Iteration 248, loss = 0.27774929\n",
      "Iteration 249, loss = 0.27719918\n",
      "Iteration 250, loss = 0.27665092\n",
      "Iteration 251, loss = 0.27610448\n",
      "Iteration 252, loss = 0.27555985\n",
      "Iteration 253, loss = 0.27501702\n",
      "Iteration 254, loss = 0.27447597\n",
      "Iteration 255, loss = 0.27393668\n",
      "Iteration 256, loss = 0.27339913\n",
      "Iteration 257, loss = 0.27286332\n",
      "Iteration 258, loss = 0.27232923\n",
      "Iteration 259, loss = 0.27179684\n",
      "Iteration 260, loss = 0.27126613\n",
      "Iteration 261, loss = 0.27073711\n",
      "Iteration 262, loss = 0.27020974\n",
      "Iteration 263, loss = 0.26968402\n",
      "Iteration 264, loss = 0.26915994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 265, loss = 0.26863747\n",
      "Iteration 266, loss = 0.26811662\n",
      "Iteration 267, loss = 0.26759736\n",
      "Iteration 268, loss = 0.26707969\n",
      "Iteration 269, loss = 0.26656358\n",
      "Iteration 270, loss = 0.26604904\n",
      "Iteration 271, loss = 0.26553604\n",
      "Iteration 272, loss = 0.26502458\n",
      "Iteration 273, loss = 0.26451465\n",
      "Iteration 274, loss = 0.26400623\n",
      "Iteration 275, loss = 0.26349931\n",
      "Iteration 276, loss = 0.26299389\n",
      "Iteration 277, loss = 0.26248995\n",
      "Iteration 278, loss = 0.26198748\n",
      "Iteration 279, loss = 0.26148647\n",
      "Iteration 280, loss = 0.26098691\n",
      "Iteration 281, loss = 0.26048879\n",
      "Iteration 282, loss = 0.25999211\n",
      "Iteration 283, loss = 0.25949685\n",
      "Iteration 284, loss = 0.25900300\n",
      "Iteration 285, loss = 0.25851056\n",
      "Iteration 286, loss = 0.25801951\n",
      "Iteration 287, loss = 0.25752985\n",
      "Iteration 288, loss = 0.25704157\n",
      "Iteration 289, loss = 0.25655466\n",
      "Iteration 290, loss = 0.25606911\n",
      "Iteration 291, loss = 0.25558491\n",
      "Iteration 292, loss = 0.25510206\n",
      "Iteration 293, loss = 0.25462054\n",
      "Iteration 294, loss = 0.25414036\n",
      "Iteration 295, loss = 0.25366149\n",
      "Iteration 296, loss = 0.25318394\n",
      "Iteration 297, loss = 0.25270770\n",
      "Iteration 298, loss = 0.25223276\n",
      "Iteration 299, loss = 0.25175911\n",
      "Iteration 300, loss = 0.25128675\n",
      "Iteration 301, loss = 0.25081567\n",
      "Iteration 302, loss = 0.25034586\n",
      "Iteration 303, loss = 0.24987732\n",
      "Iteration 304, loss = 0.24941003\n",
      "Iteration 305, loss = 0.24894401\n",
      "Iteration 306, loss = 0.24847922\n",
      "Iteration 307, loss = 0.24801568\n",
      "Iteration 308, loss = 0.24755338\n",
      "Iteration 309, loss = 0.24709230\n",
      "Iteration 310, loss = 0.24663245\n",
      "Iteration 311, loss = 0.24617381\n",
      "Iteration 312, loss = 0.24571639\n",
      "Iteration 313, loss = 0.24526018\n",
      "Iteration 314, loss = 0.24480516\n",
      "Iteration 315, loss = 0.24435135\n",
      "Iteration 316, loss = 0.24389872\n",
      "Iteration 317, loss = 0.24344728\n",
      "Iteration 318, loss = 0.24299703\n",
      "Iteration 319, loss = 0.24254795\n",
      "Iteration 320, loss = 0.24210004\n",
      "Iteration 321, loss = 0.24165330\n",
      "Iteration 322, loss = 0.24120772\n",
      "Iteration 323, loss = 0.24076330\n",
      "Iteration 324, loss = 0.24032003\n",
      "Iteration 325, loss = 0.23987792\n",
      "Iteration 326, loss = 0.23943694\n",
      "Iteration 327, loss = 0.23899711\n",
      "Iteration 328, loss = 0.23855842\n",
      "Iteration 329, loss = 0.23812086\n",
      "Iteration 330, loss = 0.23768442\n",
      "Iteration 331, loss = 0.23724912\n",
      "Iteration 332, loss = 0.23681493\n",
      "Iteration 333, loss = 0.23638186\n",
      "Iteration 334, loss = 0.23594991\n",
      "Iteration 335, loss = 0.23551907\n",
      "Iteration 336, loss = 0.23508933\n",
      "Iteration 337, loss = 0.23466070\n",
      "Iteration 338, loss = 0.23423316\n",
      "Iteration 339, loss = 0.23380673\n",
      "Iteration 340, loss = 0.23338138\n",
      "Iteration 341, loss = 0.23295713\n",
      "Iteration 342, loss = 0.23253396\n",
      "Iteration 343, loss = 0.23211188\n",
      "Iteration 344, loss = 0.23169088\n",
      "Iteration 345, loss = 0.23127096\n",
      "Iteration 346, loss = 0.23085212\n",
      "Iteration 347, loss = 0.23043434\n",
      "Iteration 348, loss = 0.23001764\n",
      "Iteration 349, loss = 0.22960200\n",
      "Iteration 350, loss = 0.22918743\n",
      "Iteration 351, loss = 0.22877392\n",
      "Iteration 352, loss = 0.22836147\n",
      "Iteration 353, loss = 0.22795007\n",
      "Iteration 354, loss = 0.22753973\n",
      "Iteration 355, loss = 0.22713044\n",
      "Iteration 356, loss = 0.22672220\n",
      "Iteration 357, loss = 0.22631501\n",
      "Iteration 358, loss = 0.22590886\n",
      "Iteration 359, loss = 0.22550375\n",
      "Iteration 360, loss = 0.22509969\n",
      "Iteration 361, loss = 0.22469666\n",
      "Iteration 362, loss = 0.22429467\n",
      "Iteration 363, loss = 0.22389371\n",
      "Iteration 364, loss = 0.22349378\n",
      "Iteration 365, loss = 0.22309489\n",
      "Iteration 366, loss = 0.22269702\n",
      "Iteration 367, loss = 0.22230018\n",
      "Iteration 368, loss = 0.22190436\n",
      "Iteration 369, loss = 0.22150956\n",
      "Iteration 370, loss = 0.22111578\n",
      "Iteration 371, loss = 0.22072303\n",
      "Iteration 372, loss = 0.22033128\n",
      "Iteration 373, loss = 0.21994056\n",
      "Iteration 374, loss = 0.21955084\n",
      "Iteration 375, loss = 0.21916214\n",
      "Iteration 376, loss = 0.21877445\n",
      "Iteration 377, loss = 0.21838777\n",
      "Iteration 378, loss = 0.21800209\n",
      "Iteration 379, loss = 0.21761742\n",
      "Iteration 380, loss = 0.21723375\n",
      "Iteration 381, loss = 0.21685108\n",
      "Iteration 382, loss = 0.21646941\n",
      "Iteration 383, loss = 0.21608875\n",
      "Iteration 384, loss = 0.21570908\n",
      "Iteration 385, loss = 0.21533040\n",
      "Iteration 386, loss = 0.21495272\n",
      "Iteration 387, loss = 0.21457604\n",
      "Iteration 388, loss = 0.21420034\n",
      "Iteration 389, loss = 0.21382564\n",
      "Iteration 390, loss = 0.21345193\n",
      "Iteration 391, loss = 0.21307920\n",
      "Iteration 392, loss = 0.21270746\n",
      "Iteration 393, loss = 0.21233671\n",
      "Iteration 394, loss = 0.21196694\n",
      "Iteration 395, loss = 0.21159815\n",
      "Iteration 396, loss = 0.21123035\n",
      "Iteration 397, loss = 0.21086352\n",
      "Iteration 398, loss = 0.21049767\n",
      "Iteration 399, loss = 0.21013281\n",
      "Iteration 400, loss = 0.20976892\n",
      "Iteration 401, loss = 0.20940600\n",
      "Iteration 402, loss = 0.20904406\n",
      "Iteration 403, loss = 0.20868309\n",
      "Iteration 404, loss = 0.20832310\n",
      "Iteration 405, loss = 0.20796407\n",
      "Iteration 406, loss = 0.20760602\n",
      "Iteration 407, loss = 0.20724894\n",
      "Iteration 408, loss = 0.20689282\n",
      "Iteration 409, loss = 0.20653767\n",
      "Iteration 410, loss = 0.20618349\n",
      "Iteration 411, loss = 0.20583027\n",
      "Iteration 412, loss = 0.20547801\n",
      "Iteration 413, loss = 0.20512672\n",
      "Iteration 414, loss = 0.20477639\n",
      "Iteration 415, loss = 0.20442702\n",
      "Iteration 416, loss = 0.20407860\n",
      "Iteration 417, loss = 0.20373115\n",
      "Iteration 418, loss = 0.20338466\n",
      "Iteration 419, loss = 0.20303912\n",
      "Iteration 420, loss = 0.20269453\n",
      "Iteration 421, loss = 0.20235090\n",
      "Iteration 422, loss = 0.20200823\n",
      "Iteration 423, loss = 0.20166650\n",
      "Iteration 424, loss = 0.20132573\n",
      "Iteration 425, loss = 0.20098591\n",
      "Iteration 426, loss = 0.20064704\n",
      "Iteration 427, loss = 0.20030912\n",
      "Iteration 428, loss = 0.19997214\n",
      "Iteration 429, loss = 0.19963611\n",
      "Iteration 430, loss = 0.19930103\n",
      "Iteration 431, loss = 0.19896689\n",
      "Iteration 432, loss = 0.19863369\n",
      "Iteration 433, loss = 0.19830144\n",
      "Iteration 434, loss = 0.19797012\n",
      "Iteration 435, loss = 0.19763975\n",
      "Iteration 436, loss = 0.19731032\n",
      "Iteration 437, loss = 0.19698183\n",
      "Iteration 438, loss = 0.19665427\n",
      "Iteration 439, loss = 0.19632765\n",
      "Iteration 440, loss = 0.19600197\n",
      "Iteration 441, loss = 0.19567722\n",
      "Iteration 442, loss = 0.19535340\n",
      "Iteration 443, loss = 0.19503052\n",
      "Iteration 444, loss = 0.19470857\n",
      "Iteration 445, loss = 0.19438755\n",
      "Iteration 446, loss = 0.19406745\n",
      "Iteration 447, loss = 0.19374829\n",
      "Iteration 448, loss = 0.19343005\n",
      "Iteration 449, loss = 0.19311274\n",
      "Iteration 450, loss = 0.19279636\n",
      "Iteration 451, loss = 0.19248090\n",
      "Iteration 452, loss = 0.19216636\n",
      "Iteration 453, loss = 0.19185274\n",
      "Iteration 454, loss = 0.19154005\n",
      "Iteration 455, loss = 0.19122827\n",
      "Iteration 456, loss = 0.19091742\n",
      "Iteration 457, loss = 0.19060748\n",
      "Iteration 458, loss = 0.19029846\n",
      "Iteration 459, loss = 0.18999035\n",
      "Iteration 460, loss = 0.18968316\n",
      "Iteration 461, loss = 0.18937688\n",
      "Iteration 462, loss = 0.18907151\n",
      "Iteration 463, loss = 0.18876706\n",
      "Iteration 464, loss = 0.18846351\n",
      "Iteration 465, loss = 0.18816088\n",
      "Iteration 466, loss = 0.18785915\n",
      "Iteration 467, loss = 0.18755833\n",
      "Iteration 468, loss = 0.18725841\n",
      "Iteration 469, loss = 0.18695940\n",
      "Iteration 470, loss = 0.18666129\n",
      "Iteration 471, loss = 0.18636408\n",
      "Iteration 472, loss = 0.18606777\n",
      "Iteration 473, loss = 0.18577236\n",
      "Iteration 474, loss = 0.18547785\n",
      "Iteration 475, loss = 0.18518424\n",
      "Iteration 476, loss = 0.18489152\n",
      "Iteration 477, loss = 0.18459969\n",
      "Iteration 478, loss = 0.18430876\n",
      "Iteration 479, loss = 0.18401872\n",
      "Iteration 480, loss = 0.18372958\n",
      "Iteration 481, loss = 0.18344132\n",
      "Iteration 482, loss = 0.18315395\n",
      "Iteration 483, loss = 0.18286746\n",
      "Iteration 484, loss = 0.18258186\n",
      "Iteration 485, loss = 0.18229715\n",
      "Iteration 486, loss = 0.18201331\n",
      "Iteration 487, loss = 0.18173036\n",
      "Iteration 488, loss = 0.18144829\n",
      "Iteration 489, loss = 0.18116710\n",
      "Iteration 490, loss = 0.18088678\n",
      "Iteration 491, loss = 0.18060734\n",
      "Iteration 492, loss = 0.18032878\n",
      "Iteration 493, loss = 0.18005109\n",
      "Iteration 494, loss = 0.17977427\n",
      "Iteration 495, loss = 0.17949832\n",
      "Iteration 496, loss = 0.17922323\n",
      "Iteration 497, loss = 0.17894902\n",
      "Iteration 498, loss = 0.17867567\n",
      "Iteration 499, loss = 0.17840319\n",
      "Iteration 500, loss = 0.17813157\n",
      "Iteration 1, loss = 1.42708177\n",
      "Iteration 2, loss = 1.41153396\n",
      "Iteration 3, loss = 1.38977450\n",
      "Iteration 4, loss = 1.36283858\n",
      "Iteration 5, loss = 1.33172349\n",
      "Iteration 6, loss = 1.29737017\n",
      "Iteration 7, loss = 1.26065009\n",
      "Iteration 8, loss = 1.22235649\n",
      "Iteration 9, loss = 1.18319923\n",
      "Iteration 10, loss = 1.14380230\n",
      "Iteration 11, loss = 1.10470368\n",
      "Iteration 12, loss = 1.06635691\n",
      "Iteration 13, loss = 1.02913414\n",
      "Iteration 14, loss = 0.99333035\n",
      "Iteration 15, loss = 0.95916855\n",
      "Iteration 16, loss = 0.92680593\n",
      "Iteration 17, loss = 0.89634058\n",
      "Iteration 18, loss = 0.86781882\n",
      "Iteration 19, loss = 0.84124268\n",
      "Iteration 20, loss = 0.81657760\n",
      "Iteration 21, loss = 0.79375977\n",
      "Iteration 22, loss = 0.77270315\n",
      "Iteration 23, loss = 0.75330588\n",
      "Iteration 24, loss = 0.73545585\n",
      "Iteration 25, loss = 0.71903559\n",
      "Iteration 26, loss = 0.70392624\n",
      "Iteration 27, loss = 0.69001069\n",
      "Iteration 28, loss = 0.67717602\n",
      "Iteration 29, loss = 0.66531526\n",
      "Iteration 30, loss = 0.65432850\n",
      "Iteration 31, loss = 0.64412364\n",
      "Iteration 32, loss = 0.63461666\n",
      "Iteration 33, loss = 0.62573166\n",
      "Iteration 34, loss = 0.61740062\n",
      "Iteration 35, loss = 0.60956303\n",
      "Iteration 36, loss = 0.60216541\n",
      "Iteration 37, loss = 0.59516078\n",
      "Iteration 38, loss = 0.58850806\n",
      "Iteration 39, loss = 0.58217148\n",
      "Iteration 40, loss = 0.57612004\n",
      "Iteration 41, loss = 0.57032690\n",
      "Iteration 42, loss = 0.56476891\n",
      "Iteration 43, loss = 0.55942613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44, loss = 0.55428134\n",
      "Iteration 45, loss = 0.54931970\n",
      "Iteration 46, loss = 0.54452837\n",
      "Iteration 47, loss = 0.53989620\n",
      "Iteration 48, loss = 0.53541344\n",
      "Iteration 49, loss = 0.53107155\n",
      "Iteration 50, loss = 0.52686294\n",
      "Iteration 51, loss = 0.52278085\n",
      "Iteration 52, loss = 0.51881914\n",
      "Iteration 53, loss = 0.51497225\n",
      "Iteration 54, loss = 0.51123503\n",
      "Iteration 55, loss = 0.50760270\n",
      "Iteration 56, loss = 0.50407078\n",
      "Iteration 57, loss = 0.50063502\n",
      "Iteration 58, loss = 0.49729139\n",
      "Iteration 59, loss = 0.49403604\n",
      "Iteration 60, loss = 0.49086528\n",
      "Iteration 61, loss = 0.48777556\n",
      "Iteration 62, loss = 0.48476346\n",
      "Iteration 63, loss = 0.48182569\n",
      "Iteration 64, loss = 0.47895910\n",
      "Iteration 65, loss = 0.47616065\n",
      "Iteration 66, loss = 0.47342742\n",
      "Iteration 67, loss = 0.47075663\n",
      "Iteration 68, loss = 0.46814559\n",
      "Iteration 69, loss = 0.46559177\n",
      "Iteration 70, loss = 0.46309274\n",
      "Iteration 71, loss = 0.46064618\n",
      "Iteration 72, loss = 0.45824991\n",
      "Iteration 73, loss = 0.45590186\n",
      "Iteration 74, loss = 0.45360005\n",
      "Iteration 75, loss = 0.45134264\n",
      "Iteration 76, loss = 0.44912786\n",
      "Iteration 77, loss = 0.44695407\n",
      "Iteration 78, loss = 0.44481969\n",
      "Iteration 79, loss = 0.44272326\n",
      "Iteration 80, loss = 0.44066338\n",
      "Iteration 81, loss = 0.43863874\n",
      "Iteration 82, loss = 0.43664809\n",
      "Iteration 83, loss = 0.43469027\n",
      "Iteration 84, loss = 0.43276415\n",
      "Iteration 85, loss = 0.43086868\n",
      "Iteration 86, loss = 0.42900285\n",
      "Iteration 87, loss = 0.42716573\n",
      "Iteration 88, loss = 0.42535639\n",
      "Iteration 89, loss = 0.42357397\n",
      "Iteration 90, loss = 0.42181764\n",
      "Iteration 91, loss = 0.42008663\n",
      "Iteration 92, loss = 0.41838017\n",
      "Iteration 93, loss = 0.41669753\n",
      "Iteration 94, loss = 0.41503803\n",
      "Iteration 95, loss = 0.41340100\n",
      "Iteration 96, loss = 0.41178580\n",
      "Iteration 97, loss = 0.41019181\n",
      "Iteration 98, loss = 0.40861845\n",
      "Iteration 99, loss = 0.40706515\n",
      "Iteration 100, loss = 0.40553135\n",
      "Iteration 101, loss = 0.40401654\n",
      "Iteration 102, loss = 0.40252020\n",
      "Iteration 103, loss = 0.40104186\n",
      "Iteration 104, loss = 0.39958103\n",
      "Iteration 105, loss = 0.39813728\n",
      "Iteration 106, loss = 0.39671015\n",
      "Iteration 107, loss = 0.39529924\n",
      "Iteration 108, loss = 0.39390413\n",
      "Iteration 109, loss = 0.39252444\n",
      "Iteration 110, loss = 0.39115978\n",
      "Iteration 111, loss = 0.38980981\n",
      "Iteration 112, loss = 0.38847416\n",
      "Iteration 113, loss = 0.38715250\n",
      "Iteration 114, loss = 0.38584451\n",
      "Iteration 115, loss = 0.38454986\n",
      "Iteration 116, loss = 0.38326826\n",
      "Iteration 117, loss = 0.38199941\n",
      "Iteration 118, loss = 0.38074302\n",
      "Iteration 119, loss = 0.37949882\n",
      "Iteration 120, loss = 0.37826655\n",
      "Iteration 121, loss = 0.37704594\n",
      "Iteration 122, loss = 0.37583676\n",
      "Iteration 123, loss = 0.37463875\n",
      "Iteration 124, loss = 0.37345168\n",
      "Iteration 125, loss = 0.37227532\n",
      "Iteration 126, loss = 0.37110946\n",
      "Iteration 127, loss = 0.36995388\n",
      "Iteration 128, loss = 0.36880837\n",
      "Iteration 129, loss = 0.36767273\n",
      "Iteration 130, loss = 0.36654677\n",
      "Iteration 131, loss = 0.36543029\n",
      "Iteration 132, loss = 0.36432312\n",
      "Iteration 133, loss = 0.36322506\n",
      "Iteration 134, loss = 0.36213595\n",
      "Iteration 135, loss = 0.36105561\n",
      "Iteration 136, loss = 0.35998389\n",
      "Iteration 137, loss = 0.35892061\n",
      "Iteration 138, loss = 0.35786562\n",
      "Iteration 139, loss = 0.35681877\n",
      "Iteration 140, loss = 0.35577992\n",
      "Iteration 141, loss = 0.35474891\n",
      "Iteration 142, loss = 0.35372560\n",
      "Iteration 143, loss = 0.35270986\n",
      "Iteration 144, loss = 0.35170155\n",
      "Iteration 145, loss = 0.35070054\n",
      "Iteration 146, loss = 0.34970671\n",
      "Iteration 147, loss = 0.34871993\n",
      "Iteration 148, loss = 0.34774008\n",
      "Iteration 149, loss = 0.34676704\n",
      "Iteration 150, loss = 0.34580069\n",
      "Iteration 151, loss = 0.34484093\n",
      "Iteration 152, loss = 0.34388765\n",
      "Iteration 153, loss = 0.34294073\n",
      "Iteration 154, loss = 0.34200007\n",
      "Iteration 155, loss = 0.34106557\n",
      "Iteration 156, loss = 0.34013713\n",
      "Iteration 157, loss = 0.33921465\n",
      "Iteration 158, loss = 0.33829804\n",
      "Iteration 159, loss = 0.33738721\n",
      "Iteration 160, loss = 0.33648206\n",
      "Iteration 161, loss = 0.33558250\n",
      "Iteration 162, loss = 0.33468846\n",
      "Iteration 163, loss = 0.33379983\n",
      "Iteration 164, loss = 0.33291655\n",
      "Iteration 165, loss = 0.33203852\n",
      "Iteration 166, loss = 0.33116568\n",
      "Iteration 167, loss = 0.33029794\n",
      "Iteration 168, loss = 0.32943522\n",
      "Iteration 169, loss = 0.32857746\n",
      "Iteration 170, loss = 0.32772458\n",
      "Iteration 171, loss = 0.32687650\n",
      "Iteration 172, loss = 0.32603317\n",
      "Iteration 173, loss = 0.32519451\n",
      "Iteration 174, loss = 0.32436045\n",
      "Iteration 175, loss = 0.32353093\n",
      "Iteration 176, loss = 0.32270589\n",
      "Iteration 177, loss = 0.32188526\n",
      "Iteration 178, loss = 0.32106899\n",
      "Iteration 179, loss = 0.32025701\n",
      "Iteration 180, loss = 0.31944926\n",
      "Iteration 181, loss = 0.31864569\n",
      "Iteration 182, loss = 0.31784624\n",
      "Iteration 183, loss = 0.31705085\n",
      "Iteration 184, loss = 0.31625948\n",
      "Iteration 185, loss = 0.31547207\n",
      "Iteration 186, loss = 0.31468856\n",
      "Iteration 187, loss = 0.31390891\n",
      "Iteration 188, loss = 0.31313307\n",
      "Iteration 189, loss = 0.31236098\n",
      "Iteration 190, loss = 0.31159261\n",
      "Iteration 191, loss = 0.31082790\n",
      "Iteration 192, loss = 0.31006681\n",
      "Iteration 193, loss = 0.30930929\n",
      "Iteration 194, loss = 0.30855530\n",
      "Iteration 195, loss = 0.30780479\n",
      "Iteration 196, loss = 0.30705773\n",
      "Iteration 197, loss = 0.30631408\n",
      "Iteration 198, loss = 0.30557378\n",
      "Iteration 199, loss = 0.30483680\n",
      "Iteration 200, loss = 0.30410311\n",
      "Iteration 201, loss = 0.30337266\n",
      "Iteration 202, loss = 0.30264542\n",
      "Iteration 203, loss = 0.30192135\n",
      "Iteration 204, loss = 0.30120041\n",
      "Iteration 205, loss = 0.30048257\n",
      "Iteration 206, loss = 0.29976779\n",
      "Iteration 207, loss = 0.29905604\n",
      "Iteration 208, loss = 0.29834728\n",
      "Iteration 209, loss = 0.29764149\n",
      "Iteration 210, loss = 0.29693863\n",
      "Iteration 211, loss = 0.29623867\n",
      "Iteration 212, loss = 0.29554158\n",
      "Iteration 213, loss = 0.29484732\n",
      "Iteration 214, loss = 0.29415588\n",
      "Iteration 215, loss = 0.29346721\n",
      "Iteration 216, loss = 0.29278129\n",
      "Iteration 217, loss = 0.29209809\n",
      "Iteration 218, loss = 0.29141758\n",
      "Iteration 219, loss = 0.29073974\n",
      "Iteration 220, loss = 0.29006454\n",
      "Iteration 221, loss = 0.28939196\n",
      "Iteration 222, loss = 0.28872196\n",
      "Iteration 223, loss = 0.28805453\n",
      "Iteration 224, loss = 0.28738963\n",
      "Iteration 225, loss = 0.28672725\n",
      "Iteration 226, loss = 0.28606736\n",
      "Iteration 227, loss = 0.28540993\n",
      "Iteration 228, loss = 0.28475495\n",
      "Iteration 229, loss = 0.28410239\n",
      "Iteration 230, loss = 0.28345222\n",
      "Iteration 231, loss = 0.28280444\n",
      "Iteration 232, loss = 0.28215901\n",
      "Iteration 233, loss = 0.28151592\n",
      "Iteration 234, loss = 0.28087514\n",
      "Iteration 235, loss = 0.28023666\n",
      "Iteration 236, loss = 0.27960045\n",
      "Iteration 237, loss = 0.27896650\n",
      "Iteration 238, loss = 0.27833478\n",
      "Iteration 239, loss = 0.27770528\n",
      "Iteration 240, loss = 0.27707798\n",
      "Iteration 241, loss = 0.27645286\n",
      "Iteration 242, loss = 0.27582991\n",
      "Iteration 243, loss = 0.27520910\n",
      "Iteration 244, loss = 0.27459042\n",
      "Iteration 245, loss = 0.27397385\n",
      "Iteration 246, loss = 0.27335938\n",
      "Iteration 247, loss = 0.27274699\n",
      "Iteration 248, loss = 0.27213666\n",
      "Iteration 249, loss = 0.27152839\n",
      "Iteration 250, loss = 0.27092214\n",
      "Iteration 251, loss = 0.27031792\n",
      "Iteration 252, loss = 0.26971570\n",
      "Iteration 253, loss = 0.26911546\n",
      "Iteration 254, loss = 0.26851721\n",
      "Iteration 255, loss = 0.26792091\n",
      "Iteration 256, loss = 0.26732657\n",
      "Iteration 257, loss = 0.26673415\n",
      "Iteration 258, loss = 0.26614366\n",
      "Iteration 259, loss = 0.26555508\n",
      "Iteration 260, loss = 0.26496839\n",
      "Iteration 261, loss = 0.26438359\n",
      "Iteration 262, loss = 0.26380066\n",
      "Iteration 263, loss = 0.26321958\n",
      "Iteration 264, loss = 0.26264036\n",
      "Iteration 265, loss = 0.26206297\n",
      "Iteration 266, loss = 0.26148741\n",
      "Iteration 267, loss = 0.26091366\n",
      "Iteration 268, loss = 0.26034171\n",
      "Iteration 269, loss = 0.25977156\n",
      "Iteration 270, loss = 0.25920318\n",
      "Iteration 271, loss = 0.25863658\n",
      "Iteration 272, loss = 0.25807174\n",
      "Iteration 273, loss = 0.25750865\n",
      "Iteration 274, loss = 0.25694731\n",
      "Iteration 275, loss = 0.25638769\n",
      "Iteration 276, loss = 0.25582980\n",
      "Iteration 277, loss = 0.25527362\n",
      "Iteration 278, loss = 0.25471915\n",
      "Iteration 279, loss = 0.25416637\n",
      "Iteration 280, loss = 0.25361528\n",
      "Iteration 281, loss = 0.25306587\n",
      "Iteration 282, loss = 0.25251813\n",
      "Iteration 283, loss = 0.25197205\n",
      "Iteration 284, loss = 0.25142763\n",
      "Iteration 285, loss = 0.25088485\n",
      "Iteration 286, loss = 0.25034371\n",
      "Iteration 287, loss = 0.24980420\n",
      "Iteration 288, loss = 0.24926632\n",
      "Iteration 289, loss = 0.24873005\n",
      "Iteration 290, loss = 0.24819539\n",
      "Iteration 291, loss = 0.24766233\n",
      "Iteration 292, loss = 0.24713087\n",
      "Iteration 293, loss = 0.24660100\n",
      "Iteration 294, loss = 0.24607271\n",
      "Iteration 295, loss = 0.24554599\n",
      "Iteration 296, loss = 0.24502084\n",
      "Iteration 297, loss = 0.24449726\n",
      "Iteration 298, loss = 0.24397523\n",
      "Iteration 299, loss = 0.24345475\n",
      "Iteration 300, loss = 0.24293582\n",
      "Iteration 301, loss = 0.24241842\n",
      "Iteration 302, loss = 0.24190256\n",
      "Iteration 303, loss = 0.24138823\n",
      "Iteration 304, loss = 0.24087542\n",
      "Iteration 305, loss = 0.24036412\n",
      "Iteration 306, loss = 0.23985433\n",
      "Iteration 307, loss = 0.23934606\n",
      "Iteration 308, loss = 0.23883928\n",
      "Iteration 309, loss = 0.23833400\n",
      "Iteration 310, loss = 0.23783020\n",
      "Iteration 311, loss = 0.23732790\n",
      "Iteration 312, loss = 0.23682708\n",
      "Iteration 313, loss = 0.23632773\n",
      "Iteration 314, loss = 0.23582986\n",
      "Iteration 315, loss = 0.23533345\n",
      "Iteration 316, loss = 0.23483851\n",
      "Iteration 317, loss = 0.23434503\n",
      "Iteration 318, loss = 0.23385300\n",
      "Iteration 319, loss = 0.23336242\n",
      "Iteration 320, loss = 0.23287329\n",
      "Iteration 321, loss = 0.23238560\n",
      "Iteration 322, loss = 0.23189935\n",
      "Iteration 323, loss = 0.23141454\n",
      "Iteration 324, loss = 0.23093115\n",
      "Iteration 325, loss = 0.23044920\n",
      "Iteration 326, loss = 0.22996866\n",
      "Iteration 327, loss = 0.22948955\n",
      "Iteration 328, loss = 0.22901186\n",
      "Iteration 329, loss = 0.22853557\n",
      "Iteration 330, loss = 0.22806070\n",
      "Iteration 331, loss = 0.22758724\n",
      "Iteration 332, loss = 0.22711517\n",
      "Iteration 333, loss = 0.22664451\n",
      "Iteration 334, loss = 0.22617524\n",
      "Iteration 335, loss = 0.22570737\n",
      "Iteration 336, loss = 0.22524089\n",
      "Iteration 337, loss = 0.22477579\n",
      "Iteration 338, loss = 0.22431208\n",
      "Iteration 339, loss = 0.22384975\n",
      "Iteration 340, loss = 0.22338880\n",
      "Iteration 341, loss = 0.22292922\n",
      "Iteration 342, loss = 0.22247102\n",
      "Iteration 343, loss = 0.22201419\n",
      "Iteration 344, loss = 0.22155872\n",
      "Iteration 345, loss = 0.22110462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 346, loss = 0.22065188\n",
      "Iteration 347, loss = 0.22020050\n",
      "Iteration 348, loss = 0.21975048\n",
      "Iteration 349, loss = 0.21930181\n",
      "Iteration 350, loss = 0.21885450\n",
      "Iteration 351, loss = 0.21840853\n",
      "Iteration 352, loss = 0.21796391\n",
      "Iteration 353, loss = 0.21752063\n",
      "Iteration 354, loss = 0.21707870\n",
      "Iteration 355, loss = 0.21663810\n",
      "Iteration 356, loss = 0.21619884\n",
      "Iteration 357, loss = 0.21576092\n",
      "Iteration 358, loss = 0.21532433\n",
      "Iteration 359, loss = 0.21488907\n",
      "Iteration 360, loss = 0.21445514\n",
      "Iteration 361, loss = 0.21402253\n",
      "Iteration 362, loss = 0.21359125\n",
      "Iteration 363, loss = 0.21316129\n",
      "Iteration 364, loss = 0.21273265\n",
      "Iteration 365, loss = 0.21230533\n",
      "Iteration 366, loss = 0.21187932\n",
      "Iteration 367, loss = 0.21145463\n",
      "Iteration 368, loss = 0.21103124\n",
      "Iteration 369, loss = 0.21060917\n",
      "Iteration 370, loss = 0.21018840\n",
      "Iteration 371, loss = 0.20976894\n",
      "Iteration 372, loss = 0.20935078\n",
      "Iteration 373, loss = 0.20893392\n",
      "Iteration 374, loss = 0.20851836\n",
      "Iteration 375, loss = 0.20810409\n",
      "Iteration 376, loss = 0.20769112\n",
      "Iteration 377, loss = 0.20727945\n",
      "Iteration 378, loss = 0.20686906\n",
      "Iteration 379, loss = 0.20645997\n",
      "Iteration 380, loss = 0.20605216\n",
      "Iteration 381, loss = 0.20564563\n",
      "Iteration 382, loss = 0.20524039\n",
      "Iteration 383, loss = 0.20483643\n",
      "Iteration 384, loss = 0.20443375\n",
      "Iteration 385, loss = 0.20403235\n",
      "Iteration 386, loss = 0.20363222\n",
      "Iteration 387, loss = 0.20323337\n",
      "Iteration 388, loss = 0.20283579\n",
      "Iteration 389, loss = 0.20243948\n",
      "Iteration 390, loss = 0.20204443\n",
      "Iteration 391, loss = 0.20165066\n",
      "Iteration 392, loss = 0.20125814\n",
      "Iteration 393, loss = 0.20086689\n",
      "Iteration 394, loss = 0.20047690\n",
      "Iteration 395, loss = 0.20008817\n",
      "Iteration 396, loss = 0.19970069\n",
      "Iteration 397, loss = 0.19931447\n",
      "Iteration 398, loss = 0.19892950\n",
      "Iteration 399, loss = 0.19854578\n",
      "Iteration 400, loss = 0.19816331\n",
      "Iteration 401, loss = 0.19778209\n",
      "Iteration 402, loss = 0.19740211\n",
      "Iteration 403, loss = 0.19702338\n",
      "Iteration 404, loss = 0.19664589\n",
      "Iteration 405, loss = 0.19626963\n",
      "Iteration 406, loss = 0.19589462\n",
      "Iteration 407, loss = 0.19552084\n",
      "Iteration 408, loss = 0.19514829\n",
      "Iteration 409, loss = 0.19477697\n",
      "Iteration 410, loss = 0.19440689\n",
      "Iteration 411, loss = 0.19403803\n",
      "Iteration 412, loss = 0.19367039\n",
      "Iteration 413, loss = 0.19330398\n",
      "Iteration 414, loss = 0.19293880\n",
      "Iteration 415, loss = 0.19257483\n",
      "Iteration 416, loss = 0.19221208\n",
      "Iteration 417, loss = 0.19185054\n",
      "Iteration 418, loss = 0.19149022\n",
      "Iteration 419, loss = 0.19113111\n",
      "Iteration 420, loss = 0.19077320\n",
      "Iteration 421, loss = 0.19041651\n",
      "Iteration 422, loss = 0.19006102\n",
      "Iteration 423, loss = 0.18970674\n",
      "Iteration 424, loss = 0.18935365\n",
      "Iteration 425, loss = 0.18900177\n",
      "Iteration 426, loss = 0.18865108\n",
      "Iteration 427, loss = 0.18830158\n",
      "Iteration 428, loss = 0.18795328\n",
      "Iteration 429, loss = 0.18760617\n",
      "Iteration 430, loss = 0.18726025\n",
      "Iteration 431, loss = 0.18691551\n",
      "Iteration 432, loss = 0.18657196\n",
      "Iteration 433, loss = 0.18622959\n",
      "Iteration 434, loss = 0.18588840\n",
      "Iteration 435, loss = 0.18554839\n",
      "Iteration 436, loss = 0.18520955\n",
      "Iteration 437, loss = 0.18487188\n",
      "Iteration 438, loss = 0.18453539\n",
      "Iteration 439, loss = 0.18420006\n",
      "Iteration 440, loss = 0.18386590\n",
      "Iteration 441, loss = 0.18353290\n",
      "Iteration 442, loss = 0.18320107\n",
      "Iteration 443, loss = 0.18287039\n",
      "Iteration 444, loss = 0.18254087\n",
      "Iteration 445, loss = 0.18221251\n",
      "Iteration 446, loss = 0.18188530\n",
      "Iteration 447, loss = 0.18155924\n",
      "Iteration 448, loss = 0.18123432\n",
      "Iteration 449, loss = 0.18091055\n",
      "Iteration 450, loss = 0.18058792\n",
      "Iteration 451, loss = 0.18026644\n",
      "Iteration 452, loss = 0.17994609\n",
      "Iteration 453, loss = 0.17962688\n",
      "Iteration 454, loss = 0.17930879\n",
      "Iteration 455, loss = 0.17899185\n",
      "Iteration 456, loss = 0.17867602\n",
      "Iteration 457, loss = 0.17836133\n",
      "Iteration 458, loss = 0.17804775\n",
      "Iteration 459, loss = 0.17773530\n",
      "Iteration 460, loss = 0.17742397\n",
      "Iteration 461, loss = 0.17711375\n",
      "Iteration 462, loss = 0.17680464\n",
      "Iteration 463, loss = 0.17649664\n",
      "Iteration 464, loss = 0.17618976\n",
      "Iteration 465, loss = 0.17588397\n",
      "Iteration 466, loss = 0.17557929\n",
      "Iteration 467, loss = 0.17527571\n",
      "Iteration 468, loss = 0.17497323\n",
      "Iteration 469, loss = 0.17467184\n",
      "Iteration 470, loss = 0.17437155\n",
      "Iteration 471, loss = 0.17407234\n",
      "Iteration 472, loss = 0.17377422\n",
      "Iteration 473, loss = 0.17347718\n",
      "Iteration 474, loss = 0.17318123\n",
      "Iteration 475, loss = 0.17288635\n",
      "Iteration 476, loss = 0.17259255\n",
      "Iteration 477, loss = 0.17229983\n",
      "Iteration 478, loss = 0.17200817\n",
      "Iteration 479, loss = 0.17171758\n",
      "Iteration 480, loss = 0.17142806\n",
      "Iteration 481, loss = 0.17113960\n",
      "Iteration 482, loss = 0.17085220\n",
      "Iteration 483, loss = 0.17056585\n",
      "Iteration 484, loss = 0.17028056\n",
      "Iteration 485, loss = 0.16999632\n",
      "Iteration 486, loss = 0.16971313\n",
      "Iteration 487, loss = 0.16943099\n",
      "Iteration 488, loss = 0.16914988\n",
      "Iteration 489, loss = 0.16886982\n",
      "Iteration 490, loss = 0.16859079\n",
      "Iteration 491, loss = 0.16831280\n",
      "Iteration 492, loss = 0.16803584\n",
      "Iteration 493, loss = 0.16775991\n",
      "Iteration 494, loss = 0.16748500\n",
      "Iteration 495, loss = 0.16721111\n",
      "Iteration 496, loss = 0.16693825\n",
      "Iteration 497, loss = 0.16666640\n",
      "Iteration 498, loss = 0.16639557\n",
      "Iteration 499, loss = 0.16612574\n",
      "Iteration 500, loss = 0.16585693\n",
      "Iteration 1, loss = 1.43656023\n",
      "Iteration 2, loss = 1.41996047\n",
      "Iteration 3, loss = 1.39675991\n",
      "Iteration 4, loss = 1.36809032\n",
      "Iteration 5, loss = 1.33504331\n",
      "Iteration 6, loss = 1.29864834\n",
      "Iteration 7, loss = 1.25985690\n",
      "Iteration 8, loss = 1.21953206\n",
      "Iteration 9, loss = 1.17844218\n",
      "Iteration 10, loss = 1.13725812\n",
      "Iteration 11, loss = 1.09655311\n",
      "Iteration 12, loss = 1.05680488\n",
      "Iteration 13, loss = 1.01839954\n",
      "Iteration 14, loss = 0.98163692\n",
      "Iteration 15, loss = 0.94673706\n",
      "Iteration 16, loss = 0.91384767\n",
      "Iteration 17, loss = 0.88305238\n",
      "Iteration 18, loss = 0.85437932\n",
      "Iteration 19, loss = 0.82781008\n",
      "Iteration 20, loss = 0.80328840\n",
      "Iteration 21, loss = 0.78072863\n",
      "Iteration 22, loss = 0.76002352\n",
      "Iteration 23, loss = 0.74105121\n",
      "Iteration 24, loss = 0.72368128\n",
      "Iteration 25, loss = 0.70777993\n",
      "Iteration 26, loss = 0.69321399\n",
      "Iteration 27, loss = 0.67985419\n",
      "Iteration 28, loss = 0.66757747\n",
      "Iteration 29, loss = 0.65626860\n",
      "Iteration 30, loss = 0.64582122\n",
      "Iteration 31, loss = 0.63613830\n",
      "Iteration 32, loss = 0.62713227\n",
      "Iteration 33, loss = 0.61872482\n",
      "Iteration 34, loss = 0.61084654\n",
      "Iteration 35, loss = 0.60343632\n",
      "Iteration 36, loss = 0.59644073\n",
      "Iteration 37, loss = 0.58981336\n",
      "Iteration 38, loss = 0.58351407\n",
      "Iteration 39, loss = 0.57750832\n",
      "Iteration 40, loss = 0.57176652\n",
      "Iteration 41, loss = 0.56626337\n",
      "Iteration 42, loss = 0.56097728\n",
      "Iteration 43, loss = 0.55588988\n",
      "Iteration 44, loss = 0.55098549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.54625073\n",
      "Iteration 46, loss = 0.54167412\n",
      "Iteration 47, loss = 0.53724579\n",
      "Iteration 48, loss = 0.53295714\n",
      "Iteration 49, loss = 0.52880065\n",
      "Iteration 50, loss = 0.52476966\n",
      "Iteration 51, loss = 0.52085820\n",
      "Iteration 52, loss = 0.51706083\n",
      "Iteration 53, loss = 0.51337258\n",
      "Iteration 54, loss = 0.50978880\n",
      "Iteration 55, loss = 0.50630514\n",
      "Iteration 56, loss = 0.50291746\n",
      "Iteration 57, loss = 0.49962180\n",
      "Iteration 58, loss = 0.49641437\n",
      "Iteration 59, loss = 0.49329152\n",
      "Iteration 60, loss = 0.49024971\n",
      "Iteration 61, loss = 0.48728552\n",
      "Iteration 62, loss = 0.48439565\n",
      "Iteration 63, loss = 0.48157692\n",
      "Iteration 64, loss = 0.47882626\n",
      "Iteration 65, loss = 0.47614071\n",
      "Iteration 66, loss = 0.47351743\n",
      "Iteration 67, loss = 0.47095373\n",
      "Iteration 68, loss = 0.46844700\n",
      "Iteration 69, loss = 0.46599479\n",
      "Iteration 70, loss = 0.46359475\n",
      "Iteration 71, loss = 0.46124466\n",
      "Iteration 72, loss = 0.45894243\n",
      "Iteration 73, loss = 0.45668607\n",
      "Iteration 74, loss = 0.45447371\n",
      "Iteration 75, loss = 0.45230357\n",
      "Iteration 76, loss = 0.45017401\n",
      "Iteration 77, loss = 0.44808345\n",
      "Iteration 78, loss = 0.44603042\n",
      "Iteration 79, loss = 0.44401352\n",
      "Iteration 80, loss = 0.44203145\n",
      "Iteration 81, loss = 0.44008297\n",
      "Iteration 82, loss = 0.43816690\n",
      "Iteration 83, loss = 0.43628215\n",
      "Iteration 84, loss = 0.43442764\n",
      "Iteration 85, loss = 0.43260240\n",
      "Iteration 86, loss = 0.43080547\n",
      "Iteration 87, loss = 0.42903595\n",
      "Iteration 88, loss = 0.42729298\n",
      "Iteration 89, loss = 0.42557573\n",
      "Iteration 90, loss = 0.42388342\n",
      "Iteration 91, loss = 0.42221529\n",
      "Iteration 92, loss = 0.42057062\n",
      "Iteration 93, loss = 0.41894872\n",
      "Iteration 94, loss = 0.41734892\n",
      "Iteration 95, loss = 0.41577058\n",
      "Iteration 96, loss = 0.41421308\n",
      "Iteration 97, loss = 0.41267584\n",
      "Iteration 98, loss = 0.41115829\n",
      "Iteration 99, loss = 0.40965987\n",
      "Iteration 100, loss = 0.40818006\n",
      "Iteration 101, loss = 0.40671836\n",
      "Iteration 102, loss = 0.40527428\n",
      "Iteration 103, loss = 0.40384734\n",
      "Iteration 104, loss = 0.40243710\n",
      "Iteration 105, loss = 0.40104311\n",
      "Iteration 106, loss = 0.39966497\n",
      "Iteration 107, loss = 0.39830226\n",
      "Iteration 108, loss = 0.39695460\n",
      "Iteration 109, loss = 0.39562161\n",
      "Iteration 110, loss = 0.39430293\n",
      "Iteration 111, loss = 0.39299821\n",
      "Iteration 112, loss = 0.39170711\n",
      "Iteration 113, loss = 0.39042932\n",
      "Iteration 114, loss = 0.38916452\n",
      "Iteration 115, loss = 0.38791240\n",
      "Iteration 116, loss = 0.38667267\n",
      "Iteration 117, loss = 0.38544505\n",
      "Iteration 118, loss = 0.38422927\n",
      "Iteration 119, loss = 0.38302507\n",
      "Iteration 120, loss = 0.38183218\n",
      "Iteration 121, loss = 0.38065036\n",
      "Iteration 122, loss = 0.37947937\n",
      "Iteration 123, loss = 0.37831897\n",
      "Iteration 124, loss = 0.37716895\n",
      "Iteration 125, loss = 0.37602907\n",
      "Iteration 126, loss = 0.37489914\n",
      "Iteration 127, loss = 0.37377893\n",
      "Iteration 128, loss = 0.37266826\n",
      "Iteration 129, loss = 0.37156693\n",
      "Iteration 130, loss = 0.37047474\n",
      "Iteration 131, loss = 0.36939151\n",
      "Iteration 132, loss = 0.36831708\n",
      "Iteration 133, loss = 0.36725125\n",
      "Iteration 134, loss = 0.36619386\n",
      "Iteration 135, loss = 0.36514476\n",
      "Iteration 136, loss = 0.36410376\n",
      "Iteration 137, loss = 0.36307074\n",
      "Iteration 138, loss = 0.36204552\n",
      "Iteration 139, loss = 0.36102796\n",
      "Iteration 140, loss = 0.36001792\n",
      "Iteration 141, loss = 0.35901526\n",
      "Iteration 142, loss = 0.35801984\n",
      "Iteration 143, loss = 0.35703153\n",
      "Iteration 144, loss = 0.35605020\n",
      "Iteration 145, loss = 0.35507572\n",
      "Iteration 146, loss = 0.35410797\n",
      "Iteration 147, loss = 0.35314684\n",
      "Iteration 148, loss = 0.35219219\n",
      "Iteration 149, loss = 0.35124393\n",
      "Iteration 150, loss = 0.35030193\n",
      "Iteration 151, loss = 0.34936609\n",
      "Iteration 152, loss = 0.34843631\n",
      "Iteration 153, loss = 0.34751248\n",
      "Iteration 154, loss = 0.34659449\n",
      "Iteration 155, loss = 0.34568226\n",
      "Iteration 156, loss = 0.34477568\n",
      "Iteration 157, loss = 0.34387467\n",
      "Iteration 158, loss = 0.34297912\n",
      "Iteration 159, loss = 0.34208896\n",
      "Iteration 160, loss = 0.34120409\n",
      "Iteration 161, loss = 0.34032442\n",
      "Iteration 162, loss = 0.33944988\n",
      "Iteration 163, loss = 0.33858039\n",
      "Iteration 164, loss = 0.33771586\n",
      "Iteration 165, loss = 0.33685621\n",
      "Iteration 166, loss = 0.33600138\n",
      "Iteration 167, loss = 0.33515128\n",
      "Iteration 168, loss = 0.33430584\n",
      "Iteration 169, loss = 0.33346500\n",
      "Iteration 170, loss = 0.33262868\n",
      "Iteration 171, loss = 0.33179682\n",
      "Iteration 172, loss = 0.33096934\n",
      "Iteration 173, loss = 0.33014619\n",
      "Iteration 174, loss = 0.32932730\n",
      "Iteration 175, loss = 0.32851261\n",
      "Iteration 176, loss = 0.32770206\n",
      "Iteration 177, loss = 0.32689559\n",
      "Iteration 178, loss = 0.32609314\n",
      "Iteration 179, loss = 0.32529465\n",
      "Iteration 180, loss = 0.32450007\n",
      "Iteration 181, loss = 0.32370935\n",
      "Iteration 182, loss = 0.32292243\n",
      "Iteration 183, loss = 0.32213926\n",
      "Iteration 184, loss = 0.32135978\n",
      "Iteration 185, loss = 0.32058395\n",
      "Iteration 186, loss = 0.31981173\n",
      "Iteration 187, loss = 0.31904305\n",
      "Iteration 188, loss = 0.31827788\n",
      "Iteration 189, loss = 0.31751617\n",
      "Iteration 190, loss = 0.31675787\n",
      "Iteration 191, loss = 0.31600294\n",
      "Iteration 192, loss = 0.31525134\n",
      "Iteration 193, loss = 0.31450303\n",
      "Iteration 194, loss = 0.31375796\n",
      "Iteration 195, loss = 0.31301609\n",
      "Iteration 196, loss = 0.31227739\n",
      "Iteration 197, loss = 0.31154181\n",
      "Iteration 198, loss = 0.31080932\n",
      "Iteration 199, loss = 0.31007988\n",
      "Iteration 200, loss = 0.30935346\n",
      "Iteration 201, loss = 0.30863001\n",
      "Iteration 202, loss = 0.30790951\n",
      "Iteration 203, loss = 0.30719191\n",
      "Iteration 204, loss = 0.30647719\n",
      "Iteration 205, loss = 0.30576532\n",
      "Iteration 206, loss = 0.30505625\n",
      "Iteration 207, loss = 0.30434996\n",
      "Iteration 208, loss = 0.30364643\n",
      "Iteration 209, loss = 0.30294560\n",
      "Iteration 210, loss = 0.30224747\n",
      "Iteration 211, loss = 0.30155200\n",
      "Iteration 212, loss = 0.30085916\n",
      "Iteration 213, loss = 0.30016892\n",
      "Iteration 214, loss = 0.29948125\n",
      "Iteration 215, loss = 0.29879614\n",
      "Iteration 216, loss = 0.29811355\n",
      "Iteration 217, loss = 0.29743345\n",
      "Iteration 218, loss = 0.29675583\n",
      "Iteration 219, loss = 0.29608065\n",
      "Iteration 220, loss = 0.29540789\n",
      "Iteration 221, loss = 0.29473754\n",
      "Iteration 222, loss = 0.29406956\n",
      "Iteration 223, loss = 0.29340393\n",
      "Iteration 224, loss = 0.29274063\n",
      "Iteration 225, loss = 0.29207964\n",
      "Iteration 226, loss = 0.29142094\n",
      "Iteration 227, loss = 0.29076451\n",
      "Iteration 228, loss = 0.29011032\n",
      "Iteration 229, loss = 0.28945836\n",
      "Iteration 230, loss = 0.28880860\n",
      "Iteration 231, loss = 0.28816103\n",
      "Iteration 232, loss = 0.28751562\n",
      "Iteration 233, loss = 0.28687236\n",
      "Iteration 234, loss = 0.28623124\n",
      "Iteration 235, loss = 0.28559223\n",
      "Iteration 236, loss = 0.28495531\n",
      "Iteration 237, loss = 0.28432047\n",
      "Iteration 238, loss = 0.28368769\n",
      "Iteration 239, loss = 0.28305696\n",
      "Iteration 240, loss = 0.28242825\n",
      "Iteration 241, loss = 0.28180156\n",
      "Iteration 242, loss = 0.28117686\n",
      "Iteration 243, loss = 0.28055415\n",
      "Iteration 244, loss = 0.27993340\n",
      "Iteration 245, loss = 0.27931460\n",
      "Iteration 246, loss = 0.27869775\n",
      "Iteration 247, loss = 0.27808282\n",
      "Iteration 248, loss = 0.27746979\n",
      "Iteration 249, loss = 0.27685867\n",
      "Iteration 250, loss = 0.27624943\n",
      "Iteration 251, loss = 0.27564206\n",
      "Iteration 252, loss = 0.27503655\n",
      "Iteration 253, loss = 0.27443289\n",
      "Iteration 254, loss = 0.27383106\n",
      "Iteration 255, loss = 0.27323105\n",
      "Iteration 256, loss = 0.27263285\n",
      "Iteration 257, loss = 0.27203645\n",
      "Iteration 258, loss = 0.27144184\n",
      "Iteration 259, loss = 0.27084901\n",
      "Iteration 260, loss = 0.27025794\n",
      "Iteration 261, loss = 0.26966863\n",
      "Iteration 262, loss = 0.26908107\n",
      "Iteration 263, loss = 0.26849524\n",
      "Iteration 264, loss = 0.26791113\n",
      "Iteration 265, loss = 0.26732875\n",
      "Iteration 266, loss = 0.26674807\n",
      "Iteration 267, loss = 0.26616909\n",
      "Iteration 268, loss = 0.26559179\n",
      "Iteration 269, loss = 0.26501618\n",
      "Iteration 270, loss = 0.26444224\n",
      "Iteration 271, loss = 0.26386996\n",
      "Iteration 272, loss = 0.26329934\n",
      "Iteration 273, loss = 0.26273036\n",
      "Iteration 274, loss = 0.26216303\n",
      "Iteration 275, loss = 0.26159732\n",
      "Iteration 276, loss = 0.26103324\n",
      "Iteration 277, loss = 0.26047077\n",
      "Iteration 278, loss = 0.25990991\n",
      "Iteration 279, loss = 0.25935065\n",
      "Iteration 280, loss = 0.25879299\n",
      "Iteration 281, loss = 0.25823692\n",
      "Iteration 282, loss = 0.25768242\n",
      "Iteration 283, loss = 0.25712951\n",
      "Iteration 284, loss = 0.25657816\n",
      "Iteration 285, loss = 0.25602837\n",
      "Iteration 286, loss = 0.25548014\n",
      "Iteration 287, loss = 0.25493346\n",
      "Iteration 288, loss = 0.25438832\n",
      "Iteration 289, loss = 0.25384473\n",
      "Iteration 290, loss = 0.25330266\n",
      "Iteration 291, loss = 0.25276213\n",
      "Iteration 292, loss = 0.25222311\n",
      "Iteration 293, loss = 0.25168562\n",
      "Iteration 294, loss = 0.25114964\n",
      "Iteration 295, loss = 0.25061516\n",
      "Iteration 296, loss = 0.25008219\n",
      "Iteration 297, loss = 0.24955071\n",
      "Iteration 298, loss = 0.24902073\n",
      "Iteration 299, loss = 0.24849224\n",
      "Iteration 300, loss = 0.24796523\n",
      "Iteration 301, loss = 0.24743970\n",
      "Iteration 302, loss = 0.24691565\n",
      "Iteration 303, loss = 0.24639307\n",
      "Iteration 304, loss = 0.24587195\n",
      "Iteration 305, loss = 0.24535230\n",
      "Iteration 306, loss = 0.24483411\n",
      "Iteration 307, loss = 0.24431738\n",
      "Iteration 308, loss = 0.24380210\n",
      "Iteration 309, loss = 0.24328826\n",
      "Iteration 310, loss = 0.24277588\n",
      "Iteration 311, loss = 0.24226493\n",
      "Iteration 312, loss = 0.24175542\n",
      "Iteration 313, loss = 0.24124735\n",
      "Iteration 314, loss = 0.24074071\n",
      "Iteration 315, loss = 0.24023550\n",
      "Iteration 316, loss = 0.23973172\n",
      "Iteration 317, loss = 0.23922936\n",
      "Iteration 318, loss = 0.23872841\n",
      "Iteration 319, loss = 0.23822889\n",
      "Iteration 320, loss = 0.23773078\n",
      "Iteration 321, loss = 0.23723408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 322, loss = 0.23673879\n",
      "Iteration 323, loss = 0.23624491\n",
      "Iteration 324, loss = 0.23575243\n",
      "Iteration 325, loss = 0.23526135\n",
      "Iteration 326, loss = 0.23477167\n",
      "Iteration 327, loss = 0.23428339\n",
      "Iteration 328, loss = 0.23379650\n",
      "Iteration 329, loss = 0.23331101\n",
      "Iteration 330, loss = 0.23282690\n",
      "Iteration 331, loss = 0.23234418\n",
      "Iteration 332, loss = 0.23186285\n",
      "Iteration 333, loss = 0.23138290\n",
      "Iteration 334, loss = 0.23090433\n",
      "Iteration 335, loss = 0.23042714\n",
      "Iteration 336, loss = 0.22995132\n",
      "Iteration 337, loss = 0.22947688\n",
      "Iteration 338, loss = 0.22900382\n",
      "Iteration 339, loss = 0.22853212\n",
      "Iteration 340, loss = 0.22806180\n",
      "Iteration 341, loss = 0.22759284\n",
      "Iteration 342, loss = 0.22712525\n",
      "Iteration 343, loss = 0.22665903\n",
      "Iteration 344, loss = 0.22619416\n",
      "Iteration 345, loss = 0.22573066\n",
      "Iteration 346, loss = 0.22526851\n",
      "Iteration 347, loss = 0.22480773\n",
      "Iteration 348, loss = 0.22434830\n",
      "Iteration 349, loss = 0.22389022\n",
      "Iteration 350, loss = 0.22343350\n",
      "Iteration 351, loss = 0.22297813\n",
      "Iteration 352, loss = 0.22252410\n",
      "Iteration 353, loss = 0.22207143\n",
      "Iteration 354, loss = 0.22162010\n",
      "Iteration 355, loss = 0.22117012\n",
      "Iteration 356, loss = 0.22072148\n",
      "Iteration 357, loss = 0.22027418\n",
      "Iteration 358, loss = 0.21982823\n",
      "Iteration 359, loss = 0.21938361\n",
      "Iteration 360, loss = 0.21894033\n",
      "Iteration 361, loss = 0.21849839\n",
      "Iteration 362, loss = 0.21805779\n",
      "Iteration 363, loss = 0.21761851\n",
      "Iteration 364, loss = 0.21718058\n",
      "Iteration 365, loss = 0.21674397\n",
      "Iteration 366, loss = 0.21630869\n",
      "Iteration 367, loss = 0.21587474\n",
      "Iteration 368, loss = 0.21544212\n",
      "Iteration 369, loss = 0.21501082\n",
      "Iteration 370, loss = 0.21458085\n",
      "Iteration 371, loss = 0.21415220\n",
      "Iteration 372, loss = 0.21372488\n",
      "Iteration 373, loss = 0.21329887\n",
      "Iteration 374, loss = 0.21287419\n",
      "Iteration 375, loss = 0.21245082\n",
      "Iteration 376, loss = 0.21202877\n",
      "Iteration 377, loss = 0.21160803\n",
      "Iteration 378, loss = 0.21118861\n",
      "Iteration 379, loss = 0.21077051\n",
      "Iteration 380, loss = 0.21035371\n",
      "Iteration 381, loss = 0.20993822\n",
      "Iteration 382, loss = 0.20952405\n",
      "Iteration 383, loss = 0.20911118\n",
      "Iteration 384, loss = 0.20869961\n",
      "Iteration 385, loss = 0.20828935\n",
      "Iteration 386, loss = 0.20788040\n",
      "Iteration 387, loss = 0.20747275\n",
      "Iteration 388, loss = 0.20706639\n",
      "Iteration 389, loss = 0.20666134\n",
      "Iteration 390, loss = 0.20625759\n",
      "Iteration 391, loss = 0.20585513\n",
      "Iteration 392, loss = 0.20545397\n",
      "Iteration 393, loss = 0.20505410\n",
      "Iteration 394, loss = 0.20465552\n",
      "Iteration 395, loss = 0.20425824\n",
      "Iteration 396, loss = 0.20386224\n",
      "Iteration 397, loss = 0.20346754\n",
      "Iteration 398, loss = 0.20307412\n",
      "Iteration 399, loss = 0.20268198\n",
      "Iteration 400, loss = 0.20229113\n",
      "Iteration 401, loss = 0.20190156\n",
      "Iteration 402, loss = 0.20151327\n",
      "Iteration 403, loss = 0.20112627\n",
      "Iteration 404, loss = 0.20074053\n",
      "Iteration 405, loss = 0.20035608\n",
      "Iteration 406, loss = 0.19997290\n",
      "Iteration 407, loss = 0.19959099\n",
      "Iteration 408, loss = 0.19921035\n",
      "Iteration 409, loss = 0.19883099\n",
      "Iteration 410, loss = 0.19845289\n",
      "Iteration 411, loss = 0.19807606\n",
      "Iteration 412, loss = 0.19770049\n",
      "Iteration 413, loss = 0.19732619\n",
      "Iteration 414, loss = 0.19695314\n",
      "Iteration 415, loss = 0.19658136\n",
      "Iteration 416, loss = 0.19621084\n",
      "Iteration 417, loss = 0.19584157\n",
      "Iteration 418, loss = 0.19547355\n",
      "Iteration 419, loss = 0.19510679\n",
      "Iteration 420, loss = 0.19474128\n",
      "Iteration 421, loss = 0.19437702\n",
      "Iteration 422, loss = 0.19401401\n",
      "Iteration 423, loss = 0.19365224\n",
      "Iteration 424, loss = 0.19329171\n",
      "Iteration 425, loss = 0.19293243\n",
      "Iteration 426, loss = 0.19257438\n",
      "Iteration 427, loss = 0.19221758\n",
      "Iteration 428, loss = 0.19186200\n",
      "Iteration 429, loss = 0.19150767\n",
      "Iteration 430, loss = 0.19115456\n",
      "Iteration 431, loss = 0.19080269\n",
      "Iteration 432, loss = 0.19045204\n",
      "Iteration 433, loss = 0.19010262\n",
      "Iteration 434, loss = 0.18975442\n",
      "Iteration 435, loss = 0.18940744\n",
      "Iteration 436, loss = 0.18906168\n",
      "Iteration 437, loss = 0.18871714\n",
      "Iteration 438, loss = 0.18837382\n",
      "Iteration 439, loss = 0.18803170\n",
      "Iteration 440, loss = 0.18769080\n",
      "Iteration 441, loss = 0.18735111\n",
      "Iteration 442, loss = 0.18701262\n",
      "Iteration 443, loss = 0.18667534\n",
      "Iteration 444, loss = 0.18633926\n",
      "Iteration 445, loss = 0.18600438\n",
      "Iteration 446, loss = 0.18567070\n",
      "Iteration 447, loss = 0.18533821\n",
      "Iteration 448, loss = 0.18500691\n",
      "Iteration 449, loss = 0.18467681\n",
      "Iteration 450, loss = 0.18434789\n",
      "Iteration 451, loss = 0.18402016\n",
      "Iteration 452, loss = 0.18369361\n",
      "Iteration 453, loss = 0.18336824\n",
      "Iteration 454, loss = 0.18304405\n",
      "Iteration 455, loss = 0.18272104\n",
      "Iteration 456, loss = 0.18239920\n",
      "Iteration 457, loss = 0.18207853\n",
      "Iteration 458, loss = 0.18175903\n",
      "Iteration 459, loss = 0.18144069\n",
      "Iteration 460, loss = 0.18112352\n",
      "Iteration 461, loss = 0.18080751\n",
      "Iteration 462, loss = 0.18049265\n",
      "Iteration 463, loss = 0.18017896\n",
      "Iteration 464, loss = 0.17986641\n",
      "Iteration 465, loss = 0.17955502\n",
      "Iteration 466, loss = 0.17924477\n",
      "Iteration 467, loss = 0.17893567\n",
      "Iteration 468, loss = 0.17862771\n",
      "Iteration 469, loss = 0.17832088\n",
      "Iteration 470, loss = 0.17801520\n",
      "Iteration 471, loss = 0.17771065\n",
      "Iteration 472, loss = 0.17740723\n",
      "Iteration 473, loss = 0.17710494\n",
      "Iteration 474, loss = 0.17680378\n",
      "Iteration 475, loss = 0.17650374\n",
      "Iteration 476, loss = 0.17620481\n",
      "Iteration 477, loss = 0.17590701\n",
      "Iteration 478, loss = 0.17561032\n",
      "Iteration 479, loss = 0.17531474\n",
      "Iteration 480, loss = 0.17502027\n",
      "Iteration 481, loss = 0.17472691\n",
      "Iteration 482, loss = 0.17443465\n",
      "Iteration 483, loss = 0.17414349\n",
      "Iteration 484, loss = 0.17385343\n",
      "Iteration 485, loss = 0.17356446\n",
      "Iteration 486, loss = 0.17327658\n",
      "Iteration 487, loss = 0.17298979\n",
      "Iteration 488, loss = 0.17270408\n",
      "Iteration 489, loss = 0.17241946\n",
      "Iteration 490, loss = 0.17213592\n",
      "Iteration 491, loss = 0.17185345\n",
      "Iteration 492, loss = 0.17157206\n",
      "Iteration 493, loss = 0.17129174\n",
      "Iteration 494, loss = 0.17101248\n",
      "Iteration 495, loss = 0.17073429\n",
      "Iteration 496, loss = 0.17045716\n",
      "Iteration 497, loss = 0.17018109\n",
      "Iteration 498, loss = 0.16990607\n",
      "Iteration 499, loss = 0.16963211\n",
      "Iteration 500, loss = 0.16935919\n",
      "Iteration 1, loss = 1.43194546\n",
      "Iteration 2, loss = 1.41590180\n",
      "Iteration 3, loss = 1.39346149\n",
      "Iteration 4, loss = 1.36570409\n",
      "Iteration 5, loss = 1.33367046\n",
      "Iteration 6, loss = 1.29834292\n",
      "Iteration 7, loss = 1.26063104\n",
      "Iteration 8, loss = 1.22136225\n",
      "Iteration 9, loss = 1.18127614\n",
      "Iteration 10, loss = 1.14102184\n",
      "Iteration 11, loss = 1.10115768\n",
      "Iteration 12, loss = 1.06215277\n",
      "Iteration 13, loss = 1.02439010\n",
      "Iteration 14, loss = 0.98817090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.95372015\n",
      "Iteration 16, loss = 0.92119300\n",
      "Iteration 17, loss = 0.89068213\n",
      "Iteration 18, loss = 0.86222549\n",
      "Iteration 19, loss = 0.83581466\n",
      "Iteration 20, loss = 0.81140305\n",
      "Iteration 21, loss = 0.78891405\n",
      "Iteration 22, loss = 0.76824860\n",
      "Iteration 23, loss = 0.74929212\n",
      "Iteration 24, loss = 0.73192054\n",
      "Iteration 25, loss = 0.71600540\n",
      "Iteration 26, loss = 0.70141805\n",
      "Iteration 27, loss = 0.68803289\n",
      "Iteration 28, loss = 0.67572979\n",
      "Iteration 29, loss = 0.66439577\n",
      "Iteration 30, loss = 0.65392611\n",
      "Iteration 31, loss = 0.64422492\n",
      "Iteration 32, loss = 0.63520533\n",
      "Iteration 33, loss = 0.62678935\n",
      "Iteration 34, loss = 0.61890758\n",
      "Iteration 35, loss = 0.61149867\n",
      "Iteration 36, loss = 0.60450881\n",
      "Iteration 37, loss = 0.59789101\n",
      "Iteration 38, loss = 0.59160452\n",
      "Iteration 39, loss = 0.58561410\n",
      "Iteration 40, loss = 0.57988947\n",
      "Iteration 41, loss = 0.57440465\n",
      "Iteration 42, loss = 0.56913742\n",
      "Iteration 43, loss = 0.56406880\n",
      "Iteration 44, loss = 0.55918260\n",
      "Iteration 45, loss = 0.55446500\n",
      "Iteration 46, loss = 0.54990415\n",
      "Iteration 47, loss = 0.54548989\n",
      "Iteration 48, loss = 0.54121344\n",
      "Iteration 49, loss = 0.53706713\n",
      "Iteration 50, loss = 0.53304426\n",
      "Iteration 51, loss = 0.52913885\n",
      "Iteration 52, loss = 0.52534554\n",
      "Iteration 53, loss = 0.52165946\n",
      "Iteration 54, loss = 0.51807609\n",
      "Iteration 55, loss = 0.51459126\n",
      "Iteration 56, loss = 0.51120101\n",
      "Iteration 57, loss = 0.50790159\n",
      "Iteration 58, loss = 0.50468942\n",
      "Iteration 59, loss = 0.50156103\n",
      "Iteration 60, loss = 0.49851309\n",
      "Iteration 61, loss = 0.49554235\n",
      "Iteration 62, loss = 0.49264567\n",
      "Iteration 63, loss = 0.48982003\n",
      "Iteration 64, loss = 0.48706246\n",
      "Iteration 65, loss = 0.48437012\n",
      "Iteration 66, loss = 0.48174026\n",
      "Iteration 67, loss = 0.47917022\n",
      "Iteration 68, loss = 0.47665747\n",
      "Iteration 69, loss = 0.47419956\n",
      "Iteration 70, loss = 0.47179415\n",
      "Iteration 71, loss = 0.46943902\n",
      "Iteration 72, loss = 0.46713206\n",
      "Iteration 73, loss = 0.46487124\n",
      "Iteration 74, loss = 0.46265468\n",
      "Iteration 75, loss = 0.46048055\n",
      "Iteration 76, loss = 0.45834715\n",
      "Iteration 77, loss = 0.45625288\n",
      "Iteration 78, loss = 0.45419621\n",
      "Iteration 79, loss = 0.45217570\n",
      "Iteration 80, loss = 0.45019001\n",
      "Iteration 81, loss = 0.44823785\n",
      "Iteration 82, loss = 0.44631802\n",
      "Iteration 83, loss = 0.44442938\n",
      "Iteration 84, loss = 0.44257085\n",
      "Iteration 85, loss = 0.44074142\n",
      "Iteration 86, loss = 0.43894011\n",
      "Iteration 87, loss = 0.43716601\n",
      "Iteration 88, loss = 0.43541824\n",
      "Iteration 89, loss = 0.43369598\n",
      "Iteration 90, loss = 0.43199843\n",
      "Iteration 91, loss = 0.43032484\n",
      "Iteration 92, loss = 0.42867448\n",
      "Iteration 93, loss = 0.42704667\n",
      "Iteration 94, loss = 0.42544074\n",
      "Iteration 95, loss = 0.42385606\n",
      "Iteration 96, loss = 0.42229202\n",
      "Iteration 97, loss = 0.42074803\n",
      "Iteration 98, loss = 0.41922353\n",
      "Iteration 99, loss = 0.41771799\n",
      "Iteration 100, loss = 0.41623088\n",
      "Iteration 101, loss = 0.41476170\n",
      "Iteration 102, loss = 0.41330997\n",
      "Iteration 103, loss = 0.41187523\n",
      "Iteration 104, loss = 0.41045703\n",
      "Iteration 105, loss = 0.40905494\n",
      "Iteration 106, loss = 0.40766855\n",
      "Iteration 107, loss = 0.40629746\n",
      "Iteration 108, loss = 0.40494128\n",
      "Iteration 109, loss = 0.40359964\n",
      "Iteration 110, loss = 0.40227218\n",
      "Iteration 111, loss = 0.40095856\n",
      "Iteration 112, loss = 0.39965844\n",
      "Iteration 113, loss = 0.39837151\n",
      "Iteration 114, loss = 0.39709745\n",
      "Iteration 115, loss = 0.39583596\n",
      "Iteration 116, loss = 0.39458675\n",
      "Iteration 117, loss = 0.39334954\n",
      "Iteration 118, loss = 0.39212407\n",
      "Iteration 119, loss = 0.39091006\n",
      "Iteration 120, loss = 0.38970726\n",
      "Iteration 121, loss = 0.38851543\n",
      "Iteration 122, loss = 0.38733434\n",
      "Iteration 123, loss = 0.38616374\n",
      "Iteration 124, loss = 0.38500341\n",
      "Iteration 125, loss = 0.38385315\n",
      "Iteration 126, loss = 0.38271273\n",
      "Iteration 127, loss = 0.38158196\n",
      "Iteration 128, loss = 0.38046063\n",
      "Iteration 129, loss = 0.37934855\n",
      "Iteration 130, loss = 0.37824554\n",
      "Iteration 131, loss = 0.37715142\n",
      "Iteration 132, loss = 0.37606600\n",
      "Iteration 133, loss = 0.37498911\n",
      "Iteration 134, loss = 0.37392059\n",
      "Iteration 135, loss = 0.37286028\n",
      "Iteration 136, loss = 0.37180802\n",
      "Iteration 137, loss = 0.37076365\n",
      "Iteration 138, loss = 0.36972702\n",
      "Iteration 139, loss = 0.36869799\n",
      "Iteration 140, loss = 0.36767641\n",
      "Iteration 141, loss = 0.36666216\n",
      "Iteration 142, loss = 0.36565508\n",
      "Iteration 143, loss = 0.36465506\n",
      "Iteration 144, loss = 0.36366197\n",
      "Iteration 145, loss = 0.36267567\n",
      "Iteration 146, loss = 0.36169605\n",
      "Iteration 147, loss = 0.36072299\n",
      "Iteration 148, loss = 0.35975638\n",
      "Iteration 149, loss = 0.35879609\n",
      "Iteration 150, loss = 0.35784203\n",
      "Iteration 151, loss = 0.35689409\n",
      "Iteration 152, loss = 0.35595216\n",
      "Iteration 153, loss = 0.35501613\n",
      "Iteration 154, loss = 0.35408592\n",
      "Iteration 155, loss = 0.35316141\n",
      "Iteration 156, loss = 0.35224253\n",
      "Iteration 157, loss = 0.35132917\n",
      "Iteration 158, loss = 0.35042125\n",
      "Iteration 159, loss = 0.34951867\n",
      "Iteration 160, loss = 0.34862136\n",
      "Iteration 161, loss = 0.34772922\n",
      "Iteration 162, loss = 0.34684218\n",
      "Iteration 163, loss = 0.34596016\n",
      "Iteration 164, loss = 0.34508307\n",
      "Iteration 165, loss = 0.34421084\n",
      "Iteration 166, loss = 0.34334339\n",
      "Iteration 167, loss = 0.34248066\n",
      "Iteration 168, loss = 0.34162256\n",
      "Iteration 169, loss = 0.34076904\n",
      "Iteration 170, loss = 0.33992002\n",
      "Iteration 171, loss = 0.33907543\n",
      "Iteration 172, loss = 0.33823521\n",
      "Iteration 173, loss = 0.33739930\n",
      "Iteration 174, loss = 0.33656763\n",
      "Iteration 175, loss = 0.33574015\n",
      "Iteration 176, loss = 0.33491678\n",
      "Iteration 177, loss = 0.33409748\n",
      "Iteration 178, loss = 0.33328219\n",
      "Iteration 179, loss = 0.33247084\n",
      "Iteration 180, loss = 0.33166339\n",
      "Iteration 181, loss = 0.33085978\n",
      "Iteration 182, loss = 0.33005996\n",
      "Iteration 183, loss = 0.32926388\n",
      "Iteration 184, loss = 0.32847149\n",
      "Iteration 185, loss = 0.32768273\n",
      "Iteration 186, loss = 0.32689756\n",
      "Iteration 187, loss = 0.32611594\n",
      "Iteration 188, loss = 0.32533781\n",
      "Iteration 189, loss = 0.32456313\n",
      "Iteration 190, loss = 0.32379186\n",
      "Iteration 191, loss = 0.32302395\n",
      "Iteration 192, loss = 0.32225937\n",
      "Iteration 193, loss = 0.32149806\n",
      "Iteration 194, loss = 0.32073999\n",
      "Iteration 195, loss = 0.31998512\n",
      "Iteration 196, loss = 0.31923340\n",
      "Iteration 197, loss = 0.31848481\n",
      "Iteration 198, loss = 0.31773930\n",
      "Iteration 199, loss = 0.31699684\n",
      "Iteration 200, loss = 0.31625739\n",
      "Iteration 201, loss = 0.31552091\n",
      "Iteration 202, loss = 0.31478737\n",
      "Iteration 203, loss = 0.31405674\n",
      "Iteration 204, loss = 0.31332898\n",
      "Iteration 205, loss = 0.31260407\n",
      "Iteration 206, loss = 0.31188196\n",
      "Iteration 207, loss = 0.31116263\n",
      "Iteration 208, loss = 0.31044605\n",
      "Iteration 209, loss = 0.30973219\n",
      "Iteration 210, loss = 0.30902101\n",
      "Iteration 211, loss = 0.30831249\n",
      "Iteration 212, loss = 0.30760660\n",
      "Iteration 213, loss = 0.30690332\n",
      "Iteration 214, loss = 0.30620261\n",
      "Iteration 215, loss = 0.30550445\n",
      "Iteration 216, loss = 0.30480881\n",
      "Iteration 217, loss = 0.30411567\n",
      "Iteration 218, loss = 0.30342501\n",
      "Iteration 219, loss = 0.30273679\n",
      "Iteration 220, loss = 0.30205100\n",
      "Iteration 221, loss = 0.30136761\n",
      "Iteration 222, loss = 0.30068659\n",
      "Iteration 223, loss = 0.30000793\n",
      "Iteration 224, loss = 0.29933160\n",
      "Iteration 225, loss = 0.29865759\n",
      "Iteration 226, loss = 0.29798586\n",
      "Iteration 227, loss = 0.29731640\n",
      "Iteration 228, loss = 0.29664919\n",
      "Iteration 229, loss = 0.29598421\n",
      "Iteration 230, loss = 0.29532144\n",
      "Iteration 231, loss = 0.29466086\n",
      "Iteration 232, loss = 0.29400245\n",
      "Iteration 233, loss = 0.29334619\n",
      "Iteration 234, loss = 0.29269206\n",
      "Iteration 235, loss = 0.29204005\n",
      "Iteration 236, loss = 0.29139014\n",
      "Iteration 237, loss = 0.29074231\n",
      "Iteration 238, loss = 0.29009655\n",
      "Iteration 239, loss = 0.28945283\n",
      "Iteration 240, loss = 0.28881115\n",
      "Iteration 241, loss = 0.28817148\n",
      "Iteration 242, loss = 0.28753382\n",
      "Iteration 243, loss = 0.28689814\n",
      "Iteration 244, loss = 0.28626443\n",
      "Iteration 245, loss = 0.28563269\n",
      "Iteration 246, loss = 0.28500288\n",
      "Iteration 247, loss = 0.28437500\n",
      "Iteration 248, loss = 0.28374904\n",
      "Iteration 249, loss = 0.28312498\n",
      "Iteration 250, loss = 0.28250281\n",
      "Iteration 251, loss = 0.28188251\n",
      "Iteration 252, loss = 0.28126408\n",
      "Iteration 253, loss = 0.28064750\n",
      "Iteration 254, loss = 0.28003276\n",
      "Iteration 255, loss = 0.27941985\n",
      "Iteration 256, loss = 0.27880875\n",
      "Iteration 257, loss = 0.27819946\n",
      "Iteration 258, loss = 0.27759196\n",
      "Iteration 259, loss = 0.27698625\n",
      "Iteration 260, loss = 0.27638231\n",
      "Iteration 261, loss = 0.27578013\n",
      "Iteration 262, loss = 0.27517970\n",
      "Iteration 263, loss = 0.27458101\n",
      "Iteration 264, loss = 0.27398405\n",
      "Iteration 265, loss = 0.27338882\n",
      "Iteration 266, loss = 0.27279530\n",
      "Iteration 267, loss = 0.27220349\n",
      "Iteration 268, loss = 0.27161337\n",
      "Iteration 269, loss = 0.27102493\n",
      "Iteration 270, loss = 0.27043818\n",
      "Iteration 271, loss = 0.26985309\n",
      "Iteration 272, loss = 0.26926967\n",
      "Iteration 273, loss = 0.26868790\n",
      "Iteration 274, loss = 0.26810777\n",
      "Iteration 275, loss = 0.26752929\n",
      "Iteration 276, loss = 0.26695243\n",
      "Iteration 277, loss = 0.26637720\n",
      "Iteration 278, loss = 0.26580358\n",
      "Iteration 279, loss = 0.26523157\n",
      "Iteration 280, loss = 0.26466117\n",
      "Iteration 281, loss = 0.26409235\n",
      "Iteration 282, loss = 0.26352513\n",
      "Iteration 283, loss = 0.26295949\n",
      "Iteration 284, loss = 0.26239543\n",
      "Iteration 285, loss = 0.26183294\n",
      "Iteration 286, loss = 0.26127201\n",
      "Iteration 287, loss = 0.26071264\n",
      "Iteration 288, loss = 0.26015482\n",
      "Iteration 289, loss = 0.25959855\n",
      "Iteration 290, loss = 0.25904383\n",
      "Iteration 291, loss = 0.25849063\n",
      "Iteration 292, loss = 0.25793897\n",
      "Iteration 293, loss = 0.25738884\n",
      "Iteration 294, loss = 0.25684023\n",
      "Iteration 295, loss = 0.25629313\n",
      "Iteration 296, loss = 0.25574755\n",
      "Iteration 297, loss = 0.25520347\n",
      "Iteration 298, loss = 0.25466089\n",
      "Iteration 299, loss = 0.25411982\n",
      "Iteration 300, loss = 0.25358023\n",
      "Iteration 301, loss = 0.25304214\n",
      "Iteration 302, loss = 0.25250553\n",
      "Iteration 303, loss = 0.25197040\n",
      "Iteration 304, loss = 0.25143676\n",
      "Iteration 305, loss = 0.25090458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 306, loss = 0.25037388\n",
      "Iteration 307, loss = 0.24984464\n",
      "Iteration 308, loss = 0.24931686\n",
      "Iteration 309, loss = 0.24879055\n",
      "Iteration 310, loss = 0.24826569\n",
      "Iteration 311, loss = 0.24774228\n",
      "Iteration 312, loss = 0.24722033\n",
      "Iteration 313, loss = 0.24669982\n",
      "Iteration 314, loss = 0.24618075\n",
      "Iteration 315, loss = 0.24566312\n",
      "Iteration 316, loss = 0.24514694\n",
      "Iteration 317, loss = 0.24463218\n",
      "Iteration 318, loss = 0.24411886\n",
      "Iteration 319, loss = 0.24360697\n",
      "Iteration 320, loss = 0.24309650\n",
      "Iteration 321, loss = 0.24258746\n",
      "Iteration 322, loss = 0.24207984\n",
      "Iteration 323, loss = 0.24157364\n",
      "Iteration 324, loss = 0.24106886\n",
      "Iteration 325, loss = 0.24056549\n",
      "Iteration 326, loss = 0.24006354\n",
      "Iteration 327, loss = 0.23956299\n",
      "Iteration 328, loss = 0.23906385\n",
      "Iteration 329, loss = 0.23856612\n",
      "Iteration 330, loss = 0.23806979\n",
      "Iteration 331, loss = 0.23757486\n",
      "Iteration 332, loss = 0.23708133\n",
      "Iteration 333, loss = 0.23658920\n",
      "Iteration 334, loss = 0.23609846\n",
      "Iteration 335, loss = 0.23560912\n",
      "Iteration 336, loss = 0.23512117\n",
      "Iteration 337, loss = 0.23463461\n",
      "Iteration 338, loss = 0.23414944\n",
      "Iteration 339, loss = 0.23366566\n",
      "Iteration 340, loss = 0.23318326\n",
      "Iteration 341, loss = 0.23270225\n",
      "Iteration 342, loss = 0.23222261\n",
      "Iteration 343, loss = 0.23174436\n",
      "Iteration 344, loss = 0.23126749\n",
      "Iteration 345, loss = 0.23079199\n",
      "Iteration 346, loss = 0.23031787\n",
      "Iteration 347, loss = 0.22984513\n",
      "Iteration 348, loss = 0.22937375\n",
      "Iteration 349, loss = 0.22890375\n",
      "Iteration 350, loss = 0.22843512\n",
      "Iteration 351, loss = 0.22796786\n",
      "Iteration 352, loss = 0.22750196\n",
      "Iteration 353, loss = 0.22703743\n",
      "Iteration 354, loss = 0.22657427\n",
      "Iteration 355, loss = 0.22611247\n",
      "Iteration 356, loss = 0.22565203\n",
      "Iteration 357, loss = 0.22519295\n",
      "Iteration 358, loss = 0.22473524\n",
      "Iteration 359, loss = 0.22427888\n",
      "Iteration 360, loss = 0.22382388\n",
      "Iteration 361, loss = 0.22337023\n",
      "Iteration 362, loss = 0.22291794\n",
      "Iteration 363, loss = 0.22246700\n",
      "Iteration 364, loss = 0.22201742\n",
      "Iteration 365, loss = 0.22156919\n",
      "Iteration 366, loss = 0.22112230\n",
      "Iteration 367, loss = 0.22067677\n",
      "Iteration 368, loss = 0.22023259\n",
      "Iteration 369, loss = 0.21978975\n",
      "Iteration 370, loss = 0.21934826\n",
      "Iteration 371, loss = 0.21890811\n",
      "Iteration 372, loss = 0.21846930\n",
      "Iteration 373, loss = 0.21803184\n",
      "Iteration 374, loss = 0.21759572\n",
      "Iteration 375, loss = 0.21716094\n",
      "Iteration 376, loss = 0.21672749\n",
      "Iteration 377, loss = 0.21629539\n",
      "Iteration 378, loss = 0.21586462\n",
      "Iteration 379, loss = 0.21543519\n",
      "Iteration 380, loss = 0.21500709\n",
      "Iteration 381, loss = 0.21458032\n",
      "Iteration 382, loss = 0.21415489\n",
      "Iteration 383, loss = 0.21373079\n",
      "Iteration 384, loss = 0.21330801\n",
      "Iteration 385, loss = 0.21288657\n",
      "Iteration 386, loss = 0.21246645\n",
      "Iteration 387, loss = 0.21204766\n",
      "Iteration 388, loss = 0.21163019\n",
      "Iteration 389, loss = 0.21121405\n",
      "Iteration 390, loss = 0.21079923\n",
      "Iteration 391, loss = 0.21038573\n",
      "Iteration 392, loss = 0.20997355\n",
      "Iteration 393, loss = 0.20956269\n",
      "Iteration 394, loss = 0.20915314\n",
      "Iteration 395, loss = 0.20874491\n",
      "Iteration 396, loss = 0.20833800\n",
      "Iteration 397, loss = 0.20793240\n",
      "Iteration 398, loss = 0.20752811\n",
      "Iteration 399, loss = 0.20712514\n",
      "Iteration 400, loss = 0.20672347\n",
      "Iteration 401, loss = 0.20632311\n",
      "Iteration 402, loss = 0.20592406\n",
      "Iteration 403, loss = 0.20552631\n",
      "Iteration 404, loss = 0.20512987\n",
      "Iteration 405, loss = 0.20473473\n",
      "Iteration 406, loss = 0.20434089\n",
      "Iteration 407, loss = 0.20394835\n",
      "Iteration 408, loss = 0.20355710\n",
      "Iteration 409, loss = 0.20316716\n",
      "Iteration 410, loss = 0.20277851\n",
      "Iteration 411, loss = 0.20239115\n",
      "Iteration 412, loss = 0.20200508\n",
      "Iteration 413, loss = 0.20162031\n",
      "Iteration 414, loss = 0.20123682\n",
      "Iteration 415, loss = 0.20085462\n",
      "Iteration 416, loss = 0.20047371\n",
      "Iteration 417, loss = 0.20009408\n",
      "Iteration 418, loss = 0.19971573\n",
      "Iteration 419, loss = 0.19933867\n",
      "Iteration 420, loss = 0.19896288\n",
      "Iteration 421, loss = 0.19858837\n",
      "Iteration 422, loss = 0.19821513\n",
      "Iteration 423, loss = 0.19784317\n",
      "Iteration 424, loss = 0.19747248\n",
      "Iteration 425, loss = 0.19710306\n",
      "Iteration 426, loss = 0.19673491\n",
      "Iteration 427, loss = 0.19636802\n",
      "Iteration 428, loss = 0.19600240\n",
      "Iteration 429, loss = 0.19563804\n",
      "Iteration 430, loss = 0.19527494\n",
      "Iteration 431, loss = 0.19491310\n",
      "Iteration 432, loss = 0.19455252\n",
      "Iteration 433, loss = 0.19419319\n",
      "Iteration 434, loss = 0.19383512\n",
      "Iteration 435, loss = 0.19347829\n",
      "Iteration 436, loss = 0.19312272\n",
      "Iteration 437, loss = 0.19276839\n",
      "Iteration 438, loss = 0.19241530\n",
      "Iteration 439, loss = 0.19206346\n",
      "Iteration 440, loss = 0.19171286\n",
      "Iteration 441, loss = 0.19136350\n",
      "Iteration 442, loss = 0.19101537\n",
      "Iteration 443, loss = 0.19066847\n",
      "Iteration 444, loss = 0.19032281\n",
      "Iteration 445, loss = 0.18997838\n",
      "Iteration 446, loss = 0.18963517\n",
      "Iteration 447, loss = 0.18929319\n",
      "Iteration 448, loss = 0.18895243\n",
      "Iteration 449, loss = 0.18861289\n",
      "Iteration 450, loss = 0.18827457\n",
      "Iteration 451, loss = 0.18793746\n",
      "Iteration 452, loss = 0.18760157\n",
      "Iteration 453, loss = 0.18726689\n",
      "Iteration 454, loss = 0.18693342\n",
      "Iteration 455, loss = 0.18660115\n",
      "Iteration 456, loss = 0.18627009\n",
      "Iteration 457, loss = 0.18594022\n",
      "Iteration 458, loss = 0.18561156\n",
      "Iteration 459, loss = 0.18528409\n",
      "Iteration 460, loss = 0.18495781\n",
      "Iteration 461, loss = 0.18463273\n",
      "Iteration 462, loss = 0.18430883\n",
      "Iteration 463, loss = 0.18398612\n",
      "Iteration 464, loss = 0.18366459\n",
      "Iteration 465, loss = 0.18334424\n",
      "Iteration 466, loss = 0.18302507\n",
      "Iteration 467, loss = 0.18270708\n",
      "Iteration 468, loss = 0.18239026\n",
      "Iteration 469, loss = 0.18207460\n",
      "Iteration 470, loss = 0.18176012\n",
      "Iteration 471, loss = 0.18144679\n",
      "Iteration 472, loss = 0.18113463\n",
      "Iteration 473, loss = 0.18082363\n",
      "Iteration 474, loss = 0.18051379\n",
      "Iteration 475, loss = 0.18020509\n",
      "Iteration 476, loss = 0.17989755\n",
      "Iteration 477, loss = 0.17959115\n",
      "Iteration 478, loss = 0.17928590\n",
      "Iteration 479, loss = 0.17898179\n",
      "Iteration 480, loss = 0.17867882\n",
      "Iteration 481, loss = 0.17837699\n",
      "Iteration 482, loss = 0.17807629\n",
      "Iteration 483, loss = 0.17777671\n",
      "Iteration 484, loss = 0.17747827\n",
      "Iteration 485, loss = 0.17718095\n",
      "Iteration 486, loss = 0.17688475\n",
      "Iteration 487, loss = 0.17658967\n",
      "Iteration 488, loss = 0.17629570\n",
      "Iteration 489, loss = 0.17600285\n",
      "Iteration 490, loss = 0.17571110\n",
      "Iteration 491, loss = 0.17542046\n",
      "Iteration 492, loss = 0.17513093\n",
      "Iteration 493, loss = 0.17484249\n",
      "Iteration 494, loss = 0.17455515\n",
      "Iteration 495, loss = 0.17426890\n",
      "Iteration 496, loss = 0.17398375\n",
      "Iteration 497, loss = 0.17369968\n",
      "Iteration 498, loss = 0.17341670\n",
      "Iteration 499, loss = 0.17313479\n",
      "Iteration 500, loss = 0.17285397\n",
      "Iteration 1, loss = 1.43092250\n",
      "Iteration 2, loss = 1.41492063\n",
      "Iteration 3, loss = 1.39253901\n",
      "Iteration 4, loss = 1.36485459\n",
      "Iteration 5, loss = 1.33290580\n",
      "Iteration 6, loss = 1.29767257\n",
      "Iteration 7, loss = 1.26006215\n",
      "Iteration 8, loss = 1.22089965\n",
      "Iteration 9, loss = 1.18092224\n",
      "Iteration 10, loss = 1.14077647\n",
      "Iteration 11, loss = 1.10101795\n",
      "Iteration 12, loss = 1.06211294\n",
      "Iteration 13, loss = 1.02444153\n",
      "Iteration 14, loss = 0.98830220\n",
      "Iteration 15, loss = 0.95391742\n",
      "Iteration 16, loss = 0.92144030\n",
      "Iteration 17, loss = 0.89096196\n",
      "Iteration 18, loss = 0.86251948\n",
      "Iteration 19, loss = 0.83610413\n",
      "Iteration 20, loss = 0.81166961\n",
      "Iteration 21, loss = 0.78914010\n",
      "Iteration 22, loss = 0.76841770\n",
      "Iteration 23, loss = 0.74938920\n",
      "Iteration 24, loss = 0.73193207\n",
      "Iteration 25, loss = 0.71591944\n",
      "Iteration 26, loss = 0.70122420\n",
      "Iteration 27, loss = 0.68772219\n",
      "Iteration 28, loss = 0.67529461\n",
      "Iteration 29, loss = 0.66382972\n",
      "Iteration 30, loss = 0.65322387\n",
      "Iteration 31, loss = 0.64338211\n",
      "Iteration 32, loss = 0.63421843\n",
      "Iteration 33, loss = 0.62565557\n",
      "Iteration 34, loss = 0.61762478\n",
      "Iteration 35, loss = 0.61006530\n",
      "Iteration 36, loss = 0.60292379\n",
      "Iteration 37, loss = 0.59615372\n",
      "Iteration 38, loss = 0.58971473\n",
      "Iteration 39, loss = 0.58357194\n",
      "Iteration 40, loss = 0.57769537\n",
      "Iteration 41, loss = 0.57205931\n",
      "Iteration 42, loss = 0.56664177\n",
      "Iteration 43, loss = 0.56142399\n",
      "Iteration 44, loss = 0.55638995\n",
      "Iteration 45, loss = 0.55152599\n",
      "Iteration 46, loss = 0.54682040\n",
      "Iteration 47, loss = 0.54226312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.53784545\n",
      "Iteration 49, loss = 0.53355983\n",
      "Iteration 50, loss = 0.52939959\n",
      "Iteration 51, loss = 0.52535881\n",
      "Iteration 52, loss = 0.52143216\n",
      "Iteration 53, loss = 0.51761478\n",
      "Iteration 54, loss = 0.51390219\n",
      "Iteration 55, loss = 0.51029021\n",
      "Iteration 56, loss = 0.50677489\n",
      "Iteration 57, loss = 0.50335247\n",
      "Iteration 58, loss = 0.50001934\n",
      "Iteration 59, loss = 0.49677203\n",
      "Iteration 60, loss = 0.49360717\n",
      "Iteration 61, loss = 0.49052151\n",
      "Iteration 62, loss = 0.48751188\n",
      "Iteration 63, loss = 0.48457520\n",
      "Iteration 64, loss = 0.48170850\n",
      "Iteration 65, loss = 0.47890888\n",
      "Iteration 66, loss = 0.47617357\n",
      "Iteration 67, loss = 0.47349986\n",
      "Iteration 68, loss = 0.47088518\n",
      "Iteration 69, loss = 0.46832704\n",
      "Iteration 70, loss = 0.46582307\n",
      "Iteration 71, loss = 0.46337100\n",
      "Iteration 72, loss = 0.46096867\n",
      "Iteration 73, loss = 0.45861403\n",
      "Iteration 74, loss = 0.45630512\n",
      "Iteration 75, loss = 0.45404011\n",
      "Iteration 76, loss = 0.45181725\n",
      "Iteration 77, loss = 0.44963488\n",
      "Iteration 78, loss = 0.44749145\n",
      "Iteration 79, loss = 0.44538548\n",
      "Iteration 80, loss = 0.44331560\n",
      "Iteration 81, loss = 0.44128048\n",
      "Iteration 82, loss = 0.43927888\n",
      "Iteration 83, loss = 0.43730965\n",
      "Iteration 84, loss = 0.43537166\n",
      "Iteration 85, loss = 0.43346387\n",
      "Iteration 86, loss = 0.43158530\n",
      "Iteration 87, loss = 0.42973498\n",
      "Iteration 88, loss = 0.42791203\n",
      "Iteration 89, loss = 0.42611560\n",
      "Iteration 90, loss = 0.42434487\n",
      "Iteration 91, loss = 0.42259906\n",
      "Iteration 92, loss = 0.42087743\n",
      "Iteration 93, loss = 0.41917927\n",
      "Iteration 94, loss = 0.41750390\n",
      "Iteration 95, loss = 0.41585067\n",
      "Iteration 96, loss = 0.41421896\n",
      "Iteration 97, loss = 0.41260815\n",
      "Iteration 98, loss = 0.41101768\n",
      "Iteration 99, loss = 0.40944698\n",
      "Iteration 100, loss = 0.40789552\n",
      "Iteration 101, loss = 0.40636279\n",
      "Iteration 102, loss = 0.40484828\n",
      "Iteration 103, loss = 0.40335153\n",
      "Iteration 104, loss = 0.40187205\n",
      "Iteration 105, loss = 0.40040942\n",
      "Iteration 106, loss = 0.39896321\n",
      "Iteration 107, loss = 0.39753299\n",
      "Iteration 108, loss = 0.39611837\n",
      "Iteration 109, loss = 0.39471896\n",
      "Iteration 110, loss = 0.39333439\n",
      "Iteration 111, loss = 0.39196431\n",
      "Iteration 112, loss = 0.39060836\n",
      "Iteration 113, loss = 0.38926621\n",
      "Iteration 114, loss = 0.38793753\n",
      "Iteration 115, loss = 0.38662202\n",
      "Iteration 116, loss = 0.38531936\n",
      "Iteration 117, loss = 0.38402927\n",
      "Iteration 118, loss = 0.38275146\n",
      "Iteration 119, loss = 0.38148566\n",
      "Iteration 120, loss = 0.38023160\n",
      "Iteration 121, loss = 0.37898903\n",
      "Iteration 122, loss = 0.37775769\n",
      "Iteration 123, loss = 0.37653735\n",
      "Iteration 124, loss = 0.37532776\n",
      "Iteration 125, loss = 0.37412871\n",
      "Iteration 126, loss = 0.37293996\n",
      "Iteration 127, loss = 0.37176132\n",
      "Iteration 128, loss = 0.37059256\n",
      "Iteration 129, loss = 0.36943349\n",
      "Iteration 130, loss = 0.36828390\n",
      "Iteration 131, loss = 0.36714362\n",
      "Iteration 132, loss = 0.36601245\n",
      "Iteration 133, loss = 0.36489022\n",
      "Iteration 134, loss = 0.36377674\n",
      "Iteration 135, loss = 0.36267185\n",
      "Iteration 136, loss = 0.36157538\n",
      "Iteration 137, loss = 0.36048717\n",
      "Iteration 138, loss = 0.35940707\n",
      "Iteration 139, loss = 0.35833491\n",
      "Iteration 140, loss = 0.35727055\n",
      "Iteration 141, loss = 0.35621385\n",
      "Iteration 142, loss = 0.35516466\n",
      "Iteration 143, loss = 0.35412285\n",
      "Iteration 144, loss = 0.35308828\n",
      "Iteration 145, loss = 0.35206082\n",
      "Iteration 146, loss = 0.35104035\n",
      "Iteration 147, loss = 0.35002673\n",
      "Iteration 148, loss = 0.34901985\n",
      "Iteration 149, loss = 0.34801959\n",
      "Iteration 150, loss = 0.34702584\n",
      "Iteration 151, loss = 0.34603847\n",
      "Iteration 152, loss = 0.34505739\n",
      "Iteration 153, loss = 0.34408248\n",
      "Iteration 154, loss = 0.34311365\n",
      "Iteration 155, loss = 0.34215078\n",
      "Iteration 156, loss = 0.34119378\n",
      "Iteration 157, loss = 0.34024255\n",
      "Iteration 158, loss = 0.33929700\n",
      "Iteration 159, loss = 0.33835704\n",
      "Iteration 160, loss = 0.33742257\n",
      "Iteration 161, loss = 0.33649350\n",
      "Iteration 162, loss = 0.33556975\n",
      "Iteration 163, loss = 0.33465124\n",
      "Iteration 164, loss = 0.33373788\n",
      "Iteration 165, loss = 0.33282959\n",
      "Iteration 166, loss = 0.33192630\n",
      "Iteration 167, loss = 0.33102792\n",
      "Iteration 168, loss = 0.33013438\n",
      "Iteration 169, loss = 0.32924561\n",
      "Iteration 170, loss = 0.32836154\n",
      "Iteration 171, loss = 0.32748209\n",
      "Iteration 172, loss = 0.32660720\n",
      "Iteration 173, loss = 0.32573679\n",
      "Iteration 174, loss = 0.32487082\n",
      "Iteration 175, loss = 0.32400920\n",
      "Iteration 176, loss = 0.32315187\n",
      "Iteration 177, loss = 0.32229879\n",
      "Iteration 178, loss = 0.32144988\n",
      "Iteration 179, loss = 0.32060508\n",
      "Iteration 180, loss = 0.31976435\n",
      "Iteration 181, loss = 0.31892761\n",
      "Iteration 182, loss = 0.31809483\n",
      "Iteration 183, loss = 0.31726594\n",
      "Iteration 184, loss = 0.31644088\n",
      "Iteration 185, loss = 0.31561962\n",
      "Iteration 186, loss = 0.31480210\n",
      "Iteration 187, loss = 0.31398826\n",
      "Iteration 188, loss = 0.31317807\n",
      "Iteration 189, loss = 0.31237147\n",
      "Iteration 190, loss = 0.31156841\n",
      "Iteration 191, loss = 0.31076886\n",
      "Iteration 192, loss = 0.30997276\n",
      "Iteration 193, loss = 0.30918008\n",
      "Iteration 194, loss = 0.30839076\n",
      "Iteration 195, loss = 0.30760478\n",
      "Iteration 196, loss = 0.30682208\n",
      "Iteration 197, loss = 0.30604263\n",
      "Iteration 198, loss = 0.30526639\n",
      "Iteration 199, loss = 0.30449332\n",
      "Iteration 200, loss = 0.30372338\n",
      "Iteration 201, loss = 0.30295654\n",
      "Iteration 202, loss = 0.30219276\n",
      "Iteration 203, loss = 0.30143200\n",
      "Iteration 204, loss = 0.30067423\n",
      "Iteration 205, loss = 0.29991941\n",
      "Iteration 206, loss = 0.29916752\n",
      "Iteration 207, loss = 0.29841852\n",
      "Iteration 208, loss = 0.29767237\n",
      "Iteration 209, loss = 0.29692905\n",
      "Iteration 210, loss = 0.29618853\n",
      "Iteration 211, loss = 0.29545078\n",
      "Iteration 212, loss = 0.29471576\n",
      "Iteration 213, loss = 0.29398344\n",
      "Iteration 214, loss = 0.29325381\n",
      "Iteration 215, loss = 0.29252683\n",
      "Iteration 216, loss = 0.29180248\n",
      "Iteration 217, loss = 0.29108072\n",
      "Iteration 218, loss = 0.29036154\n",
      "Iteration 219, loss = 0.28964490\n",
      "Iteration 220, loss = 0.28893079\n",
      "Iteration 221, loss = 0.28821918\n",
      "Iteration 222, loss = 0.28751003\n",
      "Iteration 223, loss = 0.28680334\n",
      "Iteration 224, loss = 0.28609908\n",
      "Iteration 225, loss = 0.28539722\n",
      "Iteration 226, loss = 0.28469775\n",
      "Iteration 227, loss = 0.28400064\n",
      "Iteration 228, loss = 0.28330586\n",
      "Iteration 229, loss = 0.28261341\n",
      "Iteration 230, loss = 0.28192326\n",
      "Iteration 231, loss = 0.28123538\n",
      "Iteration 232, loss = 0.28054977\n",
      "Iteration 233, loss = 0.27986640\n",
      "Iteration 234, loss = 0.27918525\n",
      "Iteration 235, loss = 0.27850631\n",
      "Iteration 236, loss = 0.27782955\n",
      "Iteration 237, loss = 0.27715496\n",
      "Iteration 238, loss = 0.27648252\n",
      "Iteration 239, loss = 0.27581222\n",
      "Iteration 240, loss = 0.27514404\n",
      "Iteration 241, loss = 0.27447796\n",
      "Iteration 242, loss = 0.27381396\n",
      "Iteration 243, loss = 0.27315204\n",
      "Iteration 244, loss = 0.27249217\n",
      "Iteration 245, loss = 0.27183435\n",
      "Iteration 246, loss = 0.27117855\n",
      "Iteration 247, loss = 0.27052476\n",
      "Iteration 248, loss = 0.26987298\n",
      "Iteration 249, loss = 0.26922318\n",
      "Iteration 250, loss = 0.26857535\n",
      "Iteration 251, loss = 0.26792947\n",
      "Iteration 252, loss = 0.26728555\n",
      "Iteration 253, loss = 0.26664356\n",
      "Iteration 254, loss = 0.26600349\n",
      "Iteration 255, loss = 0.26536533\n",
      "Iteration 256, loss = 0.26472907\n",
      "Iteration 257, loss = 0.26409469\n",
      "Iteration 258, loss = 0.26346219\n",
      "Iteration 259, loss = 0.26283156\n",
      "Iteration 260, loss = 0.26220277\n",
      "Iteration 261, loss = 0.26157584\n",
      "Iteration 262, loss = 0.26095073\n",
      "Iteration 263, loss = 0.26032745\n",
      "Iteration 264, loss = 0.25970598\n",
      "Iteration 265, loss = 0.25908631\n",
      "Iteration 266, loss = 0.25846844\n",
      "Iteration 267, loss = 0.25785235\n",
      "Iteration 268, loss = 0.25723804\n",
      "Iteration 269, loss = 0.25662550\n",
      "Iteration 270, loss = 0.25601472\n",
      "Iteration 271, loss = 0.25540568\n",
      "Iteration 272, loss = 0.25479839\n",
      "Iteration 273, loss = 0.25419284\n",
      "Iteration 274, loss = 0.25358901\n",
      "Iteration 275, loss = 0.25298690\n",
      "Iteration 276, loss = 0.25238650\n",
      "Iteration 277, loss = 0.25178780\n",
      "Iteration 278, loss = 0.25119080\n",
      "Iteration 279, loss = 0.25059550\n",
      "Iteration 280, loss = 0.25000187\n",
      "Iteration 281, loss = 0.24940993\n",
      "Iteration 282, loss = 0.24881965\n",
      "Iteration 283, loss = 0.24823104\n",
      "Iteration 284, loss = 0.24764409\n",
      "Iteration 285, loss = 0.24705878\n",
      "Iteration 286, loss = 0.24647513\n",
      "Iteration 287, loss = 0.24589311\n",
      "Iteration 288, loss = 0.24531274\n",
      "Iteration 289, loss = 0.24473398\n",
      "Iteration 290, loss = 0.24415686\n",
      "Iteration 291, loss = 0.24358135\n",
      "Iteration 292, loss = 0.24300746\n",
      "Iteration 293, loss = 0.24243517\n",
      "Iteration 294, loss = 0.24186449\n",
      "Iteration 295, loss = 0.24129541\n",
      "Iteration 296, loss = 0.24072793\n",
      "Iteration 297, loss = 0.24016203\n",
      "Iteration 298, loss = 0.23959772\n",
      "Iteration 299, loss = 0.23903500\n",
      "Iteration 300, loss = 0.23847385\n",
      "Iteration 301, loss = 0.23791427\n",
      "Iteration 302, loss = 0.23735627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 303, loss = 0.23679983\n",
      "Iteration 304, loss = 0.23624495\n",
      "Iteration 305, loss = 0.23569163\n",
      "Iteration 306, loss = 0.23513986\n",
      "Iteration 307, loss = 0.23458965\n",
      "Iteration 308, loss = 0.23404099\n",
      "Iteration 309, loss = 0.23349387\n",
      "Iteration 310, loss = 0.23294829\n",
      "Iteration 311, loss = 0.23240425\n",
      "Iteration 312, loss = 0.23186175\n",
      "Iteration 313, loss = 0.23132077\n",
      "Iteration 314, loss = 0.23078133\n",
      "Iteration 315, loss = 0.23024342\n",
      "Iteration 316, loss = 0.22970703\n",
      "Iteration 317, loss = 0.22917216\n",
      "Iteration 318, loss = 0.22863881\n",
      "Iteration 319, loss = 0.22810697\n",
      "Iteration 320, loss = 0.22757665\n",
      "Iteration 321, loss = 0.22704784\n",
      "Iteration 322, loss = 0.22652054\n",
      "Iteration 323, loss = 0.22599475\n",
      "Iteration 324, loss = 0.22547046\n",
      "Iteration 325, loss = 0.22494768\n",
      "Iteration 326, loss = 0.22442639\n",
      "Iteration 327, loss = 0.22390660\n",
      "Iteration 328, loss = 0.22338831\n",
      "Iteration 329, loss = 0.22287151\n",
      "Iteration 330, loss = 0.22235621\n",
      "Iteration 331, loss = 0.22184239\n",
      "Iteration 332, loss = 0.22133006\n",
      "Iteration 333, loss = 0.22081922\n",
      "Iteration 334, loss = 0.22030987\n",
      "Iteration 335, loss = 0.21980199\n",
      "Iteration 336, loss = 0.21929560\n",
      "Iteration 337, loss = 0.21879069\n",
      "Iteration 338, loss = 0.21828726\n",
      "Iteration 339, loss = 0.21778530\n",
      "Iteration 340, loss = 0.21728482\n",
      "Iteration 341, loss = 0.21678581\n",
      "Iteration 342, loss = 0.21628827\n",
      "Iteration 343, loss = 0.21579220\n",
      "Iteration 344, loss = 0.21529761\n",
      "Iteration 345, loss = 0.21480448\n",
      "Iteration 346, loss = 0.21431281\n",
      "Iteration 347, loss = 0.21382261\n",
      "Iteration 348, loss = 0.21333388\n",
      "Iteration 349, loss = 0.21284660\n",
      "Iteration 350, loss = 0.21236079\n",
      "Iteration 351, loss = 0.21187644\n",
      "Iteration 352, loss = 0.21139354\n",
      "Iteration 353, loss = 0.21091211\n",
      "Iteration 354, loss = 0.21043212\n",
      "Iteration 355, loss = 0.20995360\n",
      "Iteration 356, loss = 0.20947652\n",
      "Iteration 357, loss = 0.20900090\n",
      "Iteration 358, loss = 0.20852673\n",
      "Iteration 359, loss = 0.20805400\n",
      "Iteration 360, loss = 0.20758273\n",
      "Iteration 361, loss = 0.20711290\n",
      "Iteration 362, loss = 0.20664452\n",
      "Iteration 363, loss = 0.20617759\n",
      "Iteration 364, loss = 0.20571210\n",
      "Iteration 365, loss = 0.20524805\n",
      "Iteration 366, loss = 0.20478544\n",
      "Iteration 367, loss = 0.20432427\n",
      "Iteration 368, loss = 0.20386454\n",
      "Iteration 369, loss = 0.20340625\n",
      "Iteration 370, loss = 0.20294939\n",
      "Iteration 371, loss = 0.20249397\n",
      "Iteration 372, loss = 0.20203999\n",
      "Iteration 373, loss = 0.20158744\n",
      "Iteration 374, loss = 0.20113632\n",
      "Iteration 375, loss = 0.20068663\n",
      "Iteration 376, loss = 0.20023837\n",
      "Iteration 377, loss = 0.19979154\n",
      "Iteration 378, loss = 0.19934613\n",
      "Iteration 379, loss = 0.19890215\n",
      "Iteration 380, loss = 0.19845960\n",
      "Iteration 381, loss = 0.19801847\n",
      "Iteration 382, loss = 0.19757876\n",
      "Iteration 383, loss = 0.19714047\n",
      "Iteration 384, loss = 0.19670360\n",
      "Iteration 385, loss = 0.19626815\n",
      "Iteration 386, loss = 0.19583412\n",
      "Iteration 387, loss = 0.19540150\n",
      "Iteration 388, loss = 0.19497030\n",
      "Iteration 389, loss = 0.19454051\n",
      "Iteration 390, loss = 0.19411213\n",
      "Iteration 391, loss = 0.19368516\n",
      "Iteration 392, loss = 0.19325960\n",
      "Iteration 393, loss = 0.19283545\n",
      "Iteration 394, loss = 0.19241270\n",
      "Iteration 395, loss = 0.19199136\n",
      "Iteration 396, loss = 0.19157142\n",
      "Iteration 397, loss = 0.19115288\n",
      "Iteration 398, loss = 0.19073575\n",
      "Iteration 399, loss = 0.19032001\n",
      "Iteration 400, loss = 0.18990567\n",
      "Iteration 401, loss = 0.18949272\n",
      "Iteration 402, loss = 0.18908117\n",
      "Iteration 403, loss = 0.18867101\n",
      "Iteration 404, loss = 0.18826224\n",
      "Iteration 405, loss = 0.18785486\n",
      "Iteration 406, loss = 0.18744886\n",
      "Iteration 407, loss = 0.18704426\n",
      "Iteration 408, loss = 0.18664103\n",
      "Iteration 409, loss = 0.18623919\n",
      "Iteration 410, loss = 0.18583873\n",
      "Iteration 411, loss = 0.18543965\n",
      "Iteration 412, loss = 0.18504195\n",
      "Iteration 413, loss = 0.18464562\n",
      "Iteration 414, loss = 0.18425066\n",
      "Iteration 415, loss = 0.18385707\n",
      "Iteration 416, loss = 0.18346486\n",
      "Iteration 417, loss = 0.18307401\n",
      "Iteration 418, loss = 0.18268453\n",
      "Iteration 419, loss = 0.18229641\n",
      "Iteration 420, loss = 0.18190965\n",
      "Iteration 421, loss = 0.18152425\n",
      "Iteration 422, loss = 0.18114021\n",
      "Iteration 423, loss = 0.18075753\n",
      "Iteration 424, loss = 0.18037620\n",
      "Iteration 425, loss = 0.17999622\n",
      "Iteration 426, loss = 0.17961759\n",
      "Iteration 427, loss = 0.17924030\n",
      "Iteration 428, loss = 0.17886436\n",
      "Iteration 429, loss = 0.17848977\n",
      "Iteration 430, loss = 0.17811651\n",
      "Iteration 431, loss = 0.17774460\n",
      "Iteration 432, loss = 0.17737401\n",
      "Iteration 433, loss = 0.17700477\n",
      "Iteration 434, loss = 0.17663685\n",
      "Iteration 435, loss = 0.17627026\n",
      "Iteration 436, loss = 0.17590500\n",
      "Iteration 437, loss = 0.17554107\n",
      "Iteration 438, loss = 0.17517845\n",
      "Iteration 439, loss = 0.17481715\n",
      "Iteration 440, loss = 0.17445718\n",
      "Iteration 441, loss = 0.17409851\n",
      "Iteration 442, loss = 0.17374116\n",
      "Iteration 443, loss = 0.17338511\n",
      "Iteration 444, loss = 0.17303038\n",
      "Iteration 445, loss = 0.17267695\n",
      "Iteration 446, loss = 0.17232481\n",
      "Iteration 447, loss = 0.17197398\n",
      "Iteration 448, loss = 0.17162444\n",
      "Iteration 449, loss = 0.17127620\n",
      "Iteration 450, loss = 0.17092925\n",
      "Iteration 451, loss = 0.17058359\n",
      "Iteration 452, loss = 0.17023921\n",
      "Iteration 453, loss = 0.16989611\n",
      "Iteration 454, loss = 0.16955429\n",
      "Iteration 455, loss = 0.16921375\n",
      "Iteration 456, loss = 0.16887449\n",
      "Iteration 457, loss = 0.16853649\n",
      "Iteration 458, loss = 0.16819977\n",
      "Iteration 459, loss = 0.16786431\n",
      "Iteration 460, loss = 0.16753011\n",
      "Iteration 461, loss = 0.16719717\n",
      "Iteration 462, loss = 0.16686549\n",
      "Iteration 463, loss = 0.16653506\n",
      "Iteration 464, loss = 0.16620588\n",
      "Iteration 465, loss = 0.16587795\n",
      "Iteration 466, loss = 0.16555126\n",
      "Iteration 467, loss = 0.16522582\n",
      "Iteration 468, loss = 0.16490161\n",
      "Iteration 469, loss = 0.16457864\n",
      "Iteration 470, loss = 0.16425690\n",
      "Iteration 471, loss = 0.16393639\n",
      "Iteration 472, loss = 0.16361710\n",
      "Iteration 473, loss = 0.16329904\n",
      "Iteration 474, loss = 0.16298220\n",
      "Iteration 475, loss = 0.16266657\n",
      "Iteration 476, loss = 0.16235216\n",
      "Iteration 477, loss = 0.16203896\n",
      "Iteration 478, loss = 0.16172696\n",
      "Iteration 479, loss = 0.16141616\n",
      "Iteration 480, loss = 0.16110657\n",
      "Iteration 481, loss = 0.16079817\n",
      "Iteration 482, loss = 0.16049097\n",
      "Iteration 483, loss = 0.16018495\n",
      "Iteration 484, loss = 0.15988013\n",
      "Iteration 485, loss = 0.15957648\n",
      "Iteration 486, loss = 0.15927402\n",
      "Iteration 487, loss = 0.15897273\n",
      "Iteration 488, loss = 0.15867261\n",
      "Iteration 489, loss = 0.15837367\n",
      "Iteration 490, loss = 0.15807589\n",
      "Iteration 491, loss = 0.15777927\n",
      "Iteration 492, loss = 0.15748381\n",
      "Iteration 493, loss = 0.15718951\n",
      "Iteration 494, loss = 0.15689636\n",
      "Iteration 495, loss = 0.15660436\n",
      "Iteration 496, loss = 0.15631351\n",
      "Iteration 497, loss = 0.15602379\n",
      "Iteration 498, loss = 0.15573522\n",
      "Iteration 499, loss = 0.15544778\n",
      "Iteration 500, loss = 0.15516147\n",
      "Iteration 1, loss = 1.43172754\n",
      "Iteration 2, loss = 1.41562849\n",
      "Iteration 3, loss = 1.39311180\n",
      "Iteration 4, loss = 1.36526189\n",
      "Iteration 5, loss = 1.33312466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 1.29768739\n",
      "Iteration 7, loss = 1.25986436\n",
      "Iteration 8, loss = 1.22048718\n",
      "Iteration 9, loss = 1.18029887\n",
      "Iteration 10, loss = 1.13995095\n",
      "Iteration 11, loss = 1.10000299\n",
      "Iteration 12, loss = 1.06092406\n",
      "Iteration 13, loss = 1.02309597\n",
      "Iteration 14, loss = 0.98681771\n",
      "Iteration 15, loss = 0.95231127\n",
      "Iteration 16, loss = 0.91972840\n",
      "Iteration 17, loss = 0.88915819\n",
      "Iteration 18, loss = 0.86063524\n",
      "Iteration 19, loss = 0.83414812\n",
      "Iteration 20, loss = 0.80964788\n",
      "Iteration 21, loss = 0.78705615\n",
      "Iteration 22, loss = 0.76627281\n",
      "Iteration 23, loss = 0.74718283\n",
      "Iteration 24, loss = 0.72966225\n",
      "Iteration 25, loss = 0.71358319\n",
      "Iteration 26, loss = 0.69881793\n",
      "Iteration 27, loss = 0.68524204\n",
      "Iteration 28, loss = 0.67273676\n",
      "Iteration 29, loss = 0.66119057\n",
      "Iteration 30, loss = 0.65050025\n",
      "Iteration 31, loss = 0.64057140\n",
      "Iteration 32, loss = 0.63131858\n",
      "Iteration 33, loss = 0.62266518\n",
      "Iteration 34, loss = 0.61454307\n",
      "Iteration 35, loss = 0.60689207\n",
      "Iteration 36, loss = 0.59965942\n",
      "Iteration 37, loss = 0.59279909\n",
      "Iteration 38, loss = 0.58627113\n",
      "Iteration 39, loss = 0.58004105\n",
      "Iteration 40, loss = 0.57407917\n",
      "Iteration 41, loss = 0.56836002\n",
      "Iteration 42, loss = 0.56286182\n",
      "Iteration 43, loss = 0.55756593\n",
      "Iteration 44, loss = 0.55245644\n",
      "Iteration 45, loss = 0.54751972\n",
      "Iteration 46, loss = 0.54274410\n",
      "Iteration 47, loss = 0.53811950\n",
      "Iteration 48, loss = 0.53363720\n",
      "Iteration 49, loss = 0.52928959\n",
      "Iteration 50, loss = 0.52506993\n",
      "Iteration 51, loss = 0.52097225\n",
      "Iteration 52, loss = 0.51699114\n",
      "Iteration 53, loss = 0.51312167\n",
      "Iteration 54, loss = 0.50935927\n",
      "Iteration 55, loss = 0.50569970\n",
      "Iteration 56, loss = 0.50213892\n",
      "Iteration 57, loss = 0.49867311\n",
      "Iteration 58, loss = 0.49529860\n",
      "Iteration 59, loss = 0.49201185\n",
      "Iteration 60, loss = 0.48880943\n",
      "Iteration 61, loss = 0.48568803\n",
      "Iteration 62, loss = 0.48264443\n",
      "Iteration 63, loss = 0.47967551\n",
      "Iteration 64, loss = 0.47677824\n",
      "Iteration 65, loss = 0.47394969\n",
      "Iteration 66, loss = 0.47118703\n",
      "Iteration 67, loss = 0.46848752\n",
      "Iteration 68, loss = 0.46584853\n",
      "Iteration 69, loss = 0.46326756\n",
      "Iteration 70, loss = 0.46074217\n",
      "Iteration 71, loss = 0.45827007\n",
      "Iteration 72, loss = 0.45584906\n",
      "Iteration 73, loss = 0.45347705\n",
      "Iteration 74, loss = 0.45115204\n",
      "Iteration 75, loss = 0.44887216\n",
      "Iteration 76, loss = 0.44663563\n",
      "Iteration 77, loss = 0.44444075\n",
      "Iteration 78, loss = 0.44228593\n",
      "Iteration 79, loss = 0.44016968\n",
      "Iteration 80, loss = 0.43809056\n",
      "Iteration 81, loss = 0.43604722\n",
      "Iteration 82, loss = 0.43403841\n",
      "Iteration 83, loss = 0.43206292\n",
      "Iteration 84, loss = 0.43011962\n",
      "Iteration 85, loss = 0.42820742\n",
      "Iteration 86, loss = 0.42632530\n",
      "Iteration 87, loss = 0.42447231\n",
      "Iteration 88, loss = 0.42264751\n",
      "Iteration 89, loss = 0.42085003\n",
      "Iteration 90, loss = 0.41907904\n",
      "Iteration 91, loss = 0.41733374\n",
      "Iteration 92, loss = 0.41561336\n",
      "Iteration 93, loss = 0.41391718\n",
      "Iteration 94, loss = 0.41224450\n",
      "Iteration 95, loss = 0.41059464\n",
      "Iteration 96, loss = 0.40896698\n",
      "Iteration 97, loss = 0.40736087\n",
      "Iteration 98, loss = 0.40577575\n",
      "Iteration 99, loss = 0.40421102\n",
      "Iteration 100, loss = 0.40266615\n",
      "Iteration 101, loss = 0.40114060\n",
      "Iteration 102, loss = 0.39963386\n",
      "Iteration 103, loss = 0.39814544\n",
      "Iteration 104, loss = 0.39667487\n",
      "Iteration 105, loss = 0.39522168\n",
      "Iteration 106, loss = 0.39378543\n",
      "Iteration 107, loss = 0.39236571\n",
      "Iteration 108, loss = 0.39096209\n",
      "Iteration 109, loss = 0.38957418\n",
      "Iteration 110, loss = 0.38820159\n",
      "Iteration 111, loss = 0.38684396\n",
      "Iteration 112, loss = 0.38550093\n",
      "Iteration 113, loss = 0.38417214\n",
      "Iteration 114, loss = 0.38285727\n",
      "Iteration 115, loss = 0.38155598\n",
      "Iteration 116, loss = 0.38026798\n",
      "Iteration 117, loss = 0.37899295\n",
      "Iteration 118, loss = 0.37773059\n",
      "Iteration 119, loss = 0.37648064\n",
      "Iteration 120, loss = 0.37524281\n",
      "Iteration 121, loss = 0.37401683\n",
      "Iteration 122, loss = 0.37280245\n",
      "Iteration 123, loss = 0.37159941\n",
      "Iteration 124, loss = 0.37040748\n",
      "Iteration 125, loss = 0.36922641\n",
      "Iteration 126, loss = 0.36805599\n",
      "Iteration 127, loss = 0.36689598\n",
      "Iteration 128, loss = 0.36574617\n",
      "Iteration 129, loss = 0.36460635\n",
      "Iteration 130, loss = 0.36347633\n",
      "Iteration 131, loss = 0.36235589\n",
      "Iteration 132, loss = 0.36124486\n",
      "Iteration 133, loss = 0.36014303\n",
      "Iteration 134, loss = 0.35905024\n",
      "Iteration 135, loss = 0.35796630\n",
      "Iteration 136, loss = 0.35689104\n",
      "Iteration 137, loss = 0.35582429\n",
      "Iteration 138, loss = 0.35476590\n",
      "Iteration 139, loss = 0.35371570\n",
      "Iteration 140, loss = 0.35267354\n",
      "Iteration 141, loss = 0.35163926\n",
      "Iteration 142, loss = 0.35061273\n",
      "Iteration 143, loss = 0.34959379\n",
      "Iteration 144, loss = 0.34858231\n",
      "Iteration 145, loss = 0.34757816\n",
      "Iteration 146, loss = 0.34658120\n",
      "Iteration 147, loss = 0.34559130\n",
      "Iteration 148, loss = 0.34460834\n",
      "Iteration 149, loss = 0.34363219\n",
      "Iteration 150, loss = 0.34266273\n",
      "Iteration 151, loss = 0.34169985\n",
      "Iteration 152, loss = 0.34074344\n",
      "Iteration 153, loss = 0.33979338\n",
      "Iteration 154, loss = 0.33884956\n",
      "Iteration 155, loss = 0.33791188\n",
      "Iteration 156, loss = 0.33698024\n",
      "Iteration 157, loss = 0.33605453\n",
      "Iteration 158, loss = 0.33513466\n",
      "Iteration 159, loss = 0.33422053\n",
      "Iteration 160, loss = 0.33331205\n",
      "Iteration 161, loss = 0.33240912\n",
      "Iteration 162, loss = 0.33151165\n",
      "Iteration 163, loss = 0.33061956\n",
      "Iteration 164, loss = 0.32973276\n",
      "Iteration 165, loss = 0.32885117\n",
      "Iteration 166, loss = 0.32797471\n",
      "Iteration 167, loss = 0.32710329\n",
      "Iteration 168, loss = 0.32623684\n",
      "Iteration 169, loss = 0.32537529\n",
      "Iteration 170, loss = 0.32451855\n",
      "Iteration 171, loss = 0.32366655\n",
      "Iteration 172, loss = 0.32281923\n",
      "Iteration 173, loss = 0.32197651\n",
      "Iteration 174, loss = 0.32113832\n",
      "Iteration 175, loss = 0.32030460\n",
      "Iteration 176, loss = 0.31947528\n",
      "Iteration 177, loss = 0.31865031\n",
      "Iteration 178, loss = 0.31782960\n",
      "Iteration 179, loss = 0.31701311\n",
      "Iteration 180, loss = 0.31620078\n",
      "Iteration 181, loss = 0.31539254\n",
      "Iteration 182, loss = 0.31458834\n",
      "Iteration 183, loss = 0.31378812\n",
      "Iteration 184, loss = 0.31299183\n",
      "Iteration 185, loss = 0.31219941\n",
      "Iteration 186, loss = 0.31141081\n",
      "Iteration 187, loss = 0.31062598\n",
      "Iteration 188, loss = 0.30984487\n",
      "Iteration 189, loss = 0.30906742\n",
      "Iteration 190, loss = 0.30829360\n",
      "Iteration 191, loss = 0.30752335\n",
      "Iteration 192, loss = 0.30675662\n",
      "Iteration 193, loss = 0.30599337\n",
      "Iteration 194, loss = 0.30523356\n",
      "Iteration 195, loss = 0.30447714\n",
      "Iteration 196, loss = 0.30372407\n",
      "Iteration 197, loss = 0.30297431\n",
      "Iteration 198, loss = 0.30222781\n",
      "Iteration 199, loss = 0.30148454\n",
      "Iteration 200, loss = 0.30074445\n",
      "Iteration 201, loss = 0.30000751\n",
      "Iteration 202, loss = 0.29927369\n",
      "Iteration 203, loss = 0.29854293\n",
      "Iteration 204, loss = 0.29781521\n",
      "Iteration 205, loss = 0.29709049\n",
      "Iteration 206, loss = 0.29636873\n",
      "Iteration 207, loss = 0.29564990\n",
      "Iteration 208, loss = 0.29493398\n",
      "Iteration 209, loss = 0.29422091\n",
      "Iteration 210, loss = 0.29351068\n",
      "Iteration 211, loss = 0.29280325\n",
      "Iteration 212, loss = 0.29209859\n",
      "Iteration 213, loss = 0.29139666\n",
      "Iteration 214, loss = 0.29069745\n",
      "Iteration 215, loss = 0.29000092\n",
      "Iteration 216, loss = 0.28930703\n",
      "Iteration 217, loss = 0.28861578\n",
      "Iteration 218, loss = 0.28792711\n",
      "Iteration 219, loss = 0.28724102\n",
      "Iteration 220, loss = 0.28655747\n",
      "Iteration 221, loss = 0.28587643\n",
      "Iteration 222, loss = 0.28519788\n",
      "Iteration 223, loss = 0.28452181\n",
      "Iteration 224, loss = 0.28384817\n",
      "Iteration 225, loss = 0.28317695\n",
      "Iteration 226, loss = 0.28250812\n",
      "Iteration 227, loss = 0.28184167\n",
      "Iteration 228, loss = 0.28117756\n",
      "Iteration 229, loss = 0.28051578\n",
      "Iteration 230, loss = 0.27985631\n",
      "Iteration 231, loss = 0.27919912\n",
      "Iteration 232, loss = 0.27854419\n",
      "Iteration 233, loss = 0.27789150\n",
      "Iteration 234, loss = 0.27724103\n",
      "Iteration 235, loss = 0.27659277\n",
      "Iteration 236, loss = 0.27594669\n",
      "Iteration 237, loss = 0.27530278\n",
      "Iteration 238, loss = 0.27466101\n",
      "Iteration 239, loss = 0.27402136\n",
      "Iteration 240, loss = 0.27338383\n",
      "Iteration 241, loss = 0.27274839\n",
      "Iteration 242, loss = 0.27211502\n",
      "Iteration 243, loss = 0.27148371\n",
      "Iteration 244, loss = 0.27085445\n",
      "Iteration 245, loss = 0.27022720\n",
      "Iteration 246, loss = 0.26960197\n",
      "Iteration 247, loss = 0.26897874\n",
      "Iteration 248, loss = 0.26835748\n",
      "Iteration 249, loss = 0.26773818\n",
      "Iteration 250, loss = 0.26712084\n",
      "Iteration 251, loss = 0.26650543\n",
      "Iteration 252, loss = 0.26589194\n",
      "Iteration 253, loss = 0.26528036\n",
      "Iteration 254, loss = 0.26467068\n",
      "Iteration 255, loss = 0.26406287\n",
      "Iteration 256, loss = 0.26345693\n",
      "Iteration 257, loss = 0.26285285\n",
      "Iteration 258, loss = 0.26225061\n",
      "Iteration 259, loss = 0.26165020\n",
      "Iteration 260, loss = 0.26105161\n",
      "Iteration 261, loss = 0.26045482\n",
      "Iteration 262, loss = 0.25985984\n",
      "Iteration 263, loss = 0.25926663\n",
      "Iteration 264, loss = 0.25867520\n",
      "Iteration 265, loss = 0.25808553\n",
      "Iteration 266, loss = 0.25749762\n",
      "Iteration 267, loss = 0.25691144\n",
      "Iteration 268, loss = 0.25632700\n",
      "Iteration 269, loss = 0.25574428\n",
      "Iteration 270, loss = 0.25516327\n",
      "Iteration 271, loss = 0.25458396\n",
      "Iteration 272, loss = 0.25400634\n",
      "Iteration 273, loss = 0.25343041\n",
      "Iteration 274, loss = 0.25285616\n",
      "Iteration 275, loss = 0.25228357\n",
      "Iteration 276, loss = 0.25171263\n",
      "Iteration 277, loss = 0.25114335\n",
      "Iteration 278, loss = 0.25057571\n",
      "Iteration 279, loss = 0.25000970\n",
      "Iteration 280, loss = 0.24944532\n",
      "Iteration 281, loss = 0.24888255\n",
      "Iteration 282, loss = 0.24832139\n",
      "Iteration 283, loss = 0.24776184\n",
      "Iteration 284, loss = 0.24720388\n",
      "Iteration 285, loss = 0.24664751\n",
      "Iteration 286, loss = 0.24609272\n",
      "Iteration 287, loss = 0.24553951\n",
      "Iteration 288, loss = 0.24498786\n",
      "Iteration 289, loss = 0.24443777\n",
      "Iteration 290, loss = 0.24388924\n",
      "Iteration 291, loss = 0.24334226\n",
      "Iteration 292, loss = 0.24279682\n",
      "Iteration 293, loss = 0.24225292\n",
      "Iteration 294, loss = 0.24171055\n",
      "Iteration 295, loss = 0.24116970\n",
      "Iteration 296, loss = 0.24063037\n",
      "Iteration 297, loss = 0.24009256\n",
      "Iteration 298, loss = 0.23955626\n",
      "Iteration 299, loss = 0.23902146\n",
      "Iteration 300, loss = 0.23848816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 301, loss = 0.23795635\n",
      "Iteration 302, loss = 0.23742603\n",
      "Iteration 303, loss = 0.23689720\n",
      "Iteration 304, loss = 0.23636984\n",
      "Iteration 305, loss = 0.23584396\n",
      "Iteration 306, loss = 0.23531955\n",
      "Iteration 307, loss = 0.23479661\n",
      "Iteration 308, loss = 0.23427513\n",
      "Iteration 309, loss = 0.23375510\n",
      "Iteration 310, loss = 0.23323653\n",
      "Iteration 311, loss = 0.23271941\n",
      "Iteration 312, loss = 0.23220374\n",
      "Iteration 313, loss = 0.23168951\n",
      "Iteration 314, loss = 0.23117671\n",
      "Iteration 315, loss = 0.23066535\n",
      "Iteration 316, loss = 0.23015542\n",
      "Iteration 317, loss = 0.22964692\n",
      "Iteration 318, loss = 0.22913984\n",
      "Iteration 319, loss = 0.22863419\n",
      "Iteration 320, loss = 0.22812995\n",
      "Iteration 321, loss = 0.22762712\n",
      "Iteration 322, loss = 0.22712571\n",
      "Iteration 323, loss = 0.22662571\n",
      "Iteration 324, loss = 0.22612711\n",
      "Iteration 325, loss = 0.22562991\n",
      "Iteration 326, loss = 0.22513412\n",
      "Iteration 327, loss = 0.22463972\n",
      "Iteration 328, loss = 0.22414671\n",
      "Iteration 329, loss = 0.22365510\n",
      "Iteration 330, loss = 0.22316487\n",
      "Iteration 331, loss = 0.22267603\n",
      "Iteration 332, loss = 0.22218858\n",
      "Iteration 333, loss = 0.22170251\n",
      "Iteration 334, loss = 0.22121781\n",
      "Iteration 335, loss = 0.22073450\n",
      "Iteration 336, loss = 0.22025255\n",
      "Iteration 337, loss = 0.21977198\n",
      "Iteration 338, loss = 0.21929278\n",
      "Iteration 339, loss = 0.21881495\n",
      "Iteration 340, loss = 0.21833849\n",
      "Iteration 341, loss = 0.21786338\n",
      "Iteration 342, loss = 0.21738964\n",
      "Iteration 343, loss = 0.21691726\n",
      "Iteration 344, loss = 0.21644624\n",
      "Iteration 345, loss = 0.21597658\n",
      "Iteration 346, loss = 0.21550826\n",
      "Iteration 347, loss = 0.21504130\n",
      "Iteration 348, loss = 0.21457569\n",
      "Iteration 349, loss = 0.21411143\n",
      "Iteration 350, loss = 0.21364852\n",
      "Iteration 351, loss = 0.21318695\n",
      "Iteration 352, loss = 0.21272673\n",
      "Iteration 353, loss = 0.21226785\n",
      "Iteration 354, loss = 0.21181030\n",
      "Iteration 355, loss = 0.21135410\n",
      "Iteration 356, loss = 0.21089923\n",
      "Iteration 357, loss = 0.21044570\n",
      "Iteration 358, loss = 0.20999351\n",
      "Iteration 359, loss = 0.20954264\n",
      "Iteration 360, loss = 0.20909311\n",
      "Iteration 361, loss = 0.20864491\n",
      "Iteration 362, loss = 0.20819803\n",
      "Iteration 363, loss = 0.20775248\n",
      "Iteration 364, loss = 0.20730826\n",
      "Iteration 365, loss = 0.20686536\n",
      "Iteration 366, loss = 0.20642378\n",
      "Iteration 367, loss = 0.20598353\n",
      "Iteration 368, loss = 0.20554459\n",
      "Iteration 369, loss = 0.20510697\n",
      "Iteration 370, loss = 0.20467067\n",
      "Iteration 371, loss = 0.20423569\n",
      "Iteration 372, loss = 0.20380202\n",
      "Iteration 373, loss = 0.20336966\n",
      "Iteration 374, loss = 0.20293862\n",
      "Iteration 375, loss = 0.20250888\n",
      "Iteration 376, loss = 0.20208045\n",
      "Iteration 377, loss = 0.20165333\n",
      "Iteration 378, loss = 0.20122752\n",
      "Iteration 379, loss = 0.20080301\n",
      "Iteration 380, loss = 0.20037981\n",
      "Iteration 381, loss = 0.19995791\n",
      "Iteration 382, loss = 0.19953731\n",
      "Iteration 383, loss = 0.19911801\n",
      "Iteration 384, loss = 0.19870001\n",
      "Iteration 385, loss = 0.19828331\n",
      "Iteration 386, loss = 0.19786790\n",
      "Iteration 387, loss = 0.19745379\n",
      "Iteration 388, loss = 0.19704097\n",
      "Iteration 389, loss = 0.19662944\n",
      "Iteration 390, loss = 0.19621920\n",
      "Iteration 391, loss = 0.19581026\n",
      "Iteration 392, loss = 0.19540260\n",
      "Iteration 393, loss = 0.19499623\n",
      "Iteration 394, loss = 0.19459114\n",
      "Iteration 395, loss = 0.19418734\n",
      "Iteration 396, loss = 0.19378482\n",
      "Iteration 397, loss = 0.19338358\n",
      "Iteration 398, loss = 0.19298362\n",
      "Iteration 399, loss = 0.19258494\n",
      "Iteration 400, loss = 0.19218754\n",
      "Iteration 401, loss = 0.19179142\n",
      "Iteration 402, loss = 0.19139657\n",
      "Iteration 403, loss = 0.19100299\n",
      "Iteration 404, loss = 0.19061068\n",
      "Iteration 405, loss = 0.19021965\n",
      "Iteration 406, loss = 0.18982988\n",
      "Iteration 407, loss = 0.18944138\n",
      "Iteration 408, loss = 0.18905415\n",
      "Iteration 409, loss = 0.18866818\n",
      "Iteration 410, loss = 0.18828347\n",
      "Iteration 411, loss = 0.18790002\n",
      "Iteration 412, loss = 0.18751784\n",
      "Iteration 413, loss = 0.18713691\n",
      "Iteration 414, loss = 0.18675724\n",
      "Iteration 415, loss = 0.18637883\n",
      "Iteration 416, loss = 0.18600166\n",
      "Iteration 417, loss = 0.18562575\n",
      "Iteration 418, loss = 0.18525109\n",
      "Iteration 419, loss = 0.18487768\n",
      "Iteration 420, loss = 0.18450552\n",
      "Iteration 421, loss = 0.18413460\n",
      "Iteration 422, loss = 0.18376493\n",
      "Iteration 423, loss = 0.18339649\n",
      "Iteration 424, loss = 0.18302930\n",
      "Iteration 425, loss = 0.18266335\n",
      "Iteration 426, loss = 0.18229863\n",
      "Iteration 427, loss = 0.18193515\n",
      "Iteration 428, loss = 0.18157290\n",
      "Iteration 429, loss = 0.18121188\n",
      "Iteration 430, loss = 0.18085209\n",
      "Iteration 431, loss = 0.18049353\n",
      "Iteration 432, loss = 0.18013619\n",
      "Iteration 433, loss = 0.17978008\n",
      "Iteration 434, loss = 0.17942519\n",
      "Iteration 435, loss = 0.17907152\n",
      "Iteration 436, loss = 0.17871907\n",
      "Iteration 437, loss = 0.17836783\n",
      "Iteration 438, loss = 0.17801781\n",
      "Iteration 439, loss = 0.17766900\n",
      "Iteration 440, loss = 0.17732141\n",
      "Iteration 441, loss = 0.17697501\n",
      "Iteration 442, loss = 0.17662983\n",
      "Iteration 443, loss = 0.17628585\n",
      "Iteration 444, loss = 0.17594307\n",
      "Iteration 445, loss = 0.17560149\n",
      "Iteration 446, loss = 0.17526111\n",
      "Iteration 447, loss = 0.17492192\n",
      "Iteration 448, loss = 0.17458393\n",
      "Iteration 449, loss = 0.17424713\n",
      "Iteration 450, loss = 0.17391152\n",
      "Iteration 451, loss = 0.17357709\n",
      "Iteration 452, loss = 0.17324385\n",
      "Iteration 453, loss = 0.17291179\n",
      "Iteration 454, loss = 0.17258091\n",
      "Iteration 455, loss = 0.17225121\n",
      "Iteration 456, loss = 0.17192268\n",
      "Iteration 457, loss = 0.17159533\n",
      "Iteration 458, loss = 0.17126914\n",
      "Iteration 459, loss = 0.17094413\n",
      "Iteration 460, loss = 0.17062028\n",
      "Iteration 461, loss = 0.17029759\n",
      "Iteration 462, loss = 0.16997607\n",
      "Iteration 463, loss = 0.16965570\n",
      "Iteration 464, loss = 0.16933649\n",
      "Iteration 465, loss = 0.16901843\n",
      "Iteration 466, loss = 0.16870153\n",
      "Iteration 467, loss = 0.16838577\n",
      "Iteration 468, loss = 0.16807116\n",
      "Iteration 469, loss = 0.16775769\n",
      "Iteration 470, loss = 0.16744536\n",
      "Iteration 471, loss = 0.16713417\n",
      "Iteration 472, loss = 0.16682412\n",
      "Iteration 473, loss = 0.16651520\n",
      "Iteration 474, loss = 0.16620741\n",
      "Iteration 475, loss = 0.16590075\n",
      "Iteration 476, loss = 0.16559521\n",
      "Iteration 477, loss = 0.16529079\n",
      "Iteration 478, loss = 0.16498750\n",
      "Iteration 479, loss = 0.16468532\n",
      "Iteration 480, loss = 0.16438426\n",
      "Iteration 481, loss = 0.16408430\n",
      "Iteration 482, loss = 0.16378546\n",
      "Iteration 483, loss = 0.16348772\n",
      "Iteration 484, loss = 0.16319108\n",
      "Iteration 485, loss = 0.16289555\n",
      "Iteration 486, loss = 0.16260111\n",
      "Iteration 487, loss = 0.16230777\n",
      "Iteration 488, loss = 0.16201552\n",
      "Iteration 489, loss = 0.16172435\n",
      "Iteration 490, loss = 0.16143428\n",
      "Iteration 491, loss = 0.16114528\n",
      "Iteration 492, loss = 0.16085737\n",
      "Iteration 493, loss = 0.16057053\n",
      "Iteration 494, loss = 0.16028477\n",
      "Iteration 495, loss = 0.16000008\n",
      "Iteration 496, loss = 0.15971646\n",
      "Iteration 497, loss = 0.15943391\n",
      "Iteration 498, loss = 0.15915241\n",
      "Iteration 499, loss = 0.15887198\n",
      "Iteration 500, loss = 0.15859260\n",
      "Iteration 1, loss = 1.43109142\n",
      "Iteration 2, loss = 1.41510075\n",
      "Iteration 3, loss = 1.39273661\n",
      "Iteration 4, loss = 1.36507667\n",
      "Iteration 5, loss = 1.33315999\n",
      "Iteration 6, loss = 1.29796694\n",
      "Iteration 7, loss = 1.26040492\n",
      "Iteration 8, loss = 1.22129885\n",
      "Iteration 9, loss = 1.18138548\n",
      "Iteration 10, loss = 1.14131074\n",
      "Iteration 11, loss = 1.10162955\n",
      "Iteration 12, loss = 1.06280749\n",
      "Iteration 13, loss = 1.02522408\n",
      "Iteration 14, loss = 0.98917736\n",
      "Iteration 15, loss = 0.95488955\n",
      "Iteration 16, loss = 0.92251359\n",
      "Iteration 17, loss = 0.89214055\n",
      "Iteration 18, loss = 0.86380746\n",
      "Iteration 19, loss = 0.83750556\n",
      "Iteration 20, loss = 0.81318849\n",
      "Iteration 21, loss = 0.79078031\n",
      "Iteration 22, loss = 0.77018294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.75128304\n",
      "Iteration 24, loss = 0.73395789\n",
      "Iteration 25, loss = 0.71808044\n",
      "Iteration 26, loss = 0.70352341\n",
      "Iteration 27, loss = 0.69016250\n",
      "Iteration 28, loss = 0.67787881\n",
      "Iteration 29, loss = 0.66656044\n",
      "Iteration 30, loss = 0.65610366\n",
      "Iteration 31, loss = 0.64641342\n",
      "Iteration 32, loss = 0.63740357\n",
      "Iteration 33, loss = 0.62899675\n",
      "Iteration 34, loss = 0.62112406\n",
      "Iteration 35, loss = 0.61372460\n",
      "Iteration 36, loss = 0.60674486\n",
      "Iteration 37, loss = 0.60013815\n",
      "Iteration 38, loss = 0.59386389\n",
      "Iteration 39, loss = 0.58788702\n",
      "Iteration 40, loss = 0.58217733\n",
      "Iteration 41, loss = 0.57670890\n",
      "Iteration 42, loss = 0.57145950\n",
      "Iteration 43, loss = 0.56641014\n",
      "Iteration 44, loss = 0.56154458\n",
      "Iteration 45, loss = 0.55684889\n",
      "Iteration 46, loss = 0.55231114\n",
      "Iteration 47, loss = 0.54792103\n",
      "Iteration 48, loss = 0.54366963\n",
      "Iteration 49, loss = 0.53954913\n",
      "Iteration 50, loss = 0.53555266\n",
      "Iteration 51, loss = 0.53167408\n",
      "Iteration 52, loss = 0.52790784\n",
      "Iteration 53, loss = 0.52424890\n",
      "Iteration 54, loss = 0.52069260\n",
      "Iteration 55, loss = 0.51723457\n",
      "Iteration 56, loss = 0.51387072\n",
      "Iteration 57, loss = 0.51059716\n",
      "Iteration 58, loss = 0.50741015\n",
      "Iteration 59, loss = 0.50430611\n",
      "Iteration 60, loss = 0.50128161\n",
      "Iteration 61, loss = 0.49833330\n",
      "Iteration 62, loss = 0.49545798\n",
      "Iteration 63, loss = 0.49265253\n",
      "Iteration 64, loss = 0.48991396\n",
      "Iteration 65, loss = 0.48723940\n",
      "Iteration 66, loss = 0.48462605\n",
      "Iteration 67, loss = 0.48207126\n",
      "Iteration 68, loss = 0.47957248\n",
      "Iteration 69, loss = 0.47712728\n",
      "Iteration 70, loss = 0.47473335\n",
      "Iteration 71, loss = 0.47238848\n",
      "Iteration 72, loss = 0.47009059\n",
      "Iteration 73, loss = 0.46783770\n",
      "Iteration 74, loss = 0.46562793\n",
      "Iteration 75, loss = 0.46345953\n",
      "Iteration 76, loss = 0.46133082\n",
      "Iteration 77, loss = 0.45924025\n",
      "Iteration 78, loss = 0.45718632\n",
      "Iteration 79, loss = 0.45516763\n",
      "Iteration 80, loss = 0.45318289\n",
      "Iteration 81, loss = 0.45123083\n",
      "Iteration 82, loss = 0.44931029\n",
      "Iteration 83, loss = 0.44742015\n",
      "Iteration 84, loss = 0.44555938\n",
      "Iteration 85, loss = 0.44372696\n",
      "Iteration 86, loss = 0.44192196\n",
      "Iteration 87, loss = 0.44014347\n",
      "Iteration 88, loss = 0.43839064\n",
      "Iteration 89, loss = 0.43666266\n",
      "Iteration 90, loss = 0.43495873\n",
      "Iteration 91, loss = 0.43327813\n",
      "Iteration 92, loss = 0.43162014\n",
      "Iteration 93, loss = 0.42998406\n",
      "Iteration 94, loss = 0.42836926\n",
      "Iteration 95, loss = 0.42677509\n",
      "Iteration 96, loss = 0.42520096\n",
      "Iteration 97, loss = 0.42364628\n",
      "Iteration 98, loss = 0.42211050\n",
      "Iteration 99, loss = 0.42059308\n",
      "Iteration 100, loss = 0.41909350\n",
      "Iteration 101, loss = 0.41761128\n",
      "Iteration 102, loss = 0.41614592\n",
      "Iteration 103, loss = 0.41469698\n",
      "Iteration 104, loss = 0.41326401\n",
      "Iteration 105, loss = 0.41184658\n",
      "Iteration 106, loss = 0.41044428\n",
      "Iteration 107, loss = 0.40905673\n",
      "Iteration 108, loss = 0.40768353\n",
      "Iteration 109, loss = 0.40632433\n",
      "Iteration 110, loss = 0.40497877\n",
      "Iteration 111, loss = 0.40364650\n",
      "Iteration 112, loss = 0.40232721\n",
      "Iteration 113, loss = 0.40102058\n",
      "Iteration 114, loss = 0.39972629\n",
      "Iteration 115, loss = 0.39844406\n",
      "Iteration 116, loss = 0.39717360\n",
      "Iteration 117, loss = 0.39591464\n",
      "Iteration 118, loss = 0.39466691\n",
      "Iteration 119, loss = 0.39343015\n",
      "Iteration 120, loss = 0.39220411\n",
      "Iteration 121, loss = 0.39098856\n",
      "Iteration 122, loss = 0.38978326\n",
      "Iteration 123, loss = 0.38858799\n",
      "Iteration 124, loss = 0.38740252\n",
      "Iteration 125, loss = 0.38622665\n",
      "Iteration 126, loss = 0.38506017\n",
      "Iteration 127, loss = 0.38390288\n",
      "Iteration 128, loss = 0.38275458\n",
      "Iteration 129, loss = 0.38161509\n",
      "Iteration 130, loss = 0.38048422\n",
      "Iteration 131, loss = 0.37936181\n",
      "Iteration 132, loss = 0.37824767\n",
      "Iteration 133, loss = 0.37714164\n",
      "Iteration 134, loss = 0.37604355\n",
      "Iteration 135, loss = 0.37495325\n",
      "Iteration 136, loss = 0.37387058\n",
      "Iteration 137, loss = 0.37279540\n",
      "Iteration 138, loss = 0.37172755\n",
      "Iteration 139, loss = 0.37066691\n",
      "Iteration 140, loss = 0.36961332\n",
      "Iteration 141, loss = 0.36856665\n",
      "Iteration 142, loss = 0.36752678\n",
      "Iteration 143, loss = 0.36649357\n",
      "Iteration 144, loss = 0.36546690\n",
      "Iteration 145, loss = 0.36444666\n",
      "Iteration 146, loss = 0.36343272\n",
      "Iteration 147, loss = 0.36242497\n",
      "Iteration 148, loss = 0.36142330\n",
      "Iteration 149, loss = 0.36042759\n",
      "Iteration 150, loss = 0.35943775\n",
      "Iteration 151, loss = 0.35845367\n",
      "Iteration 152, loss = 0.35747524\n",
      "Iteration 153, loss = 0.35650238\n",
      "Iteration 154, loss = 0.35553498\n",
      "Iteration 155, loss = 0.35457295\n",
      "Iteration 156, loss = 0.35361621\n",
      "Iteration 157, loss = 0.35266465\n",
      "Iteration 158, loss = 0.35171820\n",
      "Iteration 159, loss = 0.35077676\n",
      "Iteration 160, loss = 0.34984027\n",
      "Iteration 161, loss = 0.34890862\n",
      "Iteration 162, loss = 0.34798176\n",
      "Iteration 163, loss = 0.34705960\n",
      "Iteration 164, loss = 0.34614206\n",
      "Iteration 165, loss = 0.34522907\n",
      "Iteration 166, loss = 0.34432056\n",
      "Iteration 167, loss = 0.34341646\n",
      "Iteration 168, loss = 0.34251671\n",
      "Iteration 169, loss = 0.34162123\n",
      "Iteration 170, loss = 0.34072996\n",
      "Iteration 171, loss = 0.33984283\n",
      "Iteration 172, loss = 0.33895979\n",
      "Iteration 173, loss = 0.33808077\n",
      "Iteration 174, loss = 0.33720572\n",
      "Iteration 175, loss = 0.33633457\n",
      "Iteration 176, loss = 0.33546727\n",
      "Iteration 177, loss = 0.33460376\n",
      "Iteration 178, loss = 0.33374399\n",
      "Iteration 179, loss = 0.33288791\n",
      "Iteration 180, loss = 0.33203546\n",
      "Iteration 181, loss = 0.33118659\n",
      "Iteration 182, loss = 0.33034126\n",
      "Iteration 183, loss = 0.32949941\n",
      "Iteration 184, loss = 0.32866101\n",
      "Iteration 185, loss = 0.32782599\n",
      "Iteration 186, loss = 0.32699432\n",
      "Iteration 187, loss = 0.32616595\n",
      "Iteration 188, loss = 0.32534084\n",
      "Iteration 189, loss = 0.32451895\n",
      "Iteration 190, loss = 0.32370023\n",
      "Iteration 191, loss = 0.32288465\n",
      "Iteration 192, loss = 0.32207216\n",
      "Iteration 193, loss = 0.32126273\n",
      "Iteration 194, loss = 0.32045632\n",
      "Iteration 195, loss = 0.31965289\n",
      "Iteration 196, loss = 0.31885240\n",
      "Iteration 197, loss = 0.31805482\n",
      "Iteration 198, loss = 0.31726012\n",
      "Iteration 199, loss = 0.31646825\n",
      "Iteration 200, loss = 0.31567920\n",
      "Iteration 201, loss = 0.31489292\n",
      "Iteration 202, loss = 0.31410938\n",
      "Iteration 203, loss = 0.31332856\n",
      "Iteration 204, loss = 0.31255042\n",
      "Iteration 205, loss = 0.31177492\n",
      "Iteration 206, loss = 0.31100206\n",
      "Iteration 207, loss = 0.31023178\n",
      "Iteration 208, loss = 0.30946408\n",
      "Iteration 209, loss = 0.30869891\n",
      "Iteration 210, loss = 0.30793625\n",
      "Iteration 211, loss = 0.30717609\n",
      "Iteration 212, loss = 0.30641838\n",
      "Iteration 213, loss = 0.30566311\n",
      "Iteration 214, loss = 0.30491025\n",
      "Iteration 215, loss = 0.30415978\n",
      "Iteration 216, loss = 0.30341167\n",
      "Iteration 217, loss = 0.30266590\n",
      "Iteration 218, loss = 0.30192246\n",
      "Iteration 219, loss = 0.30118131\n",
      "Iteration 220, loss = 0.30044244\n",
      "Iteration 221, loss = 0.29970582\n",
      "Iteration 222, loss = 0.29897144\n",
      "Iteration 223, loss = 0.29823927\n",
      "Iteration 224, loss = 0.29750930\n",
      "Iteration 225, loss = 0.29678150\n",
      "Iteration 226, loss = 0.29605587\n",
      "Iteration 227, loss = 0.29533237\n",
      "Iteration 228, loss = 0.29461099\n",
      "Iteration 229, loss = 0.29389172\n",
      "Iteration 230, loss = 0.29317454\n",
      "Iteration 231, loss = 0.29245942\n",
      "Iteration 232, loss = 0.29174636\n",
      "Iteration 233, loss = 0.29103534\n",
      "Iteration 234, loss = 0.29032634\n",
      "Iteration 235, loss = 0.28961935\n",
      "Iteration 236, loss = 0.28891435\n",
      "Iteration 237, loss = 0.28821133\n",
      "Iteration 238, loss = 0.28751028\n",
      "Iteration 239, loss = 0.28681117\n",
      "Iteration 240, loss = 0.28611400\n",
      "Iteration 241, loss = 0.28541876\n",
      "Iteration 242, loss = 0.28472543\n",
      "Iteration 243, loss = 0.28403399\n",
      "Iteration 244, loss = 0.28334444\n",
      "Iteration 245, loss = 0.28265677\n",
      "Iteration 246, loss = 0.28197096\n",
      "Iteration 247, loss = 0.28128700\n",
      "Iteration 248, loss = 0.28060488\n",
      "Iteration 249, loss = 0.27992459\n",
      "Iteration 250, loss = 0.27924612\n",
      "Iteration 251, loss = 0.27856945\n",
      "Iteration 252, loss = 0.27789459\n",
      "Iteration 253, loss = 0.27722152\n",
      "Iteration 254, loss = 0.27655022\n",
      "Iteration 255, loss = 0.27588069\n",
      "Iteration 256, loss = 0.27521293\n",
      "Iteration 257, loss = 0.27454691\n",
      "Iteration 258, loss = 0.27388264\n",
      "Iteration 259, loss = 0.27322011\n",
      "Iteration 260, loss = 0.27255930\n",
      "Iteration 261, loss = 0.27190021\n",
      "Iteration 262, loss = 0.27124283\n",
      "Iteration 263, loss = 0.27058716\n",
      "Iteration 264, loss = 0.26993318\n",
      "Iteration 265, loss = 0.26928090\n",
      "Iteration 266, loss = 0.26863029\n",
      "Iteration 267, loss = 0.26798136\n",
      "Iteration 268, loss = 0.26733410\n",
      "Iteration 269, loss = 0.26668850\n",
      "Iteration 270, loss = 0.26604456\n",
      "Iteration 271, loss = 0.26540227\n",
      "Iteration 272, loss = 0.26476163\n",
      "Iteration 273, loss = 0.26412263\n",
      "Iteration 274, loss = 0.26348525\n",
      "Iteration 275, loss = 0.26284951\n",
      "Iteration 276, loss = 0.26221539\n",
      "Iteration 277, loss = 0.26158289\n",
      "Iteration 278, loss = 0.26095200\n",
      "Iteration 279, loss = 0.26032273\n",
      "Iteration 280, loss = 0.25969505\n",
      "Iteration 281, loss = 0.25906898\n",
      "Iteration 282, loss = 0.25844450\n",
      "Iteration 283, loss = 0.25782161\n",
      "Iteration 284, loss = 0.25720031\n",
      "Iteration 285, loss = 0.25658060\n",
      "Iteration 286, loss = 0.25596246\n",
      "Iteration 287, loss = 0.25534590\n",
      "Iteration 288, loss = 0.25473091\n",
      "Iteration 289, loss = 0.25411750\n",
      "Iteration 290, loss = 0.25350565\n",
      "Iteration 291, loss = 0.25289536\n",
      "Iteration 292, loss = 0.25228663\n",
      "Iteration 293, loss = 0.25167946\n",
      "Iteration 294, loss = 0.25107384\n",
      "Iteration 295, loss = 0.25046978\n",
      "Iteration 296, loss = 0.24986726\n",
      "Iteration 297, loss = 0.24926629\n",
      "Iteration 298, loss = 0.24866687\n",
      "Iteration 299, loss = 0.24806898\n",
      "Iteration 300, loss = 0.24747264\n",
      "Iteration 301, loss = 0.24687783\n",
      "Iteration 302, loss = 0.24628456\n",
      "Iteration 303, loss = 0.24569282\n",
      "Iteration 304, loss = 0.24510262\n",
      "Iteration 305, loss = 0.24451394\n",
      "Iteration 306, loss = 0.24392679\n",
      "Iteration 307, loss = 0.24334117\n",
      "Iteration 308, loss = 0.24275707\n",
      "Iteration 309, loss = 0.24217449\n",
      "Iteration 310, loss = 0.24159344\n",
      "Iteration 311, loss = 0.24101391\n",
      "Iteration 312, loss = 0.24043589\n",
      "Iteration 313, loss = 0.23985940\n",
      "Iteration 314, loss = 0.23928442\n",
      "Iteration 315, loss = 0.23871095\n",
      "Iteration 316, loss = 0.23813900\n",
      "Iteration 317, loss = 0.23756857\n",
      "Iteration 318, loss = 0.23699964\n",
      "Iteration 319, loss = 0.23643223\n",
      "Iteration 320, loss = 0.23586633\n",
      "Iteration 321, loss = 0.23530193\n",
      "Iteration 322, loss = 0.23473905\n",
      "Iteration 323, loss = 0.23417767\n",
      "Iteration 324, loss = 0.23361780\n",
      "Iteration 325, loss = 0.23305944\n",
      "Iteration 326, loss = 0.23250258\n",
      "Iteration 327, loss = 0.23194723\n",
      "Iteration 328, loss = 0.23139339\n",
      "Iteration 329, loss = 0.23084105\n",
      "Iteration 330, loss = 0.23029021\n",
      "Iteration 331, loss = 0.22974087\n",
      "Iteration 332, loss = 0.22919304\n",
      "Iteration 333, loss = 0.22864671\n",
      "Iteration 334, loss = 0.22810189\n",
      "Iteration 335, loss = 0.22755856\n",
      "Iteration 336, loss = 0.22701674\n",
      "Iteration 337, loss = 0.22647642\n",
      "Iteration 338, loss = 0.22593759\n",
      "Iteration 339, loss = 0.22540027\n",
      "Iteration 340, loss = 0.22486445\n",
      "Iteration 341, loss = 0.22433013\n",
      "Iteration 342, loss = 0.22379731\n",
      "Iteration 343, loss = 0.22326598\n",
      "Iteration 344, loss = 0.22273616\n",
      "Iteration 345, loss = 0.22220783\n",
      "Iteration 346, loss = 0.22168101\n",
      "Iteration 347, loss = 0.22115568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 348, loss = 0.22063185\n",
      "Iteration 349, loss = 0.22010952\n",
      "Iteration 350, loss = 0.21958869\n",
      "Iteration 351, loss = 0.21906935\n",
      "Iteration 352, loss = 0.21855151\n",
      "Iteration 353, loss = 0.21803517\n",
      "Iteration 354, loss = 0.21752032\n",
      "Iteration 355, loss = 0.21700697\n",
      "Iteration 356, loss = 0.21649512\n",
      "Iteration 357, loss = 0.21598477\n",
      "Iteration 358, loss = 0.21547591\n",
      "Iteration 359, loss = 0.21496854\n",
      "Iteration 360, loss = 0.21446267\n",
      "Iteration 361, loss = 0.21395830\n",
      "Iteration 362, loss = 0.21345542\n",
      "Iteration 363, loss = 0.21295404\n",
      "Iteration 364, loss = 0.21245415\n",
      "Iteration 365, loss = 0.21195575\n",
      "Iteration 366, loss = 0.21145885\n",
      "Iteration 367, loss = 0.21096344\n",
      "Iteration 368, loss = 0.21046952\n",
      "Iteration 369, loss = 0.20997710\n",
      "Iteration 370, loss = 0.20948617\n",
      "Iteration 371, loss = 0.20899673\n",
      "Iteration 372, loss = 0.20850878\n",
      "Iteration 373, loss = 0.20802233\n",
      "Iteration 374, loss = 0.20753736\n",
      "Iteration 375, loss = 0.20705388\n",
      "Iteration 376, loss = 0.20657190\n",
      "Iteration 377, loss = 0.20609140\n",
      "Iteration 378, loss = 0.20561239\n",
      "Iteration 379, loss = 0.20513486\n",
      "Iteration 380, loss = 0.20465883\n",
      "Iteration 381, loss = 0.20418428\n",
      "Iteration 382, loss = 0.20371122\n",
      "Iteration 383, loss = 0.20323964\n",
      "Iteration 384, loss = 0.20276954\n",
      "Iteration 385, loss = 0.20230093\n",
      "Iteration 386, loss = 0.20183381\n",
      "Iteration 387, loss = 0.20136816\n",
      "Iteration 388, loss = 0.20090400\n",
      "Iteration 389, loss = 0.20044131\n",
      "Iteration 390, loss = 0.19998011\n",
      "Iteration 391, loss = 0.19952038\n",
      "Iteration 392, loss = 0.19906213\n",
      "Iteration 393, loss = 0.19860536\n",
      "Iteration 394, loss = 0.19815006\n",
      "Iteration 395, loss = 0.19769624\n",
      "Iteration 396, loss = 0.19724389\n",
      "Iteration 397, loss = 0.19679302\n",
      "Iteration 398, loss = 0.19634361\n",
      "Iteration 399, loss = 0.19589568\n",
      "Iteration 400, loss = 0.19544921\n",
      "Iteration 401, loss = 0.19500421\n",
      "Iteration 402, loss = 0.19456068\n",
      "Iteration 403, loss = 0.19411861\n",
      "Iteration 404, loss = 0.19367801\n",
      "Iteration 405, loss = 0.19323887\n",
      "Iteration 406, loss = 0.19280119\n",
      "Iteration 407, loss = 0.19236497\n",
      "Iteration 408, loss = 0.19193020\n",
      "Iteration 409, loss = 0.19149689\n",
      "Iteration 410, loss = 0.19106504\n",
      "Iteration 411, loss = 0.19063464\n",
      "Iteration 412, loss = 0.19020570\n",
      "Iteration 413, loss = 0.18977820\n",
      "Iteration 414, loss = 0.18935215\n",
      "Iteration 415, loss = 0.18892755\n",
      "Iteration 416, loss = 0.18850439\n",
      "Iteration 417, loss = 0.18808268\n",
      "Iteration 418, loss = 0.18766240\n",
      "Iteration 419, loss = 0.18724357\n",
      "Iteration 420, loss = 0.18682617\n",
      "Iteration 421, loss = 0.18641021\n",
      "Iteration 422, loss = 0.18599568\n",
      "Iteration 423, loss = 0.18558259\n",
      "Iteration 424, loss = 0.18517092\n",
      "Iteration 425, loss = 0.18476068\n",
      "Iteration 426, loss = 0.18435187\n",
      "Iteration 427, loss = 0.18394448\n",
      "Iteration 428, loss = 0.18353851\n",
      "Iteration 429, loss = 0.18313396\n",
      "Iteration 430, loss = 0.18273083\n",
      "Iteration 431, loss = 0.18232911\n",
      "Iteration 432, loss = 0.18192881\n",
      "Iteration 433, loss = 0.18152991\n",
      "Iteration 434, loss = 0.18113242\n",
      "Iteration 435, loss = 0.18073633\n",
      "Iteration 436, loss = 0.18034165\n",
      "Iteration 437, loss = 0.17994837\n",
      "Iteration 438, loss = 0.17955649\n",
      "Iteration 439, loss = 0.17916600\n",
      "Iteration 440, loss = 0.17877690\n",
      "Iteration 441, loss = 0.17838919\n",
      "Iteration 442, loss = 0.17800288\n",
      "Iteration 443, loss = 0.17761794\n",
      "Iteration 444, loss = 0.17723439\n",
      "Iteration 445, loss = 0.17685221\n",
      "Iteration 446, loss = 0.17647142\n",
      "Iteration 447, loss = 0.17609199\n",
      "Iteration 448, loss = 0.17571394\n",
      "Iteration 449, loss = 0.17533726\n",
      "Iteration 450, loss = 0.17496194\n",
      "Iteration 451, loss = 0.17458798\n",
      "Iteration 452, loss = 0.17421539\n",
      "Iteration 453, loss = 0.17384415\n",
      "Iteration 454, loss = 0.17347426\n",
      "Iteration 455, loss = 0.17310572\n",
      "Iteration 456, loss = 0.17273853\n",
      "Iteration 457, loss = 0.17237269\n",
      "Iteration 458, loss = 0.17200819\n",
      "Iteration 459, loss = 0.17164502\n",
      "Iteration 460, loss = 0.17128319\n",
      "Iteration 461, loss = 0.17092269\n",
      "Iteration 462, loss = 0.17056352\n",
      "Iteration 463, loss = 0.17020568\n",
      "Iteration 464, loss = 0.16984916\n",
      "Iteration 465, loss = 0.16949396\n",
      "Iteration 466, loss = 0.16914007\n",
      "Iteration 467, loss = 0.16878750\n",
      "Iteration 468, loss = 0.16843623\n",
      "Iteration 469, loss = 0.16808627\n",
      "Iteration 470, loss = 0.16773762\n",
      "Iteration 471, loss = 0.16739026\n",
      "Iteration 472, loss = 0.16704420\n",
      "Iteration 473, loss = 0.16669943\n",
      "Iteration 474, loss = 0.16635595\n",
      "Iteration 475, loss = 0.16601376\n",
      "Iteration 476, loss = 0.16567284\n",
      "Iteration 477, loss = 0.16533321\n",
      "Iteration 478, loss = 0.16499485\n",
      "Iteration 479, loss = 0.16465776\n",
      "Iteration 480, loss = 0.16432194\n",
      "Iteration 481, loss = 0.16398738\n",
      "Iteration 482, loss = 0.16365409\n",
      "Iteration 483, loss = 0.16332205\n",
      "Iteration 484, loss = 0.16299126\n",
      "Iteration 485, loss = 0.16266173\n",
      "Iteration 486, loss = 0.16233344\n",
      "Iteration 487, loss = 0.16200639\n",
      "Iteration 488, loss = 0.16168058\n",
      "Iteration 489, loss = 0.16135601\n",
      "Iteration 490, loss = 0.16103266\n",
      "Iteration 491, loss = 0.16071055\n",
      "Iteration 492, loss = 0.16038966\n",
      "Iteration 493, loss = 0.16006999\n",
      "Iteration 494, loss = 0.15975153\n",
      "Iteration 495, loss = 0.15943429\n",
      "Iteration 496, loss = 0.15911825\n",
      "Iteration 497, loss = 0.15880342\n",
      "Iteration 498, loss = 0.15848979\n",
      "Iteration 499, loss = 0.15817736\n",
      "Iteration 500, loss = 0.15786612\n",
      "Iteration 1, loss = 1.43038905\n",
      "Iteration 2, loss = 1.41443436\n",
      "Iteration 3, loss = 1.39212017\n",
      "Iteration 4, loss = 1.36452147\n",
      "Iteration 5, loss = 1.33267472\n",
      "Iteration 6, loss = 1.29755786\n",
      "Iteration 7, loss = 1.26007603\n",
      "Iteration 8, loss = 1.22105201\n",
      "Iteration 9, loss = 1.18122047\n",
      "Iteration 10, loss = 1.14122531\n",
      "Iteration 11, loss = 1.10161935\n",
      "Iteration 12, loss = 1.06286609\n",
      "Iteration 13, loss = 1.02534301\n",
      "Iteration 14, loss = 0.98934618\n",
      "Iteration 15, loss = 0.95509607\n",
      "Iteration 16, loss = 0.92274424\n",
      "Iteration 17, loss = 0.89238072\n",
      "Iteration 18, loss = 0.86404204\n",
      "Iteration 19, loss = 0.83771936\n",
      "Iteration 20, loss = 0.81336671\n",
      "Iteration 21, loss = 0.79090891\n",
      "Iteration 22, loss = 0.77024893\n",
      "Iteration 23, loss = 0.75127465\n",
      "Iteration 24, loss = 0.73386467\n",
      "Iteration 25, loss = 0.71789327\n",
      "Iteration 26, loss = 0.70323446\n",
      "Iteration 27, loss = 0.68976516\n",
      "Iteration 28, loss = 0.67736754\n",
      "Iteration 29, loss = 0.66593070\n",
      "Iteration 30, loss = 0.65535175\n",
      "Iteration 31, loss = 0.64553639\n",
      "Iteration 32, loss = 0.63639913\n",
      "Iteration 33, loss = 0.62786315\n",
      "Iteration 34, loss = 0.61986005\n",
      "Iteration 35, loss = 0.61232934\n",
      "Iteration 36, loss = 0.60521789\n",
      "Iteration 37, loss = 0.59847932\n",
      "Iteration 38, loss = 0.59207334\n",
      "Iteration 39, loss = 0.58596513\n",
      "Iteration 40, loss = 0.58012473\n",
      "Iteration 41, loss = 0.57452640\n",
      "Iteration 42, loss = 0.56914812\n",
      "Iteration 43, loss = 0.56397106\n",
      "Iteration 44, loss = 0.55897913\n",
      "Iteration 45, loss = 0.55415857\n",
      "Iteration 46, loss = 0.54949755\n",
      "Iteration 47, loss = 0.54498592\n",
      "Iteration 48, loss = 0.54061487\n",
      "Iteration 49, loss = 0.53637669\n",
      "Iteration 50, loss = 0.53226460\n",
      "Iteration 51, loss = 0.52827256\n",
      "Iteration 52, loss = 0.52439513\n",
      "Iteration 53, loss = 0.52062732\n",
      "Iteration 54, loss = 0.51696455\n",
      "Iteration 55, loss = 0.51340252\n",
      "Iteration 56, loss = 0.50993719\n",
      "Iteration 57, loss = 0.50656471\n",
      "Iteration 58, loss = 0.50328138\n",
      "Iteration 59, loss = 0.50008366\n",
      "Iteration 60, loss = 0.49696813\n",
      "Iteration 61, loss = 0.49393145\n",
      "Iteration 62, loss = 0.49097041\n",
      "Iteration 63, loss = 0.48808191\n",
      "Iteration 64, loss = 0.48526292\n",
      "Iteration 65, loss = 0.48251053\n",
      "Iteration 66, loss = 0.47982195\n",
      "Iteration 67, loss = 0.47719446\n",
      "Iteration 68, loss = 0.47462548\n",
      "Iteration 69, loss = 0.47211253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.46965323\n",
      "Iteration 71, loss = 0.46724532\n",
      "Iteration 72, loss = 0.46488666\n",
      "Iteration 73, loss = 0.46257521\n",
      "Iteration 74, loss = 0.46030904\n",
      "Iteration 75, loss = 0.45808630\n",
      "Iteration 76, loss = 0.45590529\n",
      "Iteration 77, loss = 0.45376435\n",
      "Iteration 78, loss = 0.45166196\n",
      "Iteration 79, loss = 0.44959666\n",
      "Iteration 80, loss = 0.44756706\n",
      "Iteration 81, loss = 0.44557189\n",
      "Iteration 82, loss = 0.44360991\n",
      "Iteration 83, loss = 0.44167996\n",
      "Iteration 84, loss = 0.43978095\n",
      "Iteration 85, loss = 0.43791185\n",
      "Iteration 86, loss = 0.43607166\n",
      "Iteration 87, loss = 0.43425945\n",
      "Iteration 88, loss = 0.43247433\n",
      "Iteration 89, loss = 0.43071545\n",
      "Iteration 90, loss = 0.42898200\n",
      "Iteration 91, loss = 0.42727322\n",
      "Iteration 92, loss = 0.42558836\n",
      "Iteration 93, loss = 0.42392671\n",
      "Iteration 94, loss = 0.42228760\n",
      "Iteration 95, loss = 0.42067037\n",
      "Iteration 96, loss = 0.41907440\n",
      "Iteration 97, loss = 0.41749909\n",
      "Iteration 98, loss = 0.41594387\n",
      "Iteration 99, loss = 0.41440817\n",
      "Iteration 100, loss = 0.41289147\n",
      "Iteration 101, loss = 0.41139324\n",
      "Iteration 102, loss = 0.40991299\n",
      "Iteration 103, loss = 0.40845025\n",
      "Iteration 104, loss = 0.40700456\n",
      "Iteration 105, loss = 0.40557546\n",
      "Iteration 106, loss = 0.40416255\n",
      "Iteration 107, loss = 0.40276539\n",
      "Iteration 108, loss = 0.40138360\n",
      "Iteration 109, loss = 0.40001679\n",
      "Iteration 110, loss = 0.39866459\n",
      "Iteration 111, loss = 0.39732665\n",
      "Iteration 112, loss = 0.39600261\n",
      "Iteration 113, loss = 0.39469216\n",
      "Iteration 114, loss = 0.39339496\n",
      "Iteration 115, loss = 0.39211071\n",
      "Iteration 116, loss = 0.39083910\n",
      "Iteration 117, loss = 0.38957986\n",
      "Iteration 118, loss = 0.38833268\n",
      "Iteration 119, loss = 0.38709731\n",
      "Iteration 120, loss = 0.38587349\n",
      "Iteration 121, loss = 0.38466095\n",
      "Iteration 122, loss = 0.38345945\n",
      "Iteration 123, loss = 0.38226875\n",
      "Iteration 124, loss = 0.38108863\n",
      "Iteration 125, loss = 0.37991884\n",
      "Iteration 126, loss = 0.37875919\n",
      "Iteration 127, loss = 0.37760945\n",
      "Iteration 128, loss = 0.37646942\n",
      "Iteration 129, loss = 0.37533891\n",
      "Iteration 130, loss = 0.37421771\n",
      "Iteration 131, loss = 0.37310564\n",
      "Iteration 132, loss = 0.37200252\n",
      "Iteration 133, loss = 0.37090817\n",
      "Iteration 134, loss = 0.36982242\n",
      "Iteration 135, loss = 0.36874509\n",
      "Iteration 136, loss = 0.36767602\n",
      "Iteration 137, loss = 0.36661507\n",
      "Iteration 138, loss = 0.36556206\n",
      "Iteration 139, loss = 0.36451685\n",
      "Iteration 140, loss = 0.36347930\n",
      "Iteration 141, loss = 0.36244926\n",
      "Iteration 142, loss = 0.36142658\n",
      "Iteration 143, loss = 0.36041115\n",
      "Iteration 144, loss = 0.35940282\n",
      "Iteration 145, loss = 0.35840146\n",
      "Iteration 146, loss = 0.35740695\n",
      "Iteration 147, loss = 0.35641917\n",
      "Iteration 148, loss = 0.35543800\n",
      "Iteration 149, loss = 0.35446332\n",
      "Iteration 150, loss = 0.35349502\n",
      "Iteration 151, loss = 0.35253299\n",
      "Iteration 152, loss = 0.35157712\n",
      "Iteration 153, loss = 0.35062730\n",
      "Iteration 154, loss = 0.34968344\n",
      "Iteration 155, loss = 0.34874544\n",
      "Iteration 156, loss = 0.34781319\n",
      "Iteration 157, loss = 0.34688660\n",
      "Iteration 158, loss = 0.34596558\n",
      "Iteration 159, loss = 0.34505003\n",
      "Iteration 160, loss = 0.34413988\n",
      "Iteration 161, loss = 0.34323502\n",
      "Iteration 162, loss = 0.34233539\n",
      "Iteration 163, loss = 0.34144089\n",
      "Iteration 164, loss = 0.34055144\n",
      "Iteration 165, loss = 0.33966698\n",
      "Iteration 166, loss = 0.33878740\n",
      "Iteration 167, loss = 0.33791266\n",
      "Iteration 168, loss = 0.33704266\n",
      "Iteration 169, loss = 0.33617734\n",
      "Iteration 170, loss = 0.33531663\n",
      "Iteration 171, loss = 0.33446046\n",
      "Iteration 172, loss = 0.33360877\n",
      "Iteration 173, loss = 0.33276148\n",
      "Iteration 174, loss = 0.33191853\n",
      "Iteration 175, loss = 0.33107986\n",
      "Iteration 176, loss = 0.33024541\n",
      "Iteration 177, loss = 0.32941512\n",
      "Iteration 178, loss = 0.32858893\n",
      "Iteration 179, loss = 0.32776679\n",
      "Iteration 180, loss = 0.32694863\n",
      "Iteration 181, loss = 0.32613440\n",
      "Iteration 182, loss = 0.32532405\n",
      "Iteration 183, loss = 0.32451752\n",
      "Iteration 184, loss = 0.32371477\n",
      "Iteration 185, loss = 0.32291575\n",
      "Iteration 186, loss = 0.32212039\n",
      "Iteration 187, loss = 0.32132867\n",
      "Iteration 188, loss = 0.32054052\n",
      "Iteration 189, loss = 0.31975591\n",
      "Iteration 190, loss = 0.31897479\n",
      "Iteration 191, loss = 0.31819710\n",
      "Iteration 192, loss = 0.31742282\n",
      "Iteration 193, loss = 0.31665190\n",
      "Iteration 194, loss = 0.31588429\n",
      "Iteration 195, loss = 0.31511996\n",
      "Iteration 196, loss = 0.31435887\n",
      "Iteration 197, loss = 0.31360097\n",
      "Iteration 198, loss = 0.31284623\n",
      "Iteration 199, loss = 0.31209462\n",
      "Iteration 200, loss = 0.31134609\n",
      "Iteration 201, loss = 0.31060060\n",
      "Iteration 202, loss = 0.30985814\n",
      "Iteration 203, loss = 0.30911865\n",
      "Iteration 204, loss = 0.30838211\n",
      "Iteration 205, loss = 0.30764848\n",
      "Iteration 206, loss = 0.30691773\n",
      "Iteration 207, loss = 0.30618983\n",
      "Iteration 208, loss = 0.30546475\n",
      "Iteration 209, loss = 0.30474246\n",
      "Iteration 210, loss = 0.30402293\n",
      "Iteration 211, loss = 0.30330613\n",
      "Iteration 212, loss = 0.30259202\n",
      "Iteration 213, loss = 0.30188059\n",
      "Iteration 214, loss = 0.30117181\n",
      "Iteration 215, loss = 0.30046564\n",
      "Iteration 216, loss = 0.29976207\n",
      "Iteration 217, loss = 0.29906106\n",
      "Iteration 218, loss = 0.29836260\n",
      "Iteration 219, loss = 0.29766665\n",
      "Iteration 220, loss = 0.29697320\n",
      "Iteration 221, loss = 0.29628221\n",
      "Iteration 222, loss = 0.29559367\n",
      "Iteration 223, loss = 0.29490755\n",
      "Iteration 224, loss = 0.29422384\n",
      "Iteration 225, loss = 0.29354250\n",
      "Iteration 226, loss = 0.29286352\n",
      "Iteration 227, loss = 0.29218688\n",
      "Iteration 228, loss = 0.29151255\n",
      "Iteration 229, loss = 0.29084052\n",
      "Iteration 230, loss = 0.29017077\n",
      "Iteration 231, loss = 0.28950327\n",
      "Iteration 232, loss = 0.28883801\n",
      "Iteration 233, loss = 0.28817498\n",
      "Iteration 234, loss = 0.28751414\n",
      "Iteration 235, loss = 0.28685549\n",
      "Iteration 236, loss = 0.28619900\n",
      "Iteration 237, loss = 0.28554466\n",
      "Iteration 238, loss = 0.28489246\n",
      "Iteration 239, loss = 0.28424237\n",
      "Iteration 240, loss = 0.28359438\n",
      "Iteration 241, loss = 0.28294848\n",
      "Iteration 242, loss = 0.28230465\n",
      "Iteration 243, loss = 0.28166287\n",
      "Iteration 244, loss = 0.28102313\n",
      "Iteration 245, loss = 0.28038541\n",
      "Iteration 246, loss = 0.27974971\n",
      "Iteration 247, loss = 0.27911600\n",
      "Iteration 248, loss = 0.27848428\n",
      "Iteration 249, loss = 0.27785453\n",
      "Iteration 250, loss = 0.27722674\n",
      "Iteration 251, loss = 0.27660089\n",
      "Iteration 252, loss = 0.27597697\n",
      "Iteration 253, loss = 0.27535497\n",
      "Iteration 254, loss = 0.27473488\n",
      "Iteration 255, loss = 0.27411669\n",
      "Iteration 256, loss = 0.27350038\n",
      "Iteration 257, loss = 0.27288595\n",
      "Iteration 258, loss = 0.27227338\n",
      "Iteration 259, loss = 0.27166266\n",
      "Iteration 260, loss = 0.27105378\n",
      "Iteration 261, loss = 0.27044674\n",
      "Iteration 262, loss = 0.26984151\n",
      "Iteration 263, loss = 0.26923810\n",
      "Iteration 264, loss = 0.26863649\n",
      "Iteration 265, loss = 0.26803667\n",
      "Iteration 266, loss = 0.26743864\n",
      "Iteration 267, loss = 0.26684238\n",
      "Iteration 268, loss = 0.26624788\n",
      "Iteration 269, loss = 0.26565514\n",
      "Iteration 270, loss = 0.26506415\n",
      "Iteration 271, loss = 0.26447490\n",
      "Iteration 272, loss = 0.26388738\n",
      "Iteration 273, loss = 0.26330159\n",
      "Iteration 274, loss = 0.26271751\n",
      "Iteration 275, loss = 0.26213514\n",
      "Iteration 276, loss = 0.26155447\n",
      "Iteration 277, loss = 0.26097550\n",
      "Iteration 278, loss = 0.26039821\n",
      "Iteration 279, loss = 0.25982260\n",
      "Iteration 280, loss = 0.25924867\n",
      "Iteration 281, loss = 0.25867640\n",
      "Iteration 282, loss = 0.25810579\n",
      "Iteration 283, loss = 0.25753684\n",
      "Iteration 284, loss = 0.25696954\n",
      "Iteration 285, loss = 0.25640387\n",
      "Iteration 286, loss = 0.25583984\n",
      "Iteration 287, loss = 0.25527745\n",
      "Iteration 288, loss = 0.25471667\n",
      "Iteration 289, loss = 0.25415752\n",
      "Iteration 290, loss = 0.25359997\n",
      "Iteration 291, loss = 0.25304404\n",
      "Iteration 292, loss = 0.25248971\n",
      "Iteration 293, loss = 0.25193698\n",
      "Iteration 294, loss = 0.25138584\n",
      "Iteration 295, loss = 0.25083628\n",
      "Iteration 296, loss = 0.25028832\n",
      "Iteration 297, loss = 0.24974193\n",
      "Iteration 298, loss = 0.24919711\n",
      "Iteration 299, loss = 0.24865386\n",
      "Iteration 300, loss = 0.24811218\n",
      "Iteration 301, loss = 0.24757207\n",
      "Iteration 302, loss = 0.24703350\n",
      "Iteration 303, loss = 0.24649650\n",
      "Iteration 304, loss = 0.24596104\n",
      "Iteration 305, loss = 0.24542712\n",
      "Iteration 306, loss = 0.24489475\n",
      "Iteration 307, loss = 0.24436392\n",
      "Iteration 308, loss = 0.24383462\n",
      "Iteration 309, loss = 0.24330685\n",
      "Iteration 310, loss = 0.24278061\n",
      "Iteration 311, loss = 0.24225589\n",
      "Iteration 312, loss = 0.24173270\n",
      "Iteration 313, loss = 0.24121102\n",
      "Iteration 314, loss = 0.24069085\n",
      "Iteration 315, loss = 0.24017220\n",
      "Iteration 316, loss = 0.23965506\n",
      "Iteration 317, loss = 0.23913942\n",
      "Iteration 318, loss = 0.23862529\n",
      "Iteration 319, loss = 0.23811265\n",
      "Iteration 320, loss = 0.23760151\n",
      "Iteration 321, loss = 0.23709187\n",
      "Iteration 322, loss = 0.23658371\n",
      "Iteration 323, loss = 0.23607705\n",
      "Iteration 324, loss = 0.23557187\n",
      "Iteration 325, loss = 0.23506818\n",
      "Iteration 326, loss = 0.23456597\n",
      "Iteration 327, loss = 0.23406524\n",
      "Iteration 328, loss = 0.23356598\n",
      "Iteration 329, loss = 0.23306820\n",
      "Iteration 330, loss = 0.23257190\n",
      "Iteration 331, loss = 0.23207706\n",
      "Iteration 332, loss = 0.23158369\n",
      "Iteration 333, loss = 0.23109178\n",
      "Iteration 334, loss = 0.23060134\n",
      "Iteration 335, loss = 0.23011237\n",
      "Iteration 336, loss = 0.22962485\n",
      "Iteration 337, loss = 0.22913879\n",
      "Iteration 338, loss = 0.22865418\n",
      "Iteration 339, loss = 0.22817103\n",
      "Iteration 340, loss = 0.22768933\n",
      "Iteration 341, loss = 0.22720908\n",
      "Iteration 342, loss = 0.22673028\n",
      "Iteration 343, loss = 0.22625292\n",
      "Iteration 344, loss = 0.22577701\n",
      "Iteration 345, loss = 0.22530254\n",
      "Iteration 346, loss = 0.22482952\n",
      "Iteration 347, loss = 0.22435793\n",
      "Iteration 348, loss = 0.22388778\n",
      "Iteration 349, loss = 0.22341906\n",
      "Iteration 350, loss = 0.22295178\n",
      "Iteration 351, loss = 0.22248593\n",
      "Iteration 352, loss = 0.22202151\n",
      "Iteration 353, loss = 0.22155853\n",
      "Iteration 354, loss = 0.22109696\n",
      "Iteration 355, loss = 0.22063683\n",
      "Iteration 356, loss = 0.22017812\n",
      "Iteration 357, loss = 0.21972083\n",
      "Iteration 358, loss = 0.21926496\n",
      "Iteration 359, loss = 0.21881051\n",
      "Iteration 360, loss = 0.21835748\n",
      "Iteration 361, loss = 0.21790587\n",
      "Iteration 362, loss = 0.21745567\n",
      "Iteration 363, loss = 0.21700689\n",
      "Iteration 364, loss = 0.21655951\n",
      "Iteration 365, loss = 0.21611355\n",
      "Iteration 366, loss = 0.21566900\n",
      "Iteration 367, loss = 0.21522585\n",
      "Iteration 368, loss = 0.21478411\n",
      "Iteration 369, loss = 0.21434377\n",
      "Iteration 370, loss = 0.21390483\n",
      "Iteration 371, loss = 0.21346730\n",
      "Iteration 372, loss = 0.21303116\n",
      "Iteration 373, loss = 0.21259642\n",
      "Iteration 374, loss = 0.21216308\n",
      "Iteration 375, loss = 0.21173114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 376, loss = 0.21130058\n",
      "Iteration 377, loss = 0.21087142\n",
      "Iteration 378, loss = 0.21044365\n",
      "Iteration 379, loss = 0.21001726\n",
      "Iteration 380, loss = 0.20959227\n",
      "Iteration 381, loss = 0.20916865\n",
      "Iteration 382, loss = 0.20874642\n",
      "Iteration 383, loss = 0.20832558\n",
      "Iteration 384, loss = 0.20790611\n",
      "Iteration 385, loss = 0.20748802\n",
      "Iteration 386, loss = 0.20707131\n",
      "Iteration 387, loss = 0.20665598\n",
      "Iteration 388, loss = 0.20624202\n",
      "Iteration 389, loss = 0.20582943\n",
      "Iteration 390, loss = 0.20541821\n",
      "Iteration 391, loss = 0.20500835\n",
      "Iteration 392, loss = 0.20459987\n",
      "Iteration 393, loss = 0.20419275\n",
      "Iteration 394, loss = 0.20378699\n",
      "Iteration 395, loss = 0.20338260\n",
      "Iteration 396, loss = 0.20297956\n",
      "Iteration 397, loss = 0.20257789\n",
      "Iteration 398, loss = 0.20217757\n",
      "Iteration 399, loss = 0.20177860\n",
      "Iteration 400, loss = 0.20138099\n",
      "Iteration 401, loss = 0.20098472\n",
      "Iteration 402, loss = 0.20058981\n",
      "Iteration 403, loss = 0.20019624\n",
      "Iteration 404, loss = 0.19980401\n",
      "Iteration 405, loss = 0.19941313\n",
      "Iteration 406, loss = 0.19902359\n",
      "Iteration 407, loss = 0.19863539\n",
      "Iteration 408, loss = 0.19824853\n",
      "Iteration 409, loss = 0.19786300\n",
      "Iteration 410, loss = 0.19747880\n",
      "Iteration 411, loss = 0.19709594\n",
      "Iteration 412, loss = 0.19671440\n",
      "Iteration 413, loss = 0.19633419\n",
      "Iteration 414, loss = 0.19595530\n",
      "Iteration 415, loss = 0.19557774\n",
      "Iteration 416, loss = 0.19520149\n",
      "Iteration 417, loss = 0.19482657\n",
      "Iteration 418, loss = 0.19445296\n",
      "Iteration 419, loss = 0.19408066\n",
      "Iteration 420, loss = 0.19370968\n",
      "Iteration 421, loss = 0.19334000\n",
      "Iteration 422, loss = 0.19297164\n",
      "Iteration 423, loss = 0.19260457\n",
      "Iteration 424, loss = 0.19223881\n",
      "Iteration 425, loss = 0.19187435\n",
      "Iteration 426, loss = 0.19151118\n",
      "Iteration 427, loss = 0.19114932\n",
      "Iteration 428, loss = 0.19078874\n",
      "Iteration 429, loss = 0.19042945\n",
      "Iteration 430, loss = 0.19007146\n",
      "Iteration 431, loss = 0.18971474\n",
      "Iteration 432, loss = 0.18935931\n",
      "Iteration 433, loss = 0.18900516\n",
      "Iteration 434, loss = 0.18865229\n",
      "Iteration 435, loss = 0.18830070\n",
      "Iteration 436, loss = 0.18795037\n",
      "Iteration 437, loss = 0.18760132\n",
      "Iteration 438, loss = 0.18725354\n",
      "Iteration 439, loss = 0.18690702\n",
      "Iteration 440, loss = 0.18656176\n",
      "Iteration 441, loss = 0.18621776\n",
      "Iteration 442, loss = 0.18587502\n",
      "Iteration 443, loss = 0.18553354\n",
      "Iteration 444, loss = 0.18519330\n",
      "Iteration 445, loss = 0.18485432\n",
      "Iteration 446, loss = 0.18451658\n",
      "Iteration 447, loss = 0.18418008\n",
      "Iteration 448, loss = 0.18384482\n",
      "Iteration 449, loss = 0.18351081\n",
      "Iteration 450, loss = 0.18317802\n",
      "Iteration 451, loss = 0.18284647\n",
      "Iteration 452, loss = 0.18251615\n",
      "Iteration 453, loss = 0.18218705\n",
      "Iteration 454, loss = 0.18185918\n",
      "Iteration 455, loss = 0.18153253\n",
      "Iteration 456, loss = 0.18120710\n",
      "Iteration 457, loss = 0.18088288\n",
      "Iteration 458, loss = 0.18055987\n",
      "Iteration 459, loss = 0.18023807\n",
      "Iteration 460, loss = 0.17991748\n",
      "Iteration 461, loss = 0.17959808\n",
      "Iteration 462, loss = 0.17927989\n",
      "Iteration 463, loss = 0.17896290\n",
      "Iteration 464, loss = 0.17864709\n",
      "Iteration 465, loss = 0.17833248\n",
      "Iteration 466, loss = 0.17801906\n",
      "Iteration 467, loss = 0.17770682\n",
      "Iteration 468, loss = 0.17739576\n",
      "Iteration 469, loss = 0.17708588\n",
      "Iteration 470, loss = 0.17677717\n",
      "Iteration 471, loss = 0.17646963\n",
      "Iteration 472, loss = 0.17616326\n",
      "Iteration 473, loss = 0.17585806\n",
      "Iteration 474, loss = 0.17555402\n",
      "Iteration 475, loss = 0.17525114\n",
      "Iteration 476, loss = 0.17494941\n",
      "Iteration 477, loss = 0.17464883\n",
      "Iteration 478, loss = 0.17434941\n",
      "Iteration 479, loss = 0.17405112\n",
      "Iteration 480, loss = 0.17375398\n",
      "Iteration 481, loss = 0.17345798\n",
      "Iteration 482, loss = 0.17316312\n",
      "Iteration 483, loss = 0.17286939\n",
      "Iteration 484, loss = 0.17257678\n",
      "Iteration 485, loss = 0.17228530\n",
      "Iteration 486, loss = 0.17199495\n",
      "Iteration 487, loss = 0.17170571\n",
      "Iteration 488, loss = 0.17141758\n",
      "Iteration 489, loss = 0.17113057\n",
      "Iteration 490, loss = 0.17084467\n",
      "Iteration 491, loss = 0.17055987\n",
      "Iteration 492, loss = 0.17027618\n",
      "Iteration 493, loss = 0.16999358\n",
      "Iteration 494, loss = 0.16971208\n",
      "Iteration 495, loss = 0.16943166\n",
      "Iteration 496, loss = 0.16915234\n",
      "Iteration 497, loss = 0.16887410\n",
      "Iteration 498, loss = 0.16859694\n",
      "Iteration 499, loss = 0.16832086\n",
      "Iteration 500, loss = 0.16804585\n",
      "Iteration 1, loss = 1.43071704\n",
      "Iteration 2, loss = 1.41471126\n",
      "Iteration 3, loss = 1.39232905\n",
      "Iteration 4, loss = 1.36465157\n",
      "Iteration 5, loss = 1.33272124\n",
      "Iteration 6, loss = 1.29752122\n",
      "Iteration 7, loss = 1.25996095\n",
      "Iteration 8, loss = 1.22086638\n",
      "Iteration 9, loss = 1.18097417\n",
      "Iteration 10, loss = 1.14092911\n",
      "Iteration 11, loss = 1.10128397\n",
      "Iteration 12, loss = 1.06250143\n",
      "Iteration 13, loss = 1.02495764\n",
      "Iteration 14, loss = 0.98894709\n",
      "Iteration 15, loss = 0.95468857\n",
      "Iteration 16, loss = 0.92233205\n",
      "Iteration 17, loss = 0.89196623\n",
      "Iteration 18, loss = 0.86362653\n",
      "Iteration 19, loss = 0.83730333\n",
      "Iteration 20, loss = 0.81295014\n",
      "Iteration 21, loss = 0.79049150\n",
      "Iteration 22, loss = 0.76983031\n",
      "Iteration 23, loss = 0.75085451\n",
      "Iteration 24, loss = 0.73344285\n",
      "Iteration 25, loss = 0.71746986\n",
      "Iteration 26, loss = 0.70280981\n",
      "Iteration 27, loss = 0.68933990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.67694259\n",
      "Iteration 29, loss = 0.66550725\n",
      "Iteration 30, loss = 0.65493125\n",
      "Iteration 31, loss = 0.64512053\n",
      "Iteration 32, loss = 0.63598980\n",
      "Iteration 33, loss = 0.62746247\n",
      "Iteration 34, loss = 0.61947028\n",
      "Iteration 35, loss = 0.61195290\n",
      "Iteration 36, loss = 0.60485731\n",
      "Iteration 37, loss = 0.59813724\n",
      "Iteration 38, loss = 0.59175245\n",
      "Iteration 39, loss = 0.58566818\n",
      "Iteration 40, loss = 0.57985445\n",
      "Iteration 41, loss = 0.57428551\n",
      "Iteration 42, loss = 0.56893930\n",
      "Iteration 43, loss = 0.56379689\n",
      "Iteration 44, loss = 0.55884210\n",
      "Iteration 45, loss = 0.55406102\n",
      "Iteration 46, loss = 0.54944168\n",
      "Iteration 47, loss = 0.54497375\n",
      "Iteration 48, loss = 0.54064821\n",
      "Iteration 49, loss = 0.53645715\n",
      "Iteration 50, loss = 0.53239356\n",
      "Iteration 51, loss = 0.52845119\n",
      "Iteration 52, loss = 0.52462433\n",
      "Iteration 53, loss = 0.52090778\n",
      "Iteration 54, loss = 0.51729671\n",
      "Iteration 55, loss = 0.51378661\n",
      "Iteration 56, loss = 0.51037321\n",
      "Iteration 57, loss = 0.50705245\n",
      "Iteration 58, loss = 0.50382045\n",
      "Iteration 59, loss = 0.50067348\n",
      "Iteration 60, loss = 0.49760796\n",
      "Iteration 61, loss = 0.49462043\n",
      "Iteration 62, loss = 0.49170754\n",
      "Iteration 63, loss = 0.48886609\n",
      "Iteration 64, loss = 0.48609298\n",
      "Iteration 65, loss = 0.48338523\n",
      "Iteration 66, loss = 0.48073999\n",
      "Iteration 67, loss = 0.47815452\n",
      "Iteration 68, loss = 0.47562623\n",
      "Iteration 69, loss = 0.47315262\n",
      "Iteration 70, loss = 0.47073133\n",
      "Iteration 71, loss = 0.46836013\n",
      "Iteration 72, loss = 0.46603688\n",
      "Iteration 73, loss = 0.46375959\n",
      "Iteration 74, loss = 0.46152637\n",
      "Iteration 75, loss = 0.45933541\n",
      "Iteration 76, loss = 0.45718505\n",
      "Iteration 77, loss = 0.45507368\n",
      "Iteration 78, loss = 0.45299982\n",
      "Iteration 79, loss = 0.45096206\n",
      "Iteration 80, loss = 0.44895905\n",
      "Iteration 81, loss = 0.44698956\n",
      "Iteration 82, loss = 0.44505238\n",
      "Iteration 83, loss = 0.44314640\n",
      "Iteration 84, loss = 0.44127054\n",
      "Iteration 85, loss = 0.43942381\n",
      "Iteration 86, loss = 0.43760525\n",
      "Iteration 87, loss = 0.43581393\n",
      "Iteration 88, loss = 0.43404898\n",
      "Iteration 89, loss = 0.43230959\n",
      "Iteration 90, loss = 0.43059494\n",
      "Iteration 91, loss = 0.42890428\n",
      "Iteration 92, loss = 0.42723689\n",
      "Iteration 93, loss = 0.42559206\n",
      "Iteration 94, loss = 0.42396913\n",
      "Iteration 95, loss = 0.42236745\n",
      "Iteration 96, loss = 0.42078639\n",
      "Iteration 97, loss = 0.41922538\n",
      "Iteration 98, loss = 0.41768383\n",
      "Iteration 99, loss = 0.41616120\n",
      "Iteration 100, loss = 0.41465696\n",
      "Iteration 101, loss = 0.41317059\n",
      "Iteration 102, loss = 0.41170162\n",
      "Iteration 103, loss = 0.41024956\n",
      "Iteration 104, loss = 0.40881396\n",
      "Iteration 105, loss = 0.40739440\n",
      "Iteration 106, loss = 0.40599044\n",
      "Iteration 107, loss = 0.40460168\n",
      "Iteration 108, loss = 0.40322774\n",
      "Iteration 109, loss = 0.40186823\n",
      "Iteration 110, loss = 0.40052280\n",
      "Iteration 111, loss = 0.39919109\n",
      "Iteration 112, loss = 0.39787277\n",
      "Iteration 113, loss = 0.39656752\n",
      "Iteration 114, loss = 0.39527503\n",
      "Iteration 115, loss = 0.39399498\n",
      "Iteration 116, loss = 0.39272710\n",
      "Iteration 117, loss = 0.39147109\n",
      "Iteration 118, loss = 0.39022669\n",
      "Iteration 119, loss = 0.38899364\n",
      "Iteration 120, loss = 0.38777167\n",
      "Iteration 121, loss = 0.38656054\n",
      "Iteration 122, loss = 0.38536002\n",
      "Iteration 123, loss = 0.38416987\n",
      "Iteration 124, loss = 0.38298987\n",
      "Iteration 125, loss = 0.38181980\n",
      "Iteration 126, loss = 0.38065945\n",
      "Iteration 127, loss = 0.37950862\n",
      "Iteration 128, loss = 0.37836710\n",
      "Iteration 129, loss = 0.37723472\n",
      "Iteration 130, loss = 0.37611127\n",
      "Iteration 131, loss = 0.37499658\n",
      "Iteration 132, loss = 0.37389048\n",
      "Iteration 133, loss = 0.37279278\n",
      "Iteration 134, loss = 0.37170333\n",
      "Iteration 135, loss = 0.37062197\n",
      "Iteration 136, loss = 0.36954853\n",
      "Iteration 137, loss = 0.36848286\n",
      "Iteration 138, loss = 0.36742481\n",
      "Iteration 139, loss = 0.36637425\n",
      "Iteration 140, loss = 0.36533102\n",
      "Iteration 141, loss = 0.36429499\n",
      "Iteration 142, loss = 0.36326603\n",
      "Iteration 143, loss = 0.36224401\n",
      "Iteration 144, loss = 0.36122879\n",
      "Iteration 145, loss = 0.36022027\n",
      "Iteration 146, loss = 0.35921831\n",
      "Iteration 147, loss = 0.35822281\n",
      "Iteration 148, loss = 0.35723364\n",
      "Iteration 149, loss = 0.35625069\n",
      "Iteration 150, loss = 0.35527386\n",
      "Iteration 151, loss = 0.35430305\n",
      "Iteration 152, loss = 0.35333814\n",
      "Iteration 153, loss = 0.35237904\n",
      "Iteration 154, loss = 0.35142565\n",
      "Iteration 155, loss = 0.35047788\n",
      "Iteration 156, loss = 0.34953562\n",
      "Iteration 157, loss = 0.34859880\n",
      "Iteration 158, loss = 0.34766733\n",
      "Iteration 159, loss = 0.34674110\n",
      "Iteration 160, loss = 0.34582005\n",
      "Iteration 161, loss = 0.34490409\n",
      "Iteration 162, loss = 0.34399314\n",
      "Iteration 163, loss = 0.34308712\n",
      "Iteration 164, loss = 0.34218595\n",
      "Iteration 165, loss = 0.34128956\n",
      "Iteration 166, loss = 0.34039788\n",
      "Iteration 167, loss = 0.33951083\n",
      "Iteration 168, loss = 0.33862835\n",
      "Iteration 169, loss = 0.33775036\n",
      "Iteration 170, loss = 0.33687680\n",
      "Iteration 171, loss = 0.33600761\n",
      "Iteration 172, loss = 0.33514272\n",
      "Iteration 173, loss = 0.33428207\n",
      "Iteration 174, loss = 0.33342560\n",
      "Iteration 175, loss = 0.33257324\n",
      "Iteration 176, loss = 0.33172495\n",
      "Iteration 177, loss = 0.33088066\n",
      "Iteration 178, loss = 0.33004032\n",
      "Iteration 179, loss = 0.32920388\n",
      "Iteration 180, loss = 0.32837128\n",
      "Iteration 181, loss = 0.32754247\n",
      "Iteration 182, loss = 0.32671740\n",
      "Iteration 183, loss = 0.32589602\n",
      "Iteration 184, loss = 0.32507828\n",
      "Iteration 185, loss = 0.32426413\n",
      "Iteration 186, loss = 0.32345354\n",
      "Iteration 187, loss = 0.32264644\n",
      "Iteration 188, loss = 0.32184281\n",
      "Iteration 189, loss = 0.32104259\n",
      "Iteration 190, loss = 0.32024574\n",
      "Iteration 191, loss = 0.31945222\n",
      "Iteration 192, loss = 0.31866200\n",
      "Iteration 193, loss = 0.31787502\n",
      "Iteration 194, loss = 0.31709126\n",
      "Iteration 195, loss = 0.31631067\n",
      "Iteration 196, loss = 0.31553322\n",
      "Iteration 197, loss = 0.31475887\n",
      "Iteration 198, loss = 0.31398758\n",
      "Iteration 199, loss = 0.31321933\n",
      "Iteration 200, loss = 0.31245407\n",
      "Iteration 201, loss = 0.31169177\n",
      "Iteration 202, loss = 0.31093240\n",
      "Iteration 203, loss = 0.31017593\n",
      "Iteration 204, loss = 0.30942233\n",
      "Iteration 205, loss = 0.30867156\n",
      "Iteration 206, loss = 0.30792360\n",
      "Iteration 207, loss = 0.30717841\n",
      "Iteration 208, loss = 0.30643598\n",
      "Iteration 209, loss = 0.30569626\n",
      "Iteration 210, loss = 0.30495923\n",
      "Iteration 211, loss = 0.30422487\n",
      "Iteration 212, loss = 0.30349315\n",
      "Iteration 213, loss = 0.30276405\n",
      "Iteration 214, loss = 0.30203753\n",
      "Iteration 215, loss = 0.30131357\n",
      "Iteration 216, loss = 0.30059215\n",
      "Iteration 217, loss = 0.29987325\n",
      "Iteration 218, loss = 0.29915684\n",
      "Iteration 219, loss = 0.29844290\n",
      "Iteration 220, loss = 0.29773140\n",
      "Iteration 221, loss = 0.29702233\n",
      "Iteration 222, loss = 0.29631567\n",
      "Iteration 223, loss = 0.29561139\n",
      "Iteration 224, loss = 0.29490947\n",
      "Iteration 225, loss = 0.29420990\n",
      "Iteration 226, loss = 0.29351264\n",
      "Iteration 227, loss = 0.29281770\n",
      "Iteration 228, loss = 0.29212503\n",
      "Iteration 229, loss = 0.29143464\n",
      "Iteration 230, loss = 0.29074649\n",
      "Iteration 231, loss = 0.29006058\n",
      "Iteration 232, loss = 0.28937688\n",
      "Iteration 233, loss = 0.28869538\n",
      "Iteration 234, loss = 0.28801606\n",
      "Iteration 235, loss = 0.28733891\n",
      "Iteration 236, loss = 0.28666390\n",
      "Iteration 237, loss = 0.28599103\n",
      "Iteration 238, loss = 0.28532028\n",
      "Iteration 239, loss = 0.28465164\n",
      "Iteration 240, loss = 0.28398508\n",
      "Iteration 241, loss = 0.28332060\n",
      "Iteration 242, loss = 0.28265819\n",
      "Iteration 243, loss = 0.28199782\n",
      "Iteration 244, loss = 0.28133948\n",
      "Iteration 245, loss = 0.28068317\n",
      "Iteration 246, loss = 0.28002887\n",
      "Iteration 247, loss = 0.27937657\n",
      "Iteration 248, loss = 0.27872626\n",
      "Iteration 249, loss = 0.27807791\n",
      "Iteration 250, loss = 0.27743153\n",
      "Iteration 251, loss = 0.27678711\n",
      "Iteration 252, loss = 0.27614462\n",
      "Iteration 253, loss = 0.27550406\n",
      "Iteration 254, loss = 0.27486542\n",
      "Iteration 255, loss = 0.27422869\n",
      "Iteration 256, loss = 0.27359386\n",
      "Iteration 257, loss = 0.27296091\n",
      "Iteration 258, loss = 0.27232985\n",
      "Iteration 259, loss = 0.27170065\n",
      "Iteration 260, loss = 0.27107332\n",
      "Iteration 261, loss = 0.27044784\n",
      "Iteration 262, loss = 0.26982420\n",
      "Iteration 263, loss = 0.26920239\n",
      "Iteration 264, loss = 0.26858241\n",
      "Iteration 265, loss = 0.26796425\n",
      "Iteration 266, loss = 0.26734790\n",
      "Iteration 267, loss = 0.26673335\n",
      "Iteration 268, loss = 0.26612060\n",
      "Iteration 269, loss = 0.26550963\n",
      "Iteration 270, loss = 0.26490044\n",
      "Iteration 271, loss = 0.26429303\n",
      "Iteration 272, loss = 0.26368738\n",
      "Iteration 273, loss = 0.26308349\n",
      "Iteration 274, loss = 0.26248135\n",
      "Iteration 275, loss = 0.26188096\n",
      "Iteration 276, loss = 0.26128231\n",
      "Iteration 277, loss = 0.26068539\n",
      "Iteration 278, loss = 0.26009020\n",
      "Iteration 279, loss = 0.25949674\n",
      "Iteration 280, loss = 0.25890499\n",
      "Iteration 281, loss = 0.25831495\n",
      "Iteration 282, loss = 0.25772661\n",
      "Iteration 283, loss = 0.25713998\n",
      "Iteration 284, loss = 0.25655504\n",
      "Iteration 285, loss = 0.25597179\n",
      "Iteration 286, loss = 0.25539023\n",
      "Iteration 287, loss = 0.25481034\n",
      "Iteration 288, loss = 0.25423214\n",
      "Iteration 289, loss = 0.25365560\n",
      "Iteration 290, loss = 0.25308073\n",
      "Iteration 291, loss = 0.25250752\n",
      "Iteration 292, loss = 0.25193597\n",
      "Iteration 293, loss = 0.25136608\n",
      "Iteration 294, loss = 0.25079783\n",
      "Iteration 295, loss = 0.25023123\n",
      "Iteration 296, loss = 0.24966628\n",
      "Iteration 297, loss = 0.24910296\n",
      "Iteration 298, loss = 0.24854128\n",
      "Iteration 299, loss = 0.24798122\n",
      "Iteration 300, loss = 0.24742280\n",
      "Iteration 301, loss = 0.24686600\n",
      "Iteration 302, loss = 0.24631082\n",
      "Iteration 303, loss = 0.24575727\n",
      "Iteration 304, loss = 0.24520532\n",
      "Iteration 305, loss = 0.24465499\n",
      "Iteration 306, loss = 0.24410627\n",
      "Iteration 307, loss = 0.24355915\n",
      "Iteration 308, loss = 0.24301364\n",
      "Iteration 309, loss = 0.24246972\n",
      "Iteration 310, loss = 0.24192741\n",
      "Iteration 311, loss = 0.24138669\n",
      "Iteration 312, loss = 0.24084756\n",
      "Iteration 313, loss = 0.24031003\n",
      "Iteration 314, loss = 0.23977408\n",
      "Iteration 315, loss = 0.23923972\n",
      "Iteration 316, loss = 0.23870694\n",
      "Iteration 317, loss = 0.23817574\n",
      "Iteration 318, loss = 0.23764612\n",
      "Iteration 319, loss = 0.23711807\n",
      "Iteration 320, loss = 0.23659160\n",
      "Iteration 321, loss = 0.23606671\n",
      "Iteration 322, loss = 0.23554338\n",
      "Iteration 323, loss = 0.23502162\n",
      "Iteration 324, loss = 0.23450142\n",
      "Iteration 325, loss = 0.23398280\n",
      "Iteration 326, loss = 0.23346573\n",
      "Iteration 327, loss = 0.23295022\n",
      "Iteration 328, loss = 0.23243627\n",
      "Iteration 329, loss = 0.23192388\n",
      "Iteration 330, loss = 0.23141305\n",
      "Iteration 331, loss = 0.23090377\n",
      "Iteration 332, loss = 0.23039603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 333, loss = 0.22988985\n",
      "Iteration 334, loss = 0.22938522\n",
      "Iteration 335, loss = 0.22888214\n",
      "Iteration 336, loss = 0.22838060\n",
      "Iteration 337, loss = 0.22788060\n",
      "Iteration 338, loss = 0.22738214\n",
      "Iteration 339, loss = 0.22688523\n",
      "Iteration 340, loss = 0.22638985\n",
      "Iteration 341, loss = 0.22589602\n",
      "Iteration 342, loss = 0.22540371\n",
      "Iteration 343, loss = 0.22491294\n",
      "Iteration 344, loss = 0.22442371\n",
      "Iteration 345, loss = 0.22393601\n",
      "Iteration 346, loss = 0.22344983\n",
      "Iteration 347, loss = 0.22296519\n",
      "Iteration 348, loss = 0.22248207\n",
      "Iteration 349, loss = 0.22200048\n",
      "Iteration 350, loss = 0.22152041\n",
      "Iteration 351, loss = 0.22104187\n",
      "Iteration 352, loss = 0.22056484\n",
      "Iteration 353, loss = 0.22008934\n",
      "Iteration 354, loss = 0.21961536\n",
      "Iteration 355, loss = 0.21914289\n",
      "Iteration 356, loss = 0.21867194\n",
      "Iteration 357, loss = 0.21820250\n",
      "Iteration 358, loss = 0.21773458\n",
      "Iteration 359, loss = 0.21726817\n",
      "Iteration 360, loss = 0.21680327\n",
      "Iteration 361, loss = 0.21633988\n",
      "Iteration 362, loss = 0.21587799\n",
      "Iteration 363, loss = 0.21541761\n",
      "Iteration 364, loss = 0.21495874\n",
      "Iteration 365, loss = 0.21450136\n",
      "Iteration 366, loss = 0.21404549\n",
      "Iteration 367, loss = 0.21359112\n",
      "Iteration 368, loss = 0.21313825\n",
      "Iteration 369, loss = 0.21268688\n",
      "Iteration 370, loss = 0.21223700\n",
      "Iteration 371, loss = 0.21178862\n",
      "Iteration 372, loss = 0.21134173\n",
      "Iteration 373, loss = 0.21089633\n",
      "Iteration 374, loss = 0.21045241\n",
      "Iteration 375, loss = 0.21000999\n",
      "Iteration 376, loss = 0.20956905\n",
      "Iteration 377, loss = 0.20912960\n",
      "Iteration 378, loss = 0.20869163\n",
      "Iteration 379, loss = 0.20825515\n",
      "Iteration 380, loss = 0.20782014\n",
      "Iteration 381, loss = 0.20738661\n",
      "Iteration 382, loss = 0.20695456\n",
      "Iteration 383, loss = 0.20652398\n",
      "Iteration 384, loss = 0.20609488\n",
      "Iteration 385, loss = 0.20566724\n",
      "Iteration 386, loss = 0.20524108\n",
      "Iteration 387, loss = 0.20481639\n",
      "Iteration 388, loss = 0.20439316\n",
      "Iteration 389, loss = 0.20397139\n",
      "Iteration 390, loss = 0.20355109\n",
      "Iteration 391, loss = 0.20313225\n",
      "Iteration 392, loss = 0.20271486\n",
      "Iteration 393, loss = 0.20229893\n",
      "Iteration 394, loss = 0.20188446\n",
      "Iteration 395, loss = 0.20147144\n",
      "Iteration 396, loss = 0.20105987\n",
      "Iteration 397, loss = 0.20064975\n",
      "Iteration 398, loss = 0.20024108\n",
      "Iteration 399, loss = 0.19983385\n",
      "Iteration 400, loss = 0.19942807\n",
      "Iteration 401, loss = 0.19902372\n",
      "Iteration 402, loss = 0.19862081\n",
      "Iteration 403, loss = 0.19821934\n",
      "Iteration 404, loss = 0.19781931\n",
      "Iteration 405, loss = 0.19742070\n",
      "Iteration 406, loss = 0.19702353\n",
      "Iteration 407, loss = 0.19662778\n",
      "Iteration 408, loss = 0.19623346\n",
      "Iteration 409, loss = 0.19584056\n",
      "Iteration 410, loss = 0.19544908\n",
      "Iteration 411, loss = 0.19505901\n",
      "Iteration 412, loss = 0.19467037\n",
      "Iteration 413, loss = 0.19428314\n",
      "Iteration 414, loss = 0.19389732\n",
      "Iteration 415, loss = 0.19351290\n",
      "Iteration 416, loss = 0.19312990\n",
      "Iteration 417, loss = 0.19274829\n",
      "Iteration 418, loss = 0.19236809\n",
      "Iteration 419, loss = 0.19198929\n",
      "Iteration 420, loss = 0.19161188\n",
      "Iteration 421, loss = 0.19123587\n",
      "Iteration 422, loss = 0.19086124\n",
      "Iteration 423, loss = 0.19048801\n",
      "Iteration 424, loss = 0.19011615\n",
      "Iteration 425, loss = 0.18974569\n",
      "Iteration 426, loss = 0.18937660\n",
      "Iteration 427, loss = 0.18900889\n",
      "Iteration 428, loss = 0.18864255\n",
      "Iteration 429, loss = 0.18827758\n",
      "Iteration 430, loss = 0.18791399\n",
      "Iteration 431, loss = 0.18755176\n",
      "Iteration 432, loss = 0.18719089\n",
      "Iteration 433, loss = 0.18683138\n",
      "Iteration 434, loss = 0.18647323\n",
      "Iteration 435, loss = 0.18611643\n",
      "Iteration 436, loss = 0.18576099\n",
      "Iteration 437, loss = 0.18540689\n",
      "Iteration 438, loss = 0.18505414\n",
      "Iteration 439, loss = 0.18470273\n",
      "Iteration 440, loss = 0.18435266\n",
      "Iteration 441, loss = 0.18400392\n",
      "Iteration 442, loss = 0.18365652\n",
      "Iteration 443, loss = 0.18331045\n",
      "Iteration 444, loss = 0.18296570\n",
      "Iteration 445, loss = 0.18262228\n",
      "Iteration 446, loss = 0.18228017\n",
      "Iteration 447, loss = 0.18193939\n",
      "Iteration 448, loss = 0.18159992\n",
      "Iteration 449, loss = 0.18126176\n",
      "Iteration 450, loss = 0.18092490\n",
      "Iteration 451, loss = 0.18058935\n",
      "Iteration 452, loss = 0.18025510\n",
      "Iteration 453, loss = 0.17992215\n",
      "Iteration 454, loss = 0.17959049\n",
      "Iteration 455, loss = 0.17926013\n",
      "Iteration 456, loss = 0.17893105\n",
      "Iteration 457, loss = 0.17860325\n",
      "Iteration 458, loss = 0.17827673\n",
      "Iteration 459, loss = 0.17795149\n",
      "Iteration 460, loss = 0.17762753\n",
      "Iteration 461, loss = 0.17730483\n",
      "Iteration 462, loss = 0.17698340\n",
      "Iteration 463, loss = 0.17666323\n",
      "Iteration 464, loss = 0.17634432\n",
      "Iteration 465, loss = 0.17602667\n",
      "Iteration 466, loss = 0.17571027\n",
      "Iteration 467, loss = 0.17539512\n",
      "Iteration 468, loss = 0.17508122\n",
      "Iteration 469, loss = 0.17476855\n",
      "Iteration 470, loss = 0.17445712\n",
      "Iteration 471, loss = 0.17414693\n",
      "Iteration 472, loss = 0.17383797\n",
      "Iteration 473, loss = 0.17353023\n",
      "Iteration 474, loss = 0.17322372\n",
      "Iteration 475, loss = 0.17291843\n",
      "Iteration 476, loss = 0.17261435\n",
      "Iteration 477, loss = 0.17231149\n",
      "Iteration 478, loss = 0.17200983\n",
      "Iteration 479, loss = 0.17170938\n",
      "Iteration 480, loss = 0.17141012\n",
      "Iteration 481, loss = 0.17111207\n",
      "Iteration 482, loss = 0.17081521\n",
      "Iteration 483, loss = 0.17051954\n",
      "Iteration 484, loss = 0.17022505\n",
      "Iteration 485, loss = 0.16993175\n",
      "Iteration 486, loss = 0.16963962\n",
      "Iteration 487, loss = 0.16934867\n",
      "Iteration 488, loss = 0.16905888\n",
      "Iteration 489, loss = 0.16877027\n",
      "Iteration 490, loss = 0.16848282\n",
      "Iteration 491, loss = 0.16819652\n",
      "Iteration 492, loss = 0.16791138\n",
      "Iteration 493, loss = 0.16762739\n",
      "Iteration 494, loss = 0.16734455\n",
      "Iteration 495, loss = 0.16706286\n",
      "Iteration 496, loss = 0.16678230\n",
      "Iteration 497, loss = 0.16650287\n",
      "Iteration 498, loss = 0.16622458\n",
      "Iteration 499, loss = 0.16594742\n",
      "Iteration 500, loss = 0.16567138\n",
      "Iteration 1, loss = 1.43607855\n",
      "Iteration 2, loss = 1.41951251\n",
      "Iteration 3, loss = 1.39635914\n",
      "Iteration 4, loss = 1.36774798\n",
      "Iteration 5, loss = 1.33476862\n",
      "Iteration 6, loss = 1.29844862\n",
      "Iteration 7, loss = 1.25973788\n",
      "Iteration 8, loss = 1.21949810\n",
      "Iteration 9, loss = 1.17849652\n",
      "Iteration 10, loss = 1.13740310\n",
      "Iteration 11, loss = 1.09679035\n",
      "Iteration 12, loss = 1.05713544\n",
      "Iteration 13, loss = 1.01882394\n",
      "Iteration 14, loss = 0.98215515\n",
      "Iteration 15, loss = 0.94734854\n",
      "Iteration 16, loss = 0.91455119\n",
      "Iteration 17, loss = 0.88384599\n",
      "Iteration 18, loss = 0.85526030\n",
      "Iteration 19, loss = 0.82877490\n",
      "Iteration 20, loss = 0.80433276\n",
      "Iteration 21, loss = 0.78184755\n",
      "Iteration 22, loss = 0.76121144\n",
      "Iteration 23, loss = 0.74230214\n",
      "Iteration 24, loss = 0.72498902\n",
      "Iteration 25, loss = 0.70913814\n",
      "Iteration 26, loss = 0.69461642\n",
      "Iteration 27, loss = 0.68129478\n",
      "Iteration 28, loss = 0.66905046\n",
      "Iteration 29, loss = 0.65776862\n",
      "Iteration 30, loss = 0.64734331\n",
      "Iteration 31, loss = 0.63767795\n",
      "Iteration 32, loss = 0.62868544\n",
      "Iteration 33, loss = 0.62028793\n",
      "Iteration 34, loss = 0.61241644\n",
      "Iteration 35, loss = 0.60501025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.59801633\n",
      "Iteration 37, loss = 0.59138858\n",
      "Iteration 38, loss = 0.58508716\n",
      "Iteration 39, loss = 0.57907780\n",
      "Iteration 40, loss = 0.57333111\n",
      "Iteration 41, loss = 0.56782198\n",
      "Iteration 42, loss = 0.56252901\n",
      "Iteration 43, loss = 0.55743393\n",
      "Iteration 44, loss = 0.55252119\n",
      "Iteration 45, loss = 0.54777748\n",
      "Iteration 46, loss = 0.54319140\n",
      "Iteration 47, loss = 0.53875312\n",
      "Iteration 48, loss = 0.53445411\n",
      "Iteration 49, loss = 0.53028685\n",
      "Iteration 50, loss = 0.52624472\n",
      "Iteration 51, loss = 0.52232176\n",
      "Iteration 52, loss = 0.51851255\n",
      "Iteration 53, loss = 0.51481212\n",
      "Iteration 54, loss = 0.51121583\n",
      "Iteration 55, loss = 0.50771932\n",
      "Iteration 56, loss = 0.50431847\n",
      "Iteration 57, loss = 0.50100932\n",
      "Iteration 58, loss = 0.49778807\n",
      "Iteration 59, loss = 0.49465108\n",
      "Iteration 60, loss = 0.49159481\n",
      "Iteration 61, loss = 0.48861585\n",
      "Iteration 62, loss = 0.48571091\n",
      "Iteration 63, loss = 0.48287679\n",
      "Iteration 64, loss = 0.48011045\n",
      "Iteration 65, loss = 0.47740892\n",
      "Iteration 66, loss = 0.47476940\n",
      "Iteration 67, loss = 0.47218916\n",
      "Iteration 68, loss = 0.46966564\n",
      "Iteration 69, loss = 0.46719637\n",
      "Iteration 70, loss = 0.46477904\n",
      "Iteration 71, loss = 0.46241142\n",
      "Iteration 72, loss = 0.46009142\n",
      "Iteration 73, loss = 0.45781707\n",
      "Iteration 74, loss = 0.45558652\n",
      "Iteration 75, loss = 0.45339799\n",
      "Iteration 76, loss = 0.45124984\n",
      "Iteration 77, loss = 0.44914051\n",
      "Iteration 78, loss = 0.44706853\n",
      "Iteration 79, loss = 0.44503252\n",
      "Iteration 80, loss = 0.44303118\n",
      "Iteration 81, loss = 0.44106326\n",
      "Iteration 82, loss = 0.43912761\n",
      "Iteration 83, loss = 0.43722313\n",
      "Iteration 84, loss = 0.43534876\n",
      "Iteration 85, loss = 0.43350352\n",
      "Iteration 86, loss = 0.43168646\n",
      "Iteration 87, loss = 0.42989669\n",
      "Iteration 88, loss = 0.42813334\n",
      "Iteration 89, loss = 0.42639559\n",
      "Iteration 90, loss = 0.42468266\n",
      "Iteration 91, loss = 0.42299380\n",
      "Iteration 92, loss = 0.42132829\n",
      "Iteration 93, loss = 0.41968543\n",
      "Iteration 94, loss = 0.41806457\n",
      "Iteration 95, loss = 0.41646506\n",
      "Iteration 96, loss = 0.41488629\n",
      "Iteration 97, loss = 0.41332767\n",
      "Iteration 98, loss = 0.41178864\n",
      "Iteration 99, loss = 0.41026864\n",
      "Iteration 100, loss = 0.40876716\n",
      "Iteration 101, loss = 0.40728368\n",
      "Iteration 102, loss = 0.40581773\n",
      "Iteration 103, loss = 0.40436882\n",
      "Iteration 104, loss = 0.40293652\n",
      "Iteration 105, loss = 0.40152038\n",
      "Iteration 106, loss = 0.40011999\n",
      "Iteration 107, loss = 0.39873494\n",
      "Iteration 108, loss = 0.39736486\n",
      "Iteration 109, loss = 0.39600935\n",
      "Iteration 110, loss = 0.39466808\n",
      "Iteration 111, loss = 0.39334067\n",
      "Iteration 112, loss = 0.39202681\n",
      "Iteration 113, loss = 0.39072617\n",
      "Iteration 114, loss = 0.38943843\n",
      "Iteration 115, loss = 0.38816330\n",
      "Iteration 116, loss = 0.38690049\n",
      "Iteration 117, loss = 0.38564970\n",
      "Iteration 118, loss = 0.38441068\n",
      "Iteration 119, loss = 0.38318316\n",
      "Iteration 120, loss = 0.38196687\n",
      "Iteration 121, loss = 0.38076159\n",
      "Iteration 122, loss = 0.37956706\n",
      "Iteration 123, loss = 0.37838306\n",
      "Iteration 124, loss = 0.37720937\n",
      "Iteration 125, loss = 0.37604575\n",
      "Iteration 126, loss = 0.37489201\n",
      "Iteration 127, loss = 0.37374794\n",
      "Iteration 128, loss = 0.37261334\n",
      "Iteration 129, loss = 0.37148801\n",
      "Iteration 130, loss = 0.37037177\n",
      "Iteration 131, loss = 0.36926443\n",
      "Iteration 132, loss = 0.36816582\n",
      "Iteration 133, loss = 0.36707577\n",
      "Iteration 134, loss = 0.36599410\n",
      "Iteration 135, loss = 0.36492066\n",
      "Iteration 136, loss = 0.36385528\n",
      "Iteration 137, loss = 0.36279782\n",
      "Iteration 138, loss = 0.36174811\n",
      "Iteration 139, loss = 0.36070602\n",
      "Iteration 140, loss = 0.35967140\n",
      "Iteration 141, loss = 0.35864411\n",
      "Iteration 142, loss = 0.35762403\n",
      "Iteration 143, loss = 0.35661100\n",
      "Iteration 144, loss = 0.35560492\n",
      "Iteration 145, loss = 0.35460565\n",
      "Iteration 146, loss = 0.35361307\n",
      "Iteration 147, loss = 0.35262707\n",
      "Iteration 148, loss = 0.35164752\n",
      "Iteration 149, loss = 0.35067432\n",
      "Iteration 150, loss = 0.34970735\n",
      "Iteration 151, loss = 0.34874651\n",
      "Iteration 152, loss = 0.34779169\n",
      "Iteration 153, loss = 0.34684280\n",
      "Iteration 154, loss = 0.34589973\n",
      "Iteration 155, loss = 0.34496238\n",
      "Iteration 156, loss = 0.34403066\n",
      "Iteration 157, loss = 0.34310448\n",
      "Iteration 158, loss = 0.34218375\n",
      "Iteration 159, loss = 0.34126838\n",
      "Iteration 160, loss = 0.34035828\n",
      "Iteration 161, loss = 0.33945338\n",
      "Iteration 162, loss = 0.33855358\n",
      "Iteration 163, loss = 0.33765881\n",
      "Iteration 164, loss = 0.33676898\n",
      "Iteration 165, loss = 0.33588404\n",
      "Iteration 166, loss = 0.33500389\n",
      "Iteration 167, loss = 0.33412846\n",
      "Iteration 168, loss = 0.33325770\n",
      "Iteration 169, loss = 0.33239151\n",
      "Iteration 170, loss = 0.33152985\n",
      "Iteration 171, loss = 0.33067263\n",
      "Iteration 172, loss = 0.32981980\n",
      "Iteration 173, loss = 0.32897129\n",
      "Iteration 174, loss = 0.32812705\n",
      "Iteration 175, loss = 0.32728700\n",
      "Iteration 176, loss = 0.32645109\n",
      "Iteration 177, loss = 0.32561926\n",
      "Iteration 178, loss = 0.32479146\n",
      "Iteration 179, loss = 0.32396763\n",
      "Iteration 180, loss = 0.32314771\n",
      "Iteration 181, loss = 0.32233166\n",
      "Iteration 182, loss = 0.32151942\n",
      "Iteration 183, loss = 0.32071093\n",
      "Iteration 184, loss = 0.31990616\n",
      "Iteration 185, loss = 0.31910505\n",
      "Iteration 186, loss = 0.31830755\n",
      "Iteration 187, loss = 0.31751362\n",
      "Iteration 188, loss = 0.31672321\n",
      "Iteration 189, loss = 0.31593627\n",
      "Iteration 190, loss = 0.31515277\n",
      "Iteration 191, loss = 0.31437267\n",
      "Iteration 192, loss = 0.31359591\n",
      "Iteration 193, loss = 0.31282246\n",
      "Iteration 194, loss = 0.31205227\n",
      "Iteration 195, loss = 0.31128532\n",
      "Iteration 196, loss = 0.31052155\n",
      "Iteration 197, loss = 0.30976094\n",
      "Iteration 198, loss = 0.30900345\n",
      "Iteration 199, loss = 0.30824904\n",
      "Iteration 200, loss = 0.30749767\n",
      "Iteration 201, loss = 0.30674931\n",
      "Iteration 202, loss = 0.30600393\n",
      "Iteration 203, loss = 0.30526149\n",
      "Iteration 204, loss = 0.30452197\n",
      "Iteration 205, loss = 0.30378532\n",
      "Iteration 206, loss = 0.30305153\n",
      "Iteration 207, loss = 0.30232055\n",
      "Iteration 208, loss = 0.30159236\n",
      "Iteration 209, loss = 0.30086693\n",
      "Iteration 210, loss = 0.30014423\n",
      "Iteration 211, loss = 0.29942424\n",
      "Iteration 212, loss = 0.29870692\n",
      "Iteration 213, loss = 0.29799225\n",
      "Iteration 214, loss = 0.29728021\n",
      "Iteration 215, loss = 0.29657076\n",
      "Iteration 216, loss = 0.29586388\n",
      "Iteration 217, loss = 0.29515955\n",
      "Iteration 218, loss = 0.29445775\n",
      "Iteration 219, loss = 0.29375845\n",
      "Iteration 220, loss = 0.29306162\n",
      "Iteration 221, loss = 0.29236724\n",
      "Iteration 222, loss = 0.29167530\n",
      "Iteration 223, loss = 0.29098577\n",
      "Iteration 224, loss = 0.29029863\n",
      "Iteration 225, loss = 0.28961386\n",
      "Iteration 226, loss = 0.28893143\n",
      "Iteration 227, loss = 0.28825133\n",
      "Iteration 228, loss = 0.28757354\n",
      "Iteration 229, loss = 0.28689804\n",
      "Iteration 230, loss = 0.28622481\n",
      "Iteration 231, loss = 0.28555384\n",
      "Iteration 232, loss = 0.28488509\n",
      "Iteration 233, loss = 0.28421857\n",
      "Iteration 234, loss = 0.28355424\n",
      "Iteration 235, loss = 0.28289210\n",
      "Iteration 236, loss = 0.28223212\n",
      "Iteration 237, loss = 0.28157429\n",
      "Iteration 238, loss = 0.28091860\n",
      "Iteration 239, loss = 0.28026503\n",
      "Iteration 240, loss = 0.27961356\n",
      "Iteration 241, loss = 0.27896417\n",
      "Iteration 242, loss = 0.27831687\n",
      "Iteration 243, loss = 0.27767162\n",
      "Iteration 244, loss = 0.27702842\n",
      "Iteration 245, loss = 0.27638725\n",
      "Iteration 246, loss = 0.27574810\n",
      "Iteration 247, loss = 0.27511096\n",
      "Iteration 248, loss = 0.27447581\n",
      "Iteration 249, loss = 0.27384264\n",
      "Iteration 250, loss = 0.27321144\n",
      "Iteration 251, loss = 0.27258220\n",
      "Iteration 252, loss = 0.27195490\n",
      "Iteration 253, loss = 0.27132953\n",
      "Iteration 254, loss = 0.27070609\n",
      "Iteration 255, loss = 0.27008456\n",
      "Iteration 256, loss = 0.26946493\n",
      "Iteration 257, loss = 0.26884720\n",
      "Iteration 258, loss = 0.26823134\n",
      "Iteration 259, loss = 0.26761735\n",
      "Iteration 260, loss = 0.26700522\n",
      "Iteration 261, loss = 0.26639494\n",
      "Iteration 262, loss = 0.26578651\n",
      "Iteration 263, loss = 0.26517990\n",
      "Iteration 264, loss = 0.26457512\n",
      "Iteration 265, loss = 0.26397216\n",
      "Iteration 266, loss = 0.26337100\n",
      "Iteration 267, loss = 0.26277163\n",
      "Iteration 268, loss = 0.26217406\n",
      "Iteration 269, loss = 0.26157827\n",
      "Iteration 270, loss = 0.26098425\n",
      "Iteration 271, loss = 0.26039200\n",
      "Iteration 272, loss = 0.25980151\n",
      "Iteration 273, loss = 0.25921276\n",
      "Iteration 274, loss = 0.25862576\n",
      "Iteration 275, loss = 0.25804050\n",
      "Iteration 276, loss = 0.25745697\n",
      "Iteration 277, loss = 0.25687516\n",
      "Iteration 278, loss = 0.25629507\n",
      "Iteration 279, loss = 0.25571669\n",
      "Iteration 280, loss = 0.25514001\n",
      "Iteration 281, loss = 0.25456503\n",
      "Iteration 282, loss = 0.25399175\n",
      "Iteration 283, loss = 0.25342015\n",
      "Iteration 284, loss = 0.25285023\n",
      "Iteration 285, loss = 0.25228199\n",
      "Iteration 286, loss = 0.25171541\n",
      "Iteration 287, loss = 0.25115050\n",
      "Iteration 288, loss = 0.25058725\n",
      "Iteration 289, loss = 0.25002566\n",
      "Iteration 290, loss = 0.24946571\n",
      "Iteration 291, loss = 0.24890741\n",
      "Iteration 292, loss = 0.24835075\n",
      "Iteration 293, loss = 0.24779573\n",
      "Iteration 294, loss = 0.24724233\n",
      "Iteration 295, loss = 0.24669057\n",
      "Iteration 296, loss = 0.24614042\n",
      "Iteration 297, loss = 0.24559189\n",
      "Iteration 298, loss = 0.24504498\n",
      "Iteration 299, loss = 0.24449968\n",
      "Iteration 300, loss = 0.24395598\n",
      "Iteration 301, loss = 0.24341388\n",
      "Iteration 302, loss = 0.24287339\n",
      "Iteration 303, loss = 0.24233449\n",
      "Iteration 304, loss = 0.24179718\n",
      "Iteration 305, loss = 0.24126146\n",
      "Iteration 306, loss = 0.24072732\n",
      "Iteration 307, loss = 0.24019476\n",
      "Iteration 308, loss = 0.23966378\n",
      "Iteration 309, loss = 0.23913438\n",
      "Iteration 310, loss = 0.23860655\n",
      "Iteration 311, loss = 0.23808029\n",
      "Iteration 312, loss = 0.23755559\n",
      "Iteration 313, loss = 0.23703246\n",
      "Iteration 314, loss = 0.23651088\n",
      "Iteration 315, loss = 0.23599087\n",
      "Iteration 316, loss = 0.23547240\n",
      "Iteration 317, loss = 0.23495549\n",
      "Iteration 318, loss = 0.23444013\n",
      "Iteration 319, loss = 0.23392632\n",
      "Iteration 320, loss = 0.23341405\n",
      "Iteration 321, loss = 0.23290332\n",
      "Iteration 322, loss = 0.23239413\n",
      "Iteration 323, loss = 0.23188648\n",
      "Iteration 324, loss = 0.23138036\n",
      "Iteration 325, loss = 0.23087578\n",
      "Iteration 326, loss = 0.23037273\n",
      "Iteration 327, loss = 0.22987120\n",
      "Iteration 328, loss = 0.22937120\n",
      "Iteration 329, loss = 0.22887272\n",
      "Iteration 330, loss = 0.22837577\n",
      "Iteration 331, loss = 0.22788033\n",
      "Iteration 332, loss = 0.22738642\n",
      "Iteration 333, loss = 0.22689402\n",
      "Iteration 334, loss = 0.22640313\n",
      "Iteration 335, loss = 0.22591375\n",
      "Iteration 336, loss = 0.22542589\n",
      "Iteration 337, loss = 0.22493953\n",
      "Iteration 338, loss = 0.22445468\n",
      "Iteration 339, loss = 0.22397134\n",
      "Iteration 340, loss = 0.22348949\n",
      "Iteration 341, loss = 0.22300915\n",
      "Iteration 342, loss = 0.22253031\n",
      "Iteration 343, loss = 0.22205296\n",
      "Iteration 344, loss = 0.22157711\n",
      "Iteration 345, loss = 0.22110276\n",
      "Iteration 346, loss = 0.22062989\n",
      "Iteration 347, loss = 0.22015852\n",
      "Iteration 348, loss = 0.21968864\n",
      "Iteration 349, loss = 0.21922024\n",
      "Iteration 350, loss = 0.21875333\n",
      "Iteration 351, loss = 0.21828791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 352, loss = 0.21782397\n",
      "Iteration 353, loss = 0.21736150\n",
      "Iteration 354, loss = 0.21690052\n",
      "Iteration 355, loss = 0.21644102\n",
      "Iteration 356, loss = 0.21598299\n",
      "Iteration 357, loss = 0.21552643\n",
      "Iteration 358, loss = 0.21507135\n",
      "Iteration 359, loss = 0.21461774\n",
      "Iteration 360, loss = 0.21416560\n",
      "Iteration 361, loss = 0.21371493\n",
      "Iteration 362, loss = 0.21326573\n",
      "Iteration 363, loss = 0.21281799\n",
      "Iteration 364, loss = 0.21237171\n",
      "Iteration 365, loss = 0.21192690\n",
      "Iteration 366, loss = 0.21148355\n",
      "Iteration 367, loss = 0.21104165\n",
      "Iteration 368, loss = 0.21060122\n",
      "Iteration 369, loss = 0.21016223\n",
      "Iteration 370, loss = 0.20972471\n",
      "Iteration 371, loss = 0.20928863\n",
      "Iteration 372, loss = 0.20885400\n",
      "Iteration 373, loss = 0.20842083\n",
      "Iteration 374, loss = 0.20798910\n",
      "Iteration 375, loss = 0.20755882\n",
      "Iteration 376, loss = 0.20712998\n",
      "Iteration 377, loss = 0.20670258\n",
      "Iteration 378, loss = 0.20627663\n",
      "Iteration 379, loss = 0.20585211\n",
      "Iteration 380, loss = 0.20542903\n",
      "Iteration 381, loss = 0.20500739\n",
      "Iteration 382, loss = 0.20458718\n",
      "Iteration 383, loss = 0.20416840\n",
      "Iteration 384, loss = 0.20375105\n",
      "Iteration 385, loss = 0.20333513\n",
      "Iteration 386, loss = 0.20292064\n",
      "Iteration 387, loss = 0.20250757\n",
      "Iteration 388, loss = 0.20209592\n",
      "Iteration 389, loss = 0.20168570\n",
      "Iteration 390, loss = 0.20127689\n",
      "Iteration 391, loss = 0.20086951\n",
      "Iteration 392, loss = 0.20046353\n",
      "Iteration 393, loss = 0.20005897\n",
      "Iteration 394, loss = 0.19965583\n",
      "Iteration 395, loss = 0.19925409\n",
      "Iteration 396, loss = 0.19885376\n",
      "Iteration 397, loss = 0.19845483\n",
      "Iteration 398, loss = 0.19805731\n",
      "Iteration 399, loss = 0.19766119\n",
      "Iteration 400, loss = 0.19726647\n",
      "Iteration 401, loss = 0.19687315\n",
      "Iteration 402, loss = 0.19648122\n",
      "Iteration 403, loss = 0.19609068\n",
      "Iteration 404, loss = 0.19570154\n",
      "Iteration 405, loss = 0.19531378\n",
      "Iteration 406, loss = 0.19492742\n",
      "Iteration 407, loss = 0.19454243\n",
      "Iteration 408, loss = 0.19415883\n",
      "Iteration 409, loss = 0.19377661\n",
      "Iteration 410, loss = 0.19339576\n",
      "Iteration 411, loss = 0.19301629\n",
      "Iteration 412, loss = 0.19263819\n",
      "Iteration 413, loss = 0.19226147\n",
      "Iteration 414, loss = 0.19188611\n",
      "Iteration 415, loss = 0.19151212\n",
      "Iteration 416, loss = 0.19113949\n",
      "Iteration 417, loss = 0.19076822\n",
      "Iteration 418, loss = 0.19039831\n",
      "Iteration 419, loss = 0.19002975\n",
      "Iteration 420, loss = 0.18966255\n",
      "Iteration 421, loss = 0.18929670\n",
      "Iteration 422, loss = 0.18893220\n",
      "Iteration 423, loss = 0.18856905\n",
      "Iteration 424, loss = 0.18820724\n",
      "Iteration 425, loss = 0.18784676\n",
      "Iteration 426, loss = 0.18748763\n",
      "Iteration 427, loss = 0.18712983\n",
      "Iteration 428, loss = 0.18677337\n",
      "Iteration 429, loss = 0.18641823\n",
      "Iteration 430, loss = 0.18606442\n",
      "Iteration 431, loss = 0.18571194\n",
      "Iteration 432, loss = 0.18536078\n",
      "Iteration 433, loss = 0.18501093\n",
      "Iteration 434, loss = 0.18466241\n",
      "Iteration 435, loss = 0.18431519\n",
      "Iteration 436, loss = 0.18396929\n",
      "Iteration 437, loss = 0.18362469\n",
      "Iteration 438, loss = 0.18328140\n",
      "Iteration 439, loss = 0.18293941\n",
      "Iteration 440, loss = 0.18259872\n",
      "Iteration 441, loss = 0.18225932\n",
      "Iteration 442, loss = 0.18192122\n",
      "Iteration 443, loss = 0.18158441\n",
      "Iteration 444, loss = 0.18124888\n",
      "Iteration 445, loss = 0.18091464\n",
      "Iteration 446, loss = 0.18058167\n",
      "Iteration 447, loss = 0.18024999\n",
      "Iteration 448, loss = 0.17991958\n",
      "Iteration 449, loss = 0.17959044\n",
      "Iteration 450, loss = 0.17926257\n",
      "Iteration 451, loss = 0.17893596\n",
      "Iteration 452, loss = 0.17861062\n",
      "Iteration 453, loss = 0.17828653\n",
      "Iteration 454, loss = 0.17796370\n",
      "Iteration 455, loss = 0.17764213\n",
      "Iteration 456, loss = 0.17732180\n",
      "Iteration 457, loss = 0.17700271\n",
      "Iteration 458, loss = 0.17668487\n",
      "Iteration 459, loss = 0.17636827\n",
      "Iteration 460, loss = 0.17605291\n",
      "Iteration 461, loss = 0.17573877\n",
      "Iteration 462, loss = 0.17542587\n",
      "Iteration 463, loss = 0.17511419\n",
      "Iteration 464, loss = 0.17480373\n",
      "Iteration 465, loss = 0.17449449\n",
      "Iteration 466, loss = 0.17418647\n",
      "Iteration 467, loss = 0.17387966\n",
      "Iteration 468, loss = 0.17357406\n",
      "Iteration 469, loss = 0.17326966\n",
      "Iteration 470, loss = 0.17296647\n",
      "Iteration 471, loss = 0.17266447\n",
      "Iteration 472, loss = 0.17236367\n",
      "Iteration 473, loss = 0.17206406\n",
      "Iteration 474, loss = 0.17176564\n",
      "Iteration 475, loss = 0.17146840\n",
      "Iteration 476, loss = 0.17117235\n",
      "Iteration 477, loss = 0.17087746\n",
      "Iteration 478, loss = 0.17058376\n",
      "Iteration 479, loss = 0.17029122\n",
      "Iteration 480, loss = 0.16999985\n",
      "Iteration 481, loss = 0.16970964\n",
      "Iteration 482, loss = 0.16942059\n",
      "Iteration 483, loss = 0.16913270\n",
      "Iteration 484, loss = 0.16884596\n",
      "Iteration 485, loss = 0.16856037\n",
      "Iteration 486, loss = 0.16827592\n",
      "Iteration 487, loss = 0.16799261\n",
      "Iteration 488, loss = 0.16771044\n",
      "Iteration 489, loss = 0.16742940\n",
      "Iteration 490, loss = 0.16714950\n",
      "Iteration 491, loss = 0.16687072\n",
      "Iteration 492, loss = 0.16659306\n",
      "Iteration 493, loss = 0.16631652\n",
      "Iteration 494, loss = 0.16604109\n",
      "Iteration 495, loss = 0.16576678\n",
      "Iteration 496, loss = 0.16549358\n",
      "Iteration 497, loss = 0.16522148\n",
      "Iteration 498, loss = 0.16495047\n",
      "Iteration 499, loss = 0.16468057\n",
      "Iteration 500, loss = 0.16441176\n",
      "0.9466666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_dt = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "print(scores_dt.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      " Neural Network model accuracy(in %): 97.77777777777777\n",
      "[[14  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import metrics \n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\" Neural Network model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADKhJREFUeJzt3V+MXGUZx/HfzwIqgmJASeVPCiogVFKUNCFGbEBjaUgQ9cKaiCBhNRGEeAPEC+QGNUQMF0JcbSEmpMSARlIBbQxIUAstWJqWYiGUhBWwJii2klh25vGiUztpd3Zmzs67c/qc78ecZPfMzHteJuTH43Pe96wjQgCAct427gkAQHYELQAURtACQGEELQAURtACQGEELQAURtACQGEELQAURtACQGGHlb7ArjtWsvWssLNvfWrcUwBGYseO7Z7zIFvvHzxzzvrC3K83ACpaACiMoAWAwoq3DgBgPkWrNfB756VvIIIWQDat6XHP4CC0DgCgMCpaAKlEe/CKdr5aB1S0AFAYFS2AXIa4GTZfCFoAqQQ3wwCgeahoAeRCRQsAzUNFCyCVYZZ3zReCFkAurDoAgLJYdQAADURFCyAXKloAaB4qWgCpRJubYQBQFDfDAKCBCFoAubSmBz/6sL3a9k7bW7rOfdf232xv6hwr+o1D0AJAb3dLWj7D+R9FxJLO8WC/QejRAkhllDfDIuIx24vmOg4VLYBcRtg6mMXVtjd3Wgvv7fdmghZAY9mesL2x65gY4GN3SvqgpCWSXpX0w34foHUAIJVhlndFxKSkyaHGj/j7vp9t/1TS2n6fIWgB5FJ4Ha3thRHxaufXSyVtme39EkELAD3ZXiNpmaTjbE9JuknSMttLJIWklyR9vd84BC2AVEa86mDlDKdXDTsOQQsgF7bgAkDzUNECSCUOxT9lY/sMSZdIOkF7m7+vSHogIrYVnhsApDBr68D29ZLulWRJT0ra0Pl5je0byk8PAIYTremBj/nSr6K9UtJZEfFW90nbt0naKun7pSYGAFn0uxnWlvSBGc4v7Lw2o+5tbXc9/sJc5gcAw2lPD37Mk34V7XWSfm/7eUkvd86dLOlDkq7u9aHubW277lgZI5gnAAzkkLsZFhEP2z5N0lLtvRlmSVOSNkRE/f5pAKCG+q46iIi2pPXzMBcAmLtDraIFgENNHf84I0ELIJcaVrRswQWAwqhoAaRSx1UHVLQAUBgVLYBURvk82lGhogWAwqhoAeRSwx4tQQsgFW6GAUADUdECSCVaPR8sODZUtABQGBUtgFxqWNEStABSqePNMIIWQCrRqt/fGqBHCwCFUdECSIVVBwDQQFS0AFKpY0VL0AJIJdrcDAOAQ4bt1bZ32t7Sde5W28/Z3mz7V7aP6TcOQQsglWjFwMcA7pa0/IBz6yQtjoizJW2XdGO/QQhaAOghIh6T9PoB534XEfv+1O56SSf2G4egBZBKtAY/bE/Y3th1TAx5ua9Jeqjfm7gZBiCVYXaGRcSkpMkq17H9HUnTku7p916CFkAq7XlY3WX7q5IulnRhRPRNdoIWAIZge7mk6yV9KiLeHOQzBC2AVGKED++yvUbSMknH2Z6SdJP2rjJ4u6R1tiVpfUR8Y7ZxCFoA6CEiVs5wetWw4xC0AFIZZUU7KgQtgFTm42bYsIoH7dm3PlX6Eo23Y+33xj2F9Bat+Pa4p4BDGBUtgFTq2DpgZxgAFEZFCyCVdtvjnsJBCFoAqdTxZhitAwAojIoWQCp1vBlG0AJIpY49WloHAFAYFS2AVNq0DgCgLFoHANBAVLQAUgkqWgBoHipaAKnUcWcYQQsgFW6GAUADUdECSIWKFgAaiIoWQCqtGla0BC2AVOrYOiBoAaTSjvoFLT1aACiMihZAKnXcsEBFCwCFUdECSKVFjxYAmoegBZBKu+2Bj35sX2t7i+2ttq+rOidaBwBSGVXrwPZiSVdJWippj6SHbf8mIp4fdiwqWgCY2UckrY+INyNiWtIfJF1aZSCCFkAq7fDARx9bJJ1v+1jbR0paIemkKnOidQCgsWxPSJroOjUZEZOSFBHbbP9A0jpJuyU9I2m6ynUIWgCp7BniWQedUJ2c5fVVklZJku1bJE1VmRNBCwA92H5/ROy0fbKkz0s6r8o4BC2AVEb8UJn7bR8r6S1J34yIf1YZhKAFgB4i4pOjGKfyqgPbV4xiAgAwSq3wwMd8mcvyrpt7vWB7wvZG2xt37XpjDpcAgEPfrK0D25t7vSTp+F6f676Td8opp0Xl2QHAkFo1TJx+PdrjJX1W0oENYEv6U5EZAcActFS/p3f1C9q1ko6KiE0HvmD70SIzAoBkZg3aiLhylte+PPrpAMDc1LF1wLMOAKAw1tECSKU17gnMgKAFkApBCwCF1XHVAT1aACiMihZAKq2o37IDKloAKIyKFkAq3AwDgMLqGLS0DgCgMCpaAKlQ0QJAA1HRAkilpfot7yJoAaRC6wAAGoiKFkAqddwZRtACSIXWAQA0EBUtgFRYdQAAhdUxaGkdAEBhVLQAUuFmGAA0EEELIJVWxMBHP7aPsX2f7edsb7N9XpU50ToAkMqIb4bdLunhiPii7SMkHVllEIIWAGZg+92Szpd0uSRFxB5Je6qMResAQCotxcBHH6dK+oeku2z/xfbPbL+rypwIWgCNZXvC9sauY6Lr5cMkfUzSnRFxjqT/SLqhynVoHQBIpT3EQ2UiYlLSZI+XpyRNRcQTnd/vU8WgpaIFkMqoWgcR8Zqkl22f3jl1oaRnq8yJihZAKiNedXCNpHs6Kw5elHRFlUEI2gROufjGcU8hvacvP3XcU8AYRMQmSefOdRyCFkAqdXzwNz1aACiMihZAKnV8TCJBCyCVYZZ3zRdaBwBQGBUtgFTq2DqgogWAwqhoAaRCRQsADURFCyCVt6I97ikchIoWAAqjogWQCltwAaCBqGgBpNJm1QEANA8VLYBU6tijJWgBpMJDZQCggahoAaTCFlwAaCAqWgCptGu4BZegBZBKHdfRErQAUqnj8i56tABQGBUtgFTq2DqgogWAwqhoAaRSx51hBC2AVOq3uIvWAQAUR0ULIJU6tg6oaAFgBrbfYftJ28/Y3mr75qpjUdECSGWEy7v+K+mCiNht+3BJj9t+KCLWDzsQQQsglVG1DiIiJO3u/Hp456g0eN/Wge0zbF9o+6gDzi+vckEAqAvbE7Y3dh0TB7y+wPYmSTslrYuIJ6pcZ9agtf0tSb+WdI2kLbYv6Xr5lioXBICS2oqBj4iYjIhzu47J7rEiohURSySdKGmp7cVV5tSvdXCVpI93ehSLJN1ne1FE3C7JVS4IACWV2IIbEf+y/aik5ZK2DPv5fq2DBRGxu3OhlyQtk3SR7ds0S9B2l+O7dr0x7JwAYOxsv8/2MZ2f3ynp05KeqzJWv6B9zfaSfb90QvdiScdJ+mivD3WX40cf/Z4q8wKAStox+NHHQkmP2N4saYP29mjXVplTv9bBZZKmu09ExLSky2z/pMoFAaCkUbUOImKzpHNGMdasQRsRU7O89sdRTAAAsmMdLYBUeB4tADQQFS2AVGr4TBmCFkAutA4AoIGoaAGkUr96looWAIqjogWQSh17tAQtgFTqF7MELYBk6hi09GgBoDAqWgCp1LFHS0ULAIVR0QJIpX71LEELIJk6Bi2tAwAojIoWQCpUtADQQFS0AFKhogWABqKiBZCMxz2Bg1DRAkBhVLQAkqGiBYDGoaIFkAwVLQA0DhUtgFzqV9AStACyqd//Ua/fjACgJmwvt/1X2y/YvqHqOAQtgFQ8xP9mHcdeIOnHki6SdKaklbbPrDInghYAZrZU0gsR8WJE7JF0r6RLqgxE0ALIxR78mN0Jkl7u+n2qc25oxW+G7dixvYb3AGdneyIiJsc9j8z4jstr6nc8TObYnpA00XVqsus7m2mcSg8Ho6Kd2UT/t2CO+I7L4zvuIyImI+LcrqP7P0xTkk7q+v1ESa9UuQ5BCwAz2yDpw7ZPsX2EpC9JeqDKQKyjBYAZRMS07asl/VbSAkmrI2JrlbEI2pk1rq81BnzH5fEdz1FEPCjpwbmO44g6/uEHAMiDHi0AFEbQdhnVdjv0Znu17Z22t4x7LlnZPsn2I7a32d5q+9pxz6npaB10dLbbbZf0Ge1d1rFB0sqIeHasE0vG9vmSdkv6eUQsHvd8MrK9UNLCiHja9tGSnpL0Of5dHh8q2v1Gtt0OvUXEY5JeH/c8MouIVyPi6c7PuyRtU8UdTRgNgna/kW23A+rC9iJJ50h6YrwzaTaCdr+RbbcD6sD2UZLul3RdRPx73PNpMoJ2v5FttwPGzfbh2huy90TEL8c9n6YjaPcb2XY7YJxsW9IqSdsi4rZxzwcE7f9FxLSkfdvttkn6RdXtdujN9hpJf5Z0uu0p21eOe04JfULSVyRdYHtT51gx7kk1Gcu7AKAwKloAKIygBYDCCFoAKIygBYDCCFoAKIygBYDCCFoAKIygBYDC/gf8dLDzjl2rzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Decision regions in 2D\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "#Plotting decision regions\n",
    "plot_decision_regions(X, y, clf=clf, legend=2)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.title('Decision Tree classifier on Iris')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
