{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Iris.csv')\n",
    "df = df.rename(index=str, columns={\"Species\": \"Class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm        Class\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.445368</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.250000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>75.500000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.750000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
       "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
       "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
       "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
       "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
       "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
       "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
       "max    150.000000       7.900000      4.400000       6.900000      2.500000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_iris = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "X = data.drop(['SepalWidthCm','PetalWidthCm','Class','Id'],axis=1).values\n",
    "y = data['Class'].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_previsores = LabelEncoder()\n",
    "y = labelencoder_previsores.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900681</td>\n",
       "      <td>1.032057</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.124958</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.385353</td>\n",
       "      <td>0.337848</td>\n",
       "      <td>-1.398138</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506521</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.021849</td>\n",
       "      <td>1.263460</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "0      -0.900681      1.032057      -1.341272     -1.312977\n",
       "1      -1.143017     -0.124958      -1.341272     -1.312977\n",
       "2      -1.385353      0.337848      -1.398138     -1.312977\n",
       "3      -1.506521      0.106445      -1.284407     -1.312977\n",
       "4      -1.021849      1.263460      -1.341272     -1.312977"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_features = scaler.transform(X)\n",
    "X = pd.DataFrame(scaled_features,columns=df.columns[1:3]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001, activation='identity',\n",
    "                     solver='sgd', verbose=10,  random_state=0,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.05772126\n",
      "Iteration 2, loss = 1.03652513\n",
      "Iteration 3, loss = 1.00737905\n",
      "Iteration 4, loss = 0.97214010\n",
      "Iteration 5, loss = 0.93262051\n",
      "Iteration 6, loss = 0.89051531\n",
      "Iteration 7, loss = 0.84734351\n",
      "Iteration 8, loss = 0.80440356\n",
      "Iteration 9, loss = 0.76274497\n",
      "Iteration 10, loss = 0.72315785\n",
      "Iteration 11, loss = 0.68618079\n",
      "Iteration 12, loss = 0.65212499\n",
      "Iteration 13, loss = 0.62111018\n",
      "Iteration 14, loss = 0.59310673\n",
      "Iteration 15, loss = 0.56797814\n",
      "Iteration 16, loss = 0.54551953\n",
      "Iteration 17, loss = 0.52548936\n",
      "Iteration 18, loss = 0.50763348\n",
      "Iteration 19, loss = 0.49170186\n",
      "Iteration 20, loss = 0.47745918\n",
      "Iteration 21, loss = 0.46469056\n",
      "Iteration 22, loss = 0.45320404\n",
      "Iteration 23, loss = 0.44283073\n",
      "Iteration 24, loss = 0.43342363\n",
      "Iteration 25, loss = 0.42485580\n",
      "Iteration 26, loss = 0.41701815\n",
      "Iteration 27, loss = 0.40981721\n",
      "Iteration 28, loss = 0.40317310\n",
      "Iteration 29, loss = 0.39701754\n",
      "Iteration 30, loss = 0.39129219\n",
      "Iteration 31, loss = 0.38594717\n",
      "Iteration 32, loss = 0.38093975\n",
      "Iteration 33, loss = 0.37623328\n",
      "Iteration 34, loss = 0.37179622\n",
      "Iteration 35, loss = 0.36760140\n",
      "Iteration 36, loss = 0.36362527\n",
      "Iteration 37, loss = 0.35984737\n",
      "Iteration 38, loss = 0.35624988\n",
      "Iteration 39, loss = 0.35281712\n",
      "Iteration 40, loss = 0.34953533\n",
      "Iteration 41, loss = 0.34639226\n",
      "Iteration 42, loss = 0.34337705\n",
      "Iteration 43, loss = 0.34047995\n",
      "Iteration 44, loss = 0.33769217\n",
      "Iteration 45, loss = 0.33500578\n",
      "Iteration 46, loss = 0.33241358\n",
      "Iteration 47, loss = 0.32990897\n",
      "Iteration 48, loss = 0.32748594\n",
      "Iteration 49, loss = 0.32513896\n",
      "Iteration 50, loss = 0.32286293\n",
      "Iteration 51, loss = 0.32065319\n",
      "Iteration 52, loss = 0.31850538\n",
      "Iteration 53, loss = 0.31641553\n",
      "Iteration 54, loss = 0.31437993\n",
      "Iteration 55, loss = 0.31239517\n",
      "Iteration 56, loss = 0.31045810\n",
      "Iteration 57, loss = 0.30856578\n",
      "Iteration 58, loss = 0.30671552\n",
      "Iteration 59, loss = 0.30490484\n",
      "Iteration 60, loss = 0.30313141\n",
      "Iteration 61, loss = 0.30139310\n",
      "Iteration 62, loss = 0.29968796\n",
      "Iteration 63, loss = 0.29801415\n",
      "Iteration 64, loss = 0.29636998\n",
      "Iteration 65, loss = 0.29475391\n",
      "Iteration 66, loss = 0.29316447\n",
      "Iteration 67, loss = 0.29160034\n",
      "Iteration 68, loss = 0.29006027\n",
      "Iteration 69, loss = 0.28854311\n",
      "Iteration 70, loss = 0.28704778\n",
      "Iteration 71, loss = 0.28557328\n",
      "Iteration 72, loss = 0.28411870\n",
      "Iteration 73, loss = 0.28268315\n",
      "Iteration 74, loss = 0.28126584\n",
      "Iteration 75, loss = 0.27986600\n",
      "Iteration 76, loss = 0.27848293\n",
      "Iteration 77, loss = 0.27711596\n",
      "Iteration 78, loss = 0.27576447\n",
      "Iteration 79, loss = 0.27442788\n",
      "Iteration 80, loss = 0.27310565\n",
      "Iteration 81, loss = 0.27179727\n",
      "Iteration 82, loss = 0.27050224\n",
      "Iteration 83, loss = 0.26922012\n",
      "Iteration 84, loss = 0.26795048\n",
      "Iteration 85, loss = 0.26669293\n",
      "Iteration 86, loss = 0.26544708\n",
      "Iteration 87, loss = 0.26421259\n",
      "Iteration 88, loss = 0.26298912\n",
      "Iteration 89, loss = 0.26177636\n",
      "Iteration 90, loss = 0.26057401\n",
      "Iteration 91, loss = 0.25938179\n",
      "Iteration 92, loss = 0.25819945\n",
      "Iteration 93, loss = 0.25702674\n",
      "Iteration 94, loss = 0.25586342\n",
      "Iteration 95, loss = 0.25470927\n",
      "Iteration 96, loss = 0.25356409\n",
      "Iteration 97, loss = 0.25242768\n",
      "Iteration 98, loss = 0.25129985\n",
      "Iteration 99, loss = 0.25018043\n",
      "Iteration 100, loss = 0.24906926\n",
      "Iteration 101, loss = 0.24796616\n",
      "Iteration 102, loss = 0.24687100\n",
      "Iteration 103, loss = 0.24578363\n",
      "Iteration 104, loss = 0.24470392\n",
      "Iteration 105, loss = 0.24363173\n",
      "Iteration 106, loss = 0.24256695\n",
      "Iteration 107, loss = 0.24150945\n",
      "Iteration 108, loss = 0.24045913\n",
      "Iteration 109, loss = 0.23941588\n",
      "Iteration 110, loss = 0.23837959\n",
      "Iteration 111, loss = 0.23735017\n",
      "Iteration 112, loss = 0.23632753\n",
      "Iteration 113, loss = 0.23531157\n",
      "Iteration 114, loss = 0.23430222\n",
      "Iteration 115, loss = 0.23329938\n",
      "Iteration 116, loss = 0.23230298\n",
      "Iteration 117, loss = 0.23131294\n",
      "Iteration 118, loss = 0.23032920\n",
      "Iteration 119, loss = 0.22935167\n",
      "Iteration 120, loss = 0.22838030\n",
      "Iteration 121, loss = 0.22741501\n",
      "Iteration 122, loss = 0.22645575\n",
      "Iteration 123, loss = 0.22550245\n",
      "Iteration 124, loss = 0.22455505\n",
      "Iteration 125, loss = 0.22361351\n",
      "Iteration 126, loss = 0.22267776\n",
      "Iteration 127, loss = 0.22174775\n",
      "Iteration 128, loss = 0.22082342\n",
      "Iteration 129, loss = 0.21990474\n",
      "Iteration 130, loss = 0.21899165\n",
      "Iteration 131, loss = 0.21808410\n",
      "Iteration 132, loss = 0.21718204\n",
      "Iteration 133, loss = 0.21628545\n",
      "Iteration 134, loss = 0.21539426\n",
      "Iteration 135, loss = 0.21450844\n",
      "Iteration 136, loss = 0.21362794\n",
      "Iteration 137, loss = 0.21275274\n",
      "Iteration 138, loss = 0.21188278\n",
      "Iteration 139, loss = 0.21101803\n",
      "Iteration 140, loss = 0.21015845\n",
      "Iteration 141, loss = 0.20930401\n",
      "Iteration 142, loss = 0.20845467\n",
      "Iteration 143, loss = 0.20761039\n",
      "Iteration 144, loss = 0.20677114\n",
      "Iteration 145, loss = 0.20593689\n",
      "Iteration 146, loss = 0.20510760\n",
      "Iteration 147, loss = 0.20428325\n",
      "Iteration 148, loss = 0.20346379\n",
      "Iteration 149, loss = 0.20264920\n",
      "Iteration 150, loss = 0.20183945\n",
      "Iteration 151, loss = 0.20103451\n",
      "Iteration 152, loss = 0.20023434\n",
      "Iteration 153, loss = 0.19943893\n",
      "Iteration 154, loss = 0.19864823\n",
      "Iteration 155, loss = 0.19786222\n",
      "Iteration 156, loss = 0.19708087\n",
      "Iteration 157, loss = 0.19630416\n",
      "Iteration 158, loss = 0.19553206\n",
      "Iteration 159, loss = 0.19476454\n",
      "Iteration 160, loss = 0.19400157\n",
      "Iteration 161, loss = 0.19324313\n",
      "Iteration 162, loss = 0.19248919\n",
      "Iteration 163, loss = 0.19173973\n",
      "Iteration 164, loss = 0.19099472\n",
      "Iteration 165, loss = 0.19025413\n",
      "Iteration 166, loss = 0.18951794\n",
      "Iteration 167, loss = 0.18878613\n",
      "Iteration 168, loss = 0.18805867\n",
      "Iteration 169, loss = 0.18733553\n",
      "Iteration 170, loss = 0.18661670\n",
      "Iteration 171, loss = 0.18590214\n",
      "Iteration 172, loss = 0.18519184\n",
      "Iteration 173, loss = 0.18448578\n",
      "Iteration 174, loss = 0.18378392\n",
      "Iteration 175, loss = 0.18308624\n",
      "Iteration 176, loss = 0.18239273\n",
      "Iteration 177, loss = 0.18170335\n",
      "Iteration 178, loss = 0.18101809\n",
      "Iteration 179, loss = 0.18033693\n",
      "Iteration 180, loss = 0.17965984\n",
      "Iteration 181, loss = 0.17898680\n",
      "Iteration 182, loss = 0.17831778\n",
      "Iteration 183, loss = 0.17765277\n",
      "Iteration 184, loss = 0.17699174\n",
      "Iteration 185, loss = 0.17633468\n",
      "Iteration 186, loss = 0.17568155\n",
      "Iteration 187, loss = 0.17503234\n",
      "Iteration 188, loss = 0.17438703\n",
      "Iteration 189, loss = 0.17374560\n",
      "Iteration 190, loss = 0.17310802\n",
      "Iteration 191, loss = 0.17247427\n",
      "Iteration 192, loss = 0.17184434\n",
      "Iteration 193, loss = 0.17121820\n",
      "Iteration 194, loss = 0.17059583\n",
      "Iteration 195, loss = 0.16997721\n",
      "Iteration 196, loss = 0.16936231\n",
      "Iteration 197, loss = 0.16875113\n",
      "Iteration 198, loss = 0.16814363\n",
      "Iteration 199, loss = 0.16753981\n",
      "Iteration 200, loss = 0.16693963\n",
      "Iteration 201, loss = 0.16634308\n",
      "Iteration 202, loss = 0.16575013\n",
      "Iteration 203, loss = 0.16516077\n",
      "Iteration 204, loss = 0.16457498\n",
      "Iteration 205, loss = 0.16399274\n",
      "Iteration 206, loss = 0.16341402\n",
      "Iteration 207, loss = 0.16283881\n",
      "Iteration 208, loss = 0.16226709\n",
      "Iteration 209, loss = 0.16169883\n",
      "Iteration 210, loss = 0.16113403\n",
      "Iteration 211, loss = 0.16057265\n",
      "Iteration 212, loss = 0.16001468\n",
      "Iteration 213, loss = 0.15946010\n",
      "Iteration 214, loss = 0.15890890\n",
      "Iteration 215, loss = 0.15836104\n",
      "Iteration 216, loss = 0.15781651\n",
      "Iteration 217, loss = 0.15727530\n",
      "Iteration 218, loss = 0.15673738\n",
      "Iteration 219, loss = 0.15620274\n",
      "Iteration 220, loss = 0.15567135\n",
      "Iteration 221, loss = 0.15514320\n",
      "Iteration 222, loss = 0.15461827\n",
      "Iteration 223, loss = 0.15409653\n",
      "Iteration 224, loss = 0.15357798\n",
      "Iteration 225, loss = 0.15306259\n",
      "Iteration 226, loss = 0.15255034\n",
      "Iteration 227, loss = 0.15204122\n",
      "Iteration 228, loss = 0.15153520\n",
      "Iteration 229, loss = 0.15103227\n",
      "Iteration 230, loss = 0.15053241\n",
      "Iteration 231, loss = 0.15003560\n",
      "Iteration 232, loss = 0.14954182\n",
      "Iteration 233, loss = 0.14905106\n",
      "Iteration 234, loss = 0.14856330\n",
      "Iteration 235, loss = 0.14807851\n",
      "Iteration 236, loss = 0.14759668\n",
      "Iteration 237, loss = 0.14711780\n",
      "Iteration 238, loss = 0.14664184\n",
      "Iteration 239, loss = 0.14616880\n",
      "Iteration 240, loss = 0.14569864\n",
      "Iteration 241, loss = 0.14523135\n",
      "Iteration 242, loss = 0.14476691\n",
      "Iteration 243, loss = 0.14430532\n",
      "Iteration 244, loss = 0.14384654\n",
      "Iteration 245, loss = 0.14339057\n",
      "Iteration 246, loss = 0.14293738\n",
      "Iteration 247, loss = 0.14248695\n",
      "Iteration 248, loss = 0.14203928\n",
      "Iteration 249, loss = 0.14159435\n",
      "Iteration 250, loss = 0.14115212\n",
      "Iteration 251, loss = 0.14071260\n",
      "Iteration 252, loss = 0.14027576\n",
      "Iteration 253, loss = 0.13984159\n",
      "Iteration 254, loss = 0.13941007\n",
      "Iteration 255, loss = 0.13898118\n",
      "Iteration 256, loss = 0.13855490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.13813123\n",
      "Iteration 258, loss = 0.13771013\n",
      "Iteration 259, loss = 0.13729161\n",
      "Iteration 260, loss = 0.13687563\n",
      "Iteration 261, loss = 0.13646219\n",
      "Iteration 262, loss = 0.13605127\n",
      "Iteration 263, loss = 0.13564285\n",
      "Iteration 264, loss = 0.13523691\n",
      "Iteration 265, loss = 0.13483345\n",
      "Iteration 266, loss = 0.13443244\n",
      "Iteration 267, loss = 0.13403386\n",
      "Iteration 268, loss = 0.13363771\n",
      "Iteration 269, loss = 0.13324397\n",
      "Iteration 270, loss = 0.13285262\n",
      "Iteration 271, loss = 0.13246364\n",
      "Iteration 272, loss = 0.13207703\n",
      "Iteration 273, loss = 0.13169276\n",
      "Iteration 274, loss = 0.13131082\n",
      "Iteration 275, loss = 0.13093120\n",
      "Iteration 276, loss = 0.13055388\n",
      "Iteration 277, loss = 0.13017884\n",
      "Iteration 278, loss = 0.12980607\n",
      "Iteration 279, loss = 0.12943555\n",
      "Iteration 280, loss = 0.12906728\n",
      "Iteration 281, loss = 0.12870123\n",
      "Iteration 282, loss = 0.12833739\n",
      "Iteration 283, loss = 0.12797575\n",
      "Iteration 284, loss = 0.12761629\n",
      "Iteration 285, loss = 0.12725900\n",
      "Iteration 286, loss = 0.12690386\n",
      "Iteration 287, loss = 0.12655087\n",
      "Iteration 288, loss = 0.12619999\n",
      "Iteration 289, loss = 0.12585123\n",
      "Iteration 290, loss = 0.12550456\n",
      "Iteration 291, loss = 0.12515998\n",
      "Iteration 292, loss = 0.12481746\n",
      "Iteration 293, loss = 0.12447700\n",
      "Iteration 294, loss = 0.12413858\n",
      "Iteration 295, loss = 0.12380219\n",
      "Iteration 296, loss = 0.12346781\n",
      "Iteration 297, loss = 0.12313544\n",
      "Iteration 298, loss = 0.12280505\n",
      "Iteration 299, loss = 0.12247664\n",
      "Iteration 300, loss = 0.12215018\n",
      "Iteration 301, loss = 0.12182568\n",
      "Iteration 302, loss = 0.12150311\n",
      "Iteration 303, loss = 0.12118246\n",
      "Iteration 304, loss = 0.12086372\n",
      "Iteration 305, loss = 0.12054687\n",
      "Iteration 306, loss = 0.12023191\n",
      "Iteration 307, loss = 0.11991882\n",
      "Iteration 308, loss = 0.11960759\n",
      "Iteration 309, loss = 0.11929820\n",
      "Iteration 310, loss = 0.11899065\n",
      "Iteration 311, loss = 0.11868491\n",
      "Iteration 312, loss = 0.11838099\n",
      "Iteration 313, loss = 0.11807886\n",
      "Iteration 314, loss = 0.11777851\n",
      "Iteration 315, loss = 0.11747994\n",
      "Iteration 316, loss = 0.11718312\n",
      "Iteration 317, loss = 0.11688806\n",
      "Iteration 318, loss = 0.11659473\n",
      "Iteration 319, loss = 0.11630312\n",
      "Iteration 320, loss = 0.11601323\n",
      "Iteration 321, loss = 0.11572503\n",
      "Iteration 322, loss = 0.11543853\n",
      "Iteration 323, loss = 0.11515370\n",
      "Iteration 324, loss = 0.11487054\n",
      "Iteration 325, loss = 0.11458904\n",
      "Iteration 326, loss = 0.11430918\n",
      "Iteration 327, loss = 0.11403095\n",
      "Iteration 328, loss = 0.11375434\n",
      "Iteration 329, loss = 0.11347935\n",
      "Iteration 330, loss = 0.11320595\n",
      "Iteration 331, loss = 0.11293414\n",
      "Iteration 332, loss = 0.11266391\n",
      "Iteration 333, loss = 0.11239525\n",
      "Iteration 334, loss = 0.11212814\n",
      "Iteration 335, loss = 0.11186258\n",
      "Iteration 336, loss = 0.11159856\n",
      "Iteration 337, loss = 0.11133606\n",
      "Iteration 338, loss = 0.11107507\n",
      "Iteration 339, loss = 0.11081559\n",
      "Iteration 340, loss = 0.11055760\n",
      "Iteration 341, loss = 0.11030110\n",
      "Iteration 342, loss = 0.11004607\n",
      "Iteration 343, loss = 0.10979250\n",
      "Iteration 344, loss = 0.10954039\n",
      "Iteration 345, loss = 0.10928972\n",
      "Iteration 346, loss = 0.10904048\n",
      "Iteration 347, loss = 0.10879267\n",
      "Iteration 348, loss = 0.10854627\n",
      "Iteration 349, loss = 0.10830128\n",
      "Iteration 350, loss = 0.10805768\n",
      "Iteration 351, loss = 0.10781547\n",
      "Iteration 352, loss = 0.10757463\n",
      "Iteration 353, loss = 0.10733516\n",
      "Iteration 354, loss = 0.10709705\n",
      "Iteration 355, loss = 0.10686028\n",
      "Iteration 356, loss = 0.10662486\n",
      "Iteration 357, loss = 0.10639076\n",
      "Iteration 358, loss = 0.10615799\n",
      "Iteration 359, loss = 0.10592653\n",
      "Iteration 360, loss = 0.10569637\n",
      "Iteration 361, loss = 0.10546750\n",
      "Iteration 362, loss = 0.10523992\n",
      "Iteration 363, loss = 0.10501362\n",
      "Iteration 364, loss = 0.10478859\n",
      "Iteration 365, loss = 0.10456481\n",
      "Iteration 366, loss = 0.10434228\n",
      "Iteration 367, loss = 0.10412100\n",
      "Iteration 368, loss = 0.10390095\n",
      "Iteration 369, loss = 0.10368213\n",
      "Iteration 370, loss = 0.10346452\n",
      "Iteration 371, loss = 0.10324812\n",
      "Iteration 372, loss = 0.10303293\n",
      "Iteration 373, loss = 0.10281892\n",
      "Iteration 374, loss = 0.10260610\n",
      "Iteration 375, loss = 0.10239446\n",
      "Iteration 376, loss = 0.10218399\n",
      "Iteration 377, loss = 0.10197467\n",
      "Iteration 378, loss = 0.10176651\n",
      "Iteration 379, loss = 0.10155949\n",
      "Iteration 380, loss = 0.10135361\n",
      "Iteration 381, loss = 0.10114887\n",
      "Iteration 382, loss = 0.10094524\n",
      "Iteration 383, loss = 0.10074272\n",
      "Iteration 384, loss = 0.10054132\n",
      "Iteration 385, loss = 0.10034101\n",
      "Iteration 386, loss = 0.10014180\n",
      "Iteration 387, loss = 0.09994367\n",
      "Iteration 388, loss = 0.09974661\n",
      "Iteration 389, loss = 0.09955063\n",
      "Iteration 390, loss = 0.09935571\n",
      "Iteration 391, loss = 0.09916185\n",
      "Iteration 392, loss = 0.09896903\n",
      "Iteration 393, loss = 0.09877726\n",
      "Iteration 394, loss = 0.09858652\n",
      "Iteration 395, loss = 0.09839681\n",
      "Iteration 396, loss = 0.09820812\n",
      "Iteration 397, loss = 0.09802044\n",
      "Iteration 398, loss = 0.09783378\n",
      "Iteration 399, loss = 0.09764811\n",
      "Iteration 400, loss = 0.09746343\n",
      "Iteration 401, loss = 0.09727975\n",
      "Iteration 402, loss = 0.09709704\n",
      "Iteration 403, loss = 0.09691531\n",
      "Iteration 404, loss = 0.09673455\n",
      "Iteration 405, loss = 0.09655474\n",
      "Iteration 406, loss = 0.09637590\n",
      "Iteration 407, loss = 0.09619800\n",
      "Iteration 408, loss = 0.09602104\n",
      "Iteration 409, loss = 0.09584502\n",
      "Iteration 410, loss = 0.09566993\n",
      "Iteration 411, loss = 0.09549576\n",
      "Iteration 412, loss = 0.09532251\n",
      "Iteration 413, loss = 0.09515017\n",
      "Iteration 414, loss = 0.09497874\n",
      "Iteration 415, loss = 0.09480821\n",
      "Iteration 416, loss = 0.09463857\n",
      "Iteration 417, loss = 0.09446981\n",
      "Iteration 418, loss = 0.09430194\n",
      "Iteration 419, loss = 0.09413495\n",
      "Iteration 420, loss = 0.09396882\n",
      "Iteration 421, loss = 0.09380356\n",
      "Iteration 422, loss = 0.09363916\n",
      "Iteration 423, loss = 0.09347562\n",
      "Iteration 424, loss = 0.09331292\n",
      "Iteration 425, loss = 0.09315106\n",
      "Iteration 426, loss = 0.09299004\n",
      "Iteration 427, loss = 0.09282985\n",
      "Iteration 428, loss = 0.09267048\n",
      "Iteration 429, loss = 0.09251194\n",
      "Iteration 430, loss = 0.09235421\n",
      "Iteration 431, loss = 0.09219729\n",
      "Iteration 432, loss = 0.09204117\n",
      "Iteration 433, loss = 0.09188585\n",
      "Iteration 434, loss = 0.09173133\n",
      "Iteration 435, loss = 0.09157760\n",
      "Iteration 436, loss = 0.09142465\n",
      "Iteration 437, loss = 0.09127248\n",
      "Iteration 438, loss = 0.09112108\n",
      "Iteration 439, loss = 0.09097045\n",
      "Iteration 440, loss = 0.09082058\n",
      "Iteration 441, loss = 0.09067148\n",
      "Iteration 442, loss = 0.09052312\n",
      "Iteration 443, loss = 0.09037552\n",
      "Iteration 444, loss = 0.09022866\n",
      "Iteration 445, loss = 0.09008254\n",
      "Iteration 446, loss = 0.08993715\n",
      "Iteration 447, loss = 0.08979249\n",
      "Iteration 448, loss = 0.08964856\n",
      "Iteration 449, loss = 0.08950535\n",
      "Iteration 450, loss = 0.08936285\n",
      "Iteration 451, loss = 0.08922107\n",
      "Iteration 452, loss = 0.08907999\n",
      "Iteration 453, loss = 0.08893962\n",
      "Iteration 454, loss = 0.08879994\n",
      "Iteration 455, loss = 0.08866096\n",
      "Iteration 456, loss = 0.08852266\n",
      "Iteration 457, loss = 0.08838505\n",
      "Iteration 458, loss = 0.08824812\n",
      "Iteration 459, loss = 0.08811187\n",
      "Iteration 460, loss = 0.08797629\n",
      "Iteration 461, loss = 0.08784137\n",
      "Iteration 462, loss = 0.08770712\n",
      "Iteration 463, loss = 0.08757353\n",
      "Iteration 464, loss = 0.08744059\n",
      "Iteration 465, loss = 0.08730830\n",
      "Iteration 466, loss = 0.08717666\n",
      "Iteration 467, loss = 0.08704566\n",
      "Iteration 468, loss = 0.08691530\n",
      "Iteration 469, loss = 0.08678558\n",
      "Iteration 470, loss = 0.08665648\n",
      "Iteration 471, loss = 0.08652801\n",
      "Iteration 472, loss = 0.08640017\n",
      "Iteration 473, loss = 0.08627294\n",
      "Iteration 474, loss = 0.08614633\n",
      "Iteration 475, loss = 0.08602033\n",
      "Iteration 476, loss = 0.08589493\n",
      "Iteration 477, loss = 0.08577014\n",
      "Iteration 478, loss = 0.08564595\n",
      "Iteration 479, loss = 0.08552236\n",
      "Iteration 480, loss = 0.08539935\n",
      "Iteration 481, loss = 0.08527694\n",
      "Iteration 482, loss = 0.08515511\n",
      "Iteration 483, loss = 0.08503386\n",
      "Iteration 484, loss = 0.08491319\n",
      "Iteration 485, loss = 0.08479310\n",
      "Iteration 486, loss = 0.08467357\n",
      "Iteration 487, loss = 0.08455461\n",
      "Iteration 488, loss = 0.08443622\n",
      "Iteration 489, loss = 0.08431838\n",
      "Iteration 490, loss = 0.08420111\n",
      "Iteration 491, loss = 0.08408438\n",
      "Iteration 492, loss = 0.08396821\n",
      "Iteration 493, loss = 0.08385258\n",
      "Iteration 494, loss = 0.08373750\n",
      "Iteration 495, loss = 0.08362295\n",
      "Iteration 496, loss = 0.08350894\n",
      "Iteration 497, loss = 0.08339547\n",
      "Iteration 498, loss = 0.08328252\n",
      "Iteration 499, loss = 0.08317011\n",
      "Iteration 500, loss = 0.08305821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06664509\n",
      "Iteration 2, loss = 1.04734649\n",
      "Iteration 3, loss = 1.02075307\n",
      "Iteration 4, loss = 0.98850714\n",
      "Iteration 5, loss = 0.95220859\n",
      "Iteration 6, loss = 0.91335318\n",
      "Iteration 7, loss = 0.87328284\n",
      "Iteration 8, loss = 0.83314919\n",
      "Iteration 9, loss = 0.79389163\n",
      "Iteration 10, loss = 0.75623083\n",
      "Iteration 11, loss = 0.72067695\n",
      "Iteration 12, loss = 0.68754999\n",
      "Iteration 13, loss = 0.65700840\n",
      "Iteration 14, loss = 0.62908164\n",
      "Iteration 15, loss = 0.60370291\n",
      "Iteration 16, loss = 0.58073885\n",
      "Iteration 17, loss = 0.56001481\n",
      "Iteration 18, loss = 0.54133475\n",
      "Iteration 19, loss = 0.52449613\n",
      "Iteration 20, loss = 0.50930029\n",
      "Iteration 21, loss = 0.49555922\n",
      "Iteration 22, loss = 0.48309953\n",
      "Iteration 23, loss = 0.47176443\n",
      "Iteration 24, loss = 0.46141425\n",
      "Iteration 25, loss = 0.45192605\n",
      "Iteration 26, loss = 0.44319270\n",
      "Iteration 27, loss = 0.43512152\n",
      "Iteration 28, loss = 0.42763289\n",
      "Iteration 29, loss = 0.42065866\n",
      "Iteration 30, loss = 0.41414075\n",
      "Iteration 31, loss = 0.40802972\n",
      "Iteration 32, loss = 0.40228349\n",
      "Iteration 33, loss = 0.39686619\n",
      "Iteration 34, loss = 0.39174712\n",
      "Iteration 35, loss = 0.38689987\n",
      "Iteration 36, loss = 0.38230154\n",
      "Iteration 37, loss = 0.37793212\n",
      "Iteration 38, loss = 0.37377391\n",
      "Iteration 39, loss = 0.36981116\n",
      "Iteration 40, loss = 0.36602964\n",
      "Iteration 41, loss = 0.36241643\n",
      "Iteration 42, loss = 0.35895967\n",
      "Iteration 43, loss = 0.35564842\n",
      "Iteration 44, loss = 0.35247254\n",
      "Iteration 45, loss = 0.34942257\n",
      "Iteration 46, loss = 0.34648973\n",
      "Iteration 47, loss = 0.34366580\n",
      "Iteration 48, loss = 0.34094314\n",
      "Iteration 49, loss = 0.33831463\n",
      "Iteration 50, loss = 0.33577365\n",
      "Iteration 51, loss = 0.33331409\n",
      "Iteration 52, loss = 0.33093028\n",
      "Iteration 53, loss = 0.32861700\n",
      "Iteration 54, loss = 0.32636946\n",
      "Iteration 55, loss = 0.32418325\n",
      "Iteration 56, loss = 0.32205433\n",
      "Iteration 57, loss = 0.31997902\n",
      "Iteration 58, loss = 0.31795395\n",
      "Iteration 59, loss = 0.31597605\n",
      "Iteration 60, loss = 0.31404249\n",
      "Iteration 61, loss = 0.31215073\n",
      "Iteration 62, loss = 0.31029840\n",
      "Iteration 63, loss = 0.30848335\n",
      "Iteration 64, loss = 0.30670362\n",
      "Iteration 65, loss = 0.30495738\n",
      "Iteration 66, loss = 0.30324295\n",
      "Iteration 67, loss = 0.30155879\n",
      "Iteration 68, loss = 0.29990345\n",
      "Iteration 69, loss = 0.29827559\n",
      "Iteration 70, loss = 0.29667397\n",
      "Iteration 71, loss = 0.29509743\n",
      "Iteration 72, loss = 0.29354486\n",
      "Iteration 73, loss = 0.29201525\n",
      "Iteration 74, loss = 0.29050763\n",
      "Iteration 75, loss = 0.28902111\n",
      "Iteration 76, loss = 0.28755483\n",
      "Iteration 77, loss = 0.28610799\n",
      "Iteration 78, loss = 0.28467984\n",
      "Iteration 79, loss = 0.28326969\n",
      "Iteration 80, loss = 0.28187685\n",
      "Iteration 81, loss = 0.28050072\n",
      "Iteration 82, loss = 0.27914069\n",
      "Iteration 83, loss = 0.27779622\n",
      "Iteration 84, loss = 0.27646680\n",
      "Iteration 85, loss = 0.27515192\n",
      "Iteration 86, loss = 0.27385115\n",
      "Iteration 87, loss = 0.27256404\n",
      "Iteration 88, loss = 0.27129019\n",
      "Iteration 89, loss = 0.27002923\n",
      "Iteration 90, loss = 0.26878080\n",
      "Iteration 91, loss = 0.26754456\n",
      "Iteration 92, loss = 0.26632019\n",
      "Iteration 93, loss = 0.26510740\n",
      "Iteration 94, loss = 0.26390590\n",
      "Iteration 95, loss = 0.26271543\n",
      "Iteration 96, loss = 0.26153573\n",
      "Iteration 97, loss = 0.26036657\n",
      "Iteration 98, loss = 0.25920772\n",
      "Iteration 99, loss = 0.25805896\n",
      "Iteration 100, loss = 0.25692009\n",
      "Iteration 101, loss = 0.25579092\n",
      "Iteration 102, loss = 0.25467125\n",
      "Iteration 103, loss = 0.25356091\n",
      "Iteration 104, loss = 0.25245973\n",
      "Iteration 105, loss = 0.25136755\n",
      "Iteration 106, loss = 0.25028421\n",
      "Iteration 107, loss = 0.24920956\n",
      "Iteration 108, loss = 0.24814346\n",
      "Iteration 109, loss = 0.24708578\n",
      "Iteration 110, loss = 0.24603637\n",
      "Iteration 111, loss = 0.24499512\n",
      "Iteration 112, loss = 0.24396191\n",
      "Iteration 113, loss = 0.24293661\n",
      "Iteration 114, loss = 0.24191911\n",
      "Iteration 115, loss = 0.24090930\n",
      "Iteration 116, loss = 0.23990709\n",
      "Iteration 117, loss = 0.23891236\n",
      "Iteration 118, loss = 0.23792502\n",
      "Iteration 119, loss = 0.23694498\n",
      "Iteration 120, loss = 0.23597215\n",
      "Iteration 121, loss = 0.23500643\n",
      "Iteration 122, loss = 0.23404775\n",
      "Iteration 123, loss = 0.23309601\n",
      "Iteration 124, loss = 0.23215114\n",
      "Iteration 125, loss = 0.23121305\n",
      "Iteration 126, loss = 0.23028169\n",
      "Iteration 127, loss = 0.22935696\n",
      "Iteration 128, loss = 0.22843880\n",
      "Iteration 129, loss = 0.22752714\n",
      "Iteration 130, loss = 0.22662191\n",
      "Iteration 131, loss = 0.22572304\n",
      "Iteration 132, loss = 0.22483047\n",
      "Iteration 133, loss = 0.22394414\n",
      "Iteration 134, loss = 0.22306398\n",
      "Iteration 135, loss = 0.22218994\n",
      "Iteration 136, loss = 0.22132195\n",
      "Iteration 137, loss = 0.22045996\n",
      "Iteration 138, loss = 0.21960391\n",
      "Iteration 139, loss = 0.21875374\n",
      "Iteration 140, loss = 0.21790941\n",
      "Iteration 141, loss = 0.21707085\n",
      "Iteration 142, loss = 0.21623802\n",
      "Iteration 143, loss = 0.21541086\n",
      "Iteration 144, loss = 0.21458933\n",
      "Iteration 145, loss = 0.21377337\n",
      "Iteration 146, loss = 0.21296294\n",
      "Iteration 147, loss = 0.21215799\n",
      "Iteration 148, loss = 0.21135847\n",
      "Iteration 149, loss = 0.21056434\n",
      "Iteration 150, loss = 0.20977554\n",
      "Iteration 151, loss = 0.20899205\n",
      "Iteration 152, loss = 0.20821380\n",
      "Iteration 153, loss = 0.20744077\n",
      "Iteration 154, loss = 0.20667289\n",
      "Iteration 155, loss = 0.20591015\n",
      "Iteration 156, loss = 0.20515248\n",
      "Iteration 157, loss = 0.20439986\n",
      "Iteration 158, loss = 0.20365224\n",
      "Iteration 159, loss = 0.20290958\n",
      "Iteration 160, loss = 0.20217185\n",
      "Iteration 161, loss = 0.20143899\n",
      "Iteration 162, loss = 0.20071099\n",
      "Iteration 163, loss = 0.19998779\n",
      "Iteration 164, loss = 0.19926936\n",
      "Iteration 165, loss = 0.19855566\n",
      "Iteration 166, loss = 0.19784666\n",
      "Iteration 167, loss = 0.19714232\n",
      "Iteration 168, loss = 0.19644261\n",
      "Iteration 169, loss = 0.19574749\n",
      "Iteration 170, loss = 0.19505692\n",
      "Iteration 171, loss = 0.19437088\n",
      "Iteration 172, loss = 0.19368932\n",
      "Iteration 173, loss = 0.19301222\n",
      "Iteration 174, loss = 0.19233953\n",
      "Iteration 175, loss = 0.19167123\n",
      "Iteration 176, loss = 0.19100729\n",
      "Iteration 177, loss = 0.19034767\n",
      "Iteration 178, loss = 0.18969234\n",
      "Iteration 179, loss = 0.18904126\n",
      "Iteration 180, loss = 0.18839442\n",
      "Iteration 181, loss = 0.18775177\n",
      "Iteration 182, loss = 0.18711328\n",
      "Iteration 183, loss = 0.18647893\n",
      "Iteration 184, loss = 0.18584869\n",
      "Iteration 185, loss = 0.18522251\n",
      "Iteration 186, loss = 0.18460039\n",
      "Iteration 187, loss = 0.18398228\n",
      "Iteration 188, loss = 0.18336815\n",
      "Iteration 189, loss = 0.18275798\n",
      "Iteration 190, loss = 0.18215174\n",
      "Iteration 191, loss = 0.18154940\n",
      "Iteration 192, loss = 0.18095093\n",
      "Iteration 193, loss = 0.18035631\n",
      "Iteration 194, loss = 0.17976550\n",
      "Iteration 195, loss = 0.17917849\n",
      "Iteration 196, loss = 0.17859523\n",
      "Iteration 197, loss = 0.17801571\n",
      "Iteration 198, loss = 0.17743990\n",
      "Iteration 199, loss = 0.17686776\n",
      "Iteration 200, loss = 0.17629929\n",
      "Iteration 201, loss = 0.17573444\n",
      "Iteration 202, loss = 0.17517319\n",
      "Iteration 203, loss = 0.17461552\n",
      "Iteration 204, loss = 0.17406141\n",
      "Iteration 205, loss = 0.17351082\n",
      "Iteration 206, loss = 0.17296373\n",
      "Iteration 207, loss = 0.17242011\n",
      "Iteration 208, loss = 0.17187995\n",
      "Iteration 209, loss = 0.17134322\n",
      "Iteration 210, loss = 0.17080989\n",
      "Iteration 211, loss = 0.17027993\n",
      "Iteration 212, loss = 0.16975333\n",
      "Iteration 213, loss = 0.16923006\n",
      "Iteration 214, loss = 0.16871010\n",
      "Iteration 215, loss = 0.16819342\n",
      "Iteration 216, loss = 0.16768000\n",
      "Iteration 217, loss = 0.16716982\n",
      "Iteration 218, loss = 0.16666285\n",
      "Iteration 219, loss = 0.16615907\n",
      "Iteration 220, loss = 0.16565846\n",
      "Iteration 221, loss = 0.16516100\n",
      "Iteration 222, loss = 0.16466666\n",
      "Iteration 223, loss = 0.16417542\n",
      "Iteration 224, loss = 0.16368726\n",
      "Iteration 225, loss = 0.16320216\n",
      "Iteration 226, loss = 0.16272009\n",
      "Iteration 227, loss = 0.16224104\n",
      "Iteration 228, loss = 0.16176498\n",
      "Iteration 229, loss = 0.16129189\n",
      "Iteration 230, loss = 0.16082175\n",
      "Iteration 231, loss = 0.16035455\n",
      "Iteration 232, loss = 0.15989025\n",
      "Iteration 233, loss = 0.15942884\n",
      "Iteration 234, loss = 0.15897030\n",
      "Iteration 235, loss = 0.15851460\n",
      "Iteration 236, loss = 0.15806173\n",
      "Iteration 237, loss = 0.15761167\n",
      "Iteration 238, loss = 0.15716440\n",
      "Iteration 239, loss = 0.15671989\n",
      "Iteration 240, loss = 0.15627814\n",
      "Iteration 241, loss = 0.15583911\n",
      "Iteration 242, loss = 0.15540279\n",
      "Iteration 243, loss = 0.15496916\n",
      "Iteration 244, loss = 0.15453820\n",
      "Iteration 245, loss = 0.15410989\n",
      "Iteration 246, loss = 0.15368421\n",
      "Iteration 247, loss = 0.15326115\n",
      "Iteration 248, loss = 0.15284069\n",
      "Iteration 249, loss = 0.15242280\n",
      "Iteration 250, loss = 0.15200747\n",
      "Iteration 251, loss = 0.15159469\n",
      "Iteration 252, loss = 0.15118442\n",
      "Iteration 253, loss = 0.15077666\n",
      "Iteration 254, loss = 0.15037139\n",
      "Iteration 255, loss = 0.14996858\n",
      "Iteration 256, loss = 0.14956823\n",
      "Iteration 257, loss = 0.14917032\n",
      "Iteration 258, loss = 0.14877482\n",
      "Iteration 259, loss = 0.14838172\n",
      "Iteration 260, loss = 0.14799100\n",
      "Iteration 261, loss = 0.14760265\n",
      "Iteration 262, loss = 0.14721665\n",
      "Iteration 263, loss = 0.14683298\n",
      "Iteration 264, loss = 0.14645162\n",
      "Iteration 265, loss = 0.14607257\n",
      "Iteration 266, loss = 0.14569580\n",
      "Iteration 267, loss = 0.14532129\n",
      "Iteration 268, loss = 0.14494904\n",
      "Iteration 269, loss = 0.14457902\n",
      "Iteration 270, loss = 0.14421122\n",
      "Iteration 271, loss = 0.14384563\n",
      "Iteration 272, loss = 0.14348222\n",
      "Iteration 273, loss = 0.14312098\n",
      "Iteration 274, loss = 0.14276190\n",
      "Iteration 275, loss = 0.14240496\n",
      "Iteration 276, loss = 0.14205015\n",
      "Iteration 277, loss = 0.14169744\n",
      "Iteration 278, loss = 0.14134684\n",
      "Iteration 279, loss = 0.14099831\n",
      "Iteration 280, loss = 0.14065185\n",
      "Iteration 281, loss = 0.14030744\n",
      "Iteration 282, loss = 0.13996507\n",
      "Iteration 283, loss = 0.13962472\n",
      "Iteration 284, loss = 0.13928638\n",
      "Iteration 285, loss = 0.13895003\n",
      "Iteration 286, loss = 0.13861566\n",
      "Iteration 287, loss = 0.13828326\n",
      "Iteration 288, loss = 0.13795281\n",
      "Iteration 289, loss = 0.13762429\n",
      "Iteration 290, loss = 0.13729770\n",
      "Iteration 291, loss = 0.13697302\n",
      "Iteration 292, loss = 0.13665024\n",
      "Iteration 293, loss = 0.13632934\n",
      "Iteration 294, loss = 0.13601031\n",
      "Iteration 295, loss = 0.13569313\n",
      "Iteration 296, loss = 0.13537780\n",
      "Iteration 297, loss = 0.13506430\n",
      "Iteration 298, loss = 0.13475262\n",
      "Iteration 299, loss = 0.13444274\n",
      "Iteration 300, loss = 0.13413465\n",
      "Iteration 301, loss = 0.13382835\n",
      "Iteration 302, loss = 0.13352380\n",
      "Iteration 303, loss = 0.13322101\n",
      "Iteration 304, loss = 0.13291997\n",
      "Iteration 305, loss = 0.13262065\n",
      "Iteration 306, loss = 0.13232305\n",
      "Iteration 307, loss = 0.13202715\n",
      "Iteration 308, loss = 0.13173295\n",
      "Iteration 309, loss = 0.13144042\n",
      "Iteration 310, loss = 0.13114957\n",
      "Iteration 311, loss = 0.13086037\n",
      "Iteration 312, loss = 0.13057282\n",
      "Iteration 313, loss = 0.13028690\n",
      "Iteration 314, loss = 0.13000260\n",
      "Iteration 315, loss = 0.12971991\n",
      "Iteration 316, loss = 0.12943882\n",
      "Iteration 317, loss = 0.12915932\n",
      "Iteration 318, loss = 0.12888140\n",
      "Iteration 319, loss = 0.12860504\n",
      "Iteration 320, loss = 0.12833023\n",
      "Iteration 321, loss = 0.12805697\n",
      "Iteration 322, loss = 0.12778524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 323, loss = 0.12751504\n",
      "Iteration 324, loss = 0.12724634\n",
      "Iteration 325, loss = 0.12697915\n",
      "Iteration 326, loss = 0.12671344\n",
      "Iteration 327, loss = 0.12644922\n",
      "Iteration 328, loss = 0.12618646\n",
      "Iteration 329, loss = 0.12592516\n",
      "Iteration 330, loss = 0.12566531\n",
      "Iteration 331, loss = 0.12540690\n",
      "Iteration 332, loss = 0.12514992\n",
      "Iteration 333, loss = 0.12489436\n",
      "Iteration 334, loss = 0.12464020\n",
      "Iteration 335, loss = 0.12438744\n",
      "Iteration 336, loss = 0.12413607\n",
      "Iteration 337, loss = 0.12388608\n",
      "Iteration 338, loss = 0.12363746\n",
      "Iteration 339, loss = 0.12339020\n",
      "Iteration 340, loss = 0.12314429\n",
      "Iteration 341, loss = 0.12289972\n",
      "Iteration 342, loss = 0.12265648\n",
      "Iteration 343, loss = 0.12241456\n",
      "Iteration 344, loss = 0.12217396\n",
      "Iteration 345, loss = 0.12193466\n",
      "Iteration 346, loss = 0.12169665\n",
      "Iteration 347, loss = 0.12145993\n",
      "Iteration 348, loss = 0.12122449\n",
      "Iteration 349, loss = 0.12099031\n",
      "Iteration 350, loss = 0.12075740\n",
      "Iteration 351, loss = 0.12052573\n",
      "Iteration 352, loss = 0.12029531\n",
      "Iteration 353, loss = 0.12006611\n",
      "Iteration 354, loss = 0.11983815\n",
      "Iteration 355, loss = 0.11961140\n",
      "Iteration 356, loss = 0.11938585\n",
      "Iteration 357, loss = 0.11916151\n",
      "Iteration 358, loss = 0.11893836\n",
      "Iteration 359, loss = 0.11871639\n",
      "Iteration 360, loss = 0.11849559\n",
      "Iteration 361, loss = 0.11827596\n",
      "Iteration 362, loss = 0.11805749\n",
      "Iteration 363, loss = 0.11784017\n",
      "Iteration 364, loss = 0.11762400\n",
      "Iteration 365, loss = 0.11740896\n",
      "Iteration 366, loss = 0.11719504\n",
      "Iteration 367, loss = 0.11698224\n",
      "Iteration 368, loss = 0.11677056\n",
      "Iteration 369, loss = 0.11655998\n",
      "Iteration 370, loss = 0.11635050\n",
      "Iteration 371, loss = 0.11614210\n",
      "Iteration 372, loss = 0.11593479\n",
      "Iteration 373, loss = 0.11572855\n",
      "Iteration 374, loss = 0.11552338\n",
      "Iteration 375, loss = 0.11531926\n",
      "Iteration 376, loss = 0.11511620\n",
      "Iteration 377, loss = 0.11491419\n",
      "Iteration 378, loss = 0.11471321\n",
      "Iteration 379, loss = 0.11451326\n",
      "Iteration 380, loss = 0.11431434\n",
      "Iteration 381, loss = 0.11411644\n",
      "Iteration 382, loss = 0.11391954\n",
      "Iteration 383, loss = 0.11372365\n",
      "Iteration 384, loss = 0.11352875\n",
      "Iteration 385, loss = 0.11333485\n",
      "Iteration 386, loss = 0.11314193\n",
      "Iteration 387, loss = 0.11294998\n",
      "Iteration 388, loss = 0.11275900\n",
      "Iteration 389, loss = 0.11256899\n",
      "Iteration 390, loss = 0.11237994\n",
      "Iteration 391, loss = 0.11219183\n",
      "Iteration 392, loss = 0.11200467\n",
      "Iteration 393, loss = 0.11181845\n",
      "Iteration 394, loss = 0.11163316\n",
      "Iteration 395, loss = 0.11144879\n",
      "Iteration 396, loss = 0.11126534\n",
      "Iteration 397, loss = 0.11108281\n",
      "Iteration 398, loss = 0.11090118\n",
      "Iteration 399, loss = 0.11072045\n",
      "Iteration 400, loss = 0.11054062\n",
      "Iteration 401, loss = 0.11036168\n",
      "Iteration 402, loss = 0.11018362\n",
      "Iteration 403, loss = 0.11000644\n",
      "Iteration 404, loss = 0.10983013\n",
      "Iteration 405, loss = 0.10965468\n",
      "Iteration 406, loss = 0.10948009\n",
      "Iteration 407, loss = 0.10930636\n",
      "Iteration 408, loss = 0.10913348\n",
      "Iteration 409, loss = 0.10896144\n",
      "Iteration 410, loss = 0.10879023\n",
      "Iteration 411, loss = 0.10861986\n",
      "Iteration 412, loss = 0.10845032\n",
      "Iteration 413, loss = 0.10828159\n",
      "Iteration 414, loss = 0.10811368\n",
      "Iteration 415, loss = 0.10794658\n",
      "Iteration 416, loss = 0.10778029\n",
      "Iteration 417, loss = 0.10761479\n",
      "Iteration 418, loss = 0.10745009\n",
      "Iteration 419, loss = 0.10728618\n",
      "Iteration 420, loss = 0.10712306\n",
      "Iteration 421, loss = 0.10696071\n",
      "Iteration 422, loss = 0.10679913\n",
      "Iteration 423, loss = 0.10663833\n",
      "Iteration 424, loss = 0.10647829\n",
      "Iteration 425, loss = 0.10631900\n",
      "Iteration 426, loss = 0.10616047\n",
      "Iteration 427, loss = 0.10600269\n",
      "Iteration 428, loss = 0.10584566\n",
      "Iteration 429, loss = 0.10568936\n",
      "Iteration 430, loss = 0.10553380\n",
      "Iteration 431, loss = 0.10537897\n",
      "Iteration 432, loss = 0.10522486\n",
      "Iteration 433, loss = 0.10507148\n",
      "Iteration 434, loss = 0.10491881\n",
      "Iteration 435, loss = 0.10476685\n",
      "Iteration 436, loss = 0.10461560\n",
      "Iteration 437, loss = 0.10446505\n",
      "Iteration 438, loss = 0.10431519\n",
      "Iteration 439, loss = 0.10416604\n",
      "Iteration 440, loss = 0.10401757\n",
      "Iteration 441, loss = 0.10386978\n",
      "Iteration 442, loss = 0.10372268\n",
      "Iteration 443, loss = 0.10357625\n",
      "Iteration 444, loss = 0.10343049\n",
      "Iteration 445, loss = 0.10328540\n",
      "Iteration 446, loss = 0.10314097\n",
      "Iteration 447, loss = 0.10299720\n",
      "Iteration 448, loss = 0.10285409\n",
      "Iteration 449, loss = 0.10271162\n",
      "Iteration 450, loss = 0.10256981\n",
      "Iteration 451, loss = 0.10242863\n",
      "Iteration 452, loss = 0.10228810\n",
      "Iteration 453, loss = 0.10214819\n",
      "Iteration 454, loss = 0.10200892\n",
      "Iteration 455, loss = 0.10187028\n",
      "Iteration 456, loss = 0.10173225\n",
      "Iteration 457, loss = 0.10159485\n",
      "Iteration 458, loss = 0.10145806\n",
      "Iteration 459, loss = 0.10132188\n",
      "Iteration 460, loss = 0.10118631\n",
      "Iteration 461, loss = 0.10105134\n",
      "Iteration 462, loss = 0.10091697\n",
      "Iteration 463, loss = 0.10078320\n",
      "Iteration 464, loss = 0.10065002\n",
      "Iteration 465, loss = 0.10051743\n",
      "Iteration 466, loss = 0.10038542\n",
      "Iteration 467, loss = 0.10025400\n",
      "Iteration 468, loss = 0.10012315\n",
      "Iteration 469, loss = 0.09999287\n",
      "Iteration 470, loss = 0.09986317\n",
      "Iteration 471, loss = 0.09973403\n",
      "Iteration 472, loss = 0.09960546\n",
      "Iteration 473, loss = 0.09947745\n",
      "Iteration 474, loss = 0.09934999\n",
      "Iteration 475, loss = 0.09922309\n",
      "Iteration 476, loss = 0.09909674\n",
      "Iteration 477, loss = 0.09897093\n",
      "Iteration 478, loss = 0.09884567\n",
      "Iteration 479, loss = 0.09872094\n",
      "Iteration 480, loss = 0.09859676\n",
      "Iteration 481, loss = 0.09847310\n",
      "Iteration 482, loss = 0.09834998\n",
      "Iteration 483, loss = 0.09822739\n",
      "Iteration 484, loss = 0.09810531\n",
      "Iteration 485, loss = 0.09798376\n",
      "Iteration 486, loss = 0.09786273\n",
      "Iteration 487, loss = 0.09774221\n",
      "Iteration 488, loss = 0.09762220\n",
      "Iteration 489, loss = 0.09750270\n",
      "Iteration 490, loss = 0.09738371\n",
      "Iteration 491, loss = 0.09726521\n",
      "Iteration 492, loss = 0.09714722\n",
      "Iteration 493, loss = 0.09702972\n",
      "Iteration 494, loss = 0.09691272\n",
      "Iteration 495, loss = 0.09679620\n",
      "Iteration 496, loss = 0.09668018\n",
      "Iteration 497, loss = 0.09656463\n",
      "Iteration 498, loss = 0.09644957\n",
      "Iteration 499, loss = 0.09633499\n",
      "Iteration 500, loss = 0.09622088\n",
      "Iteration 1, loss = 1.08919085\n",
      "Iteration 2, loss = 1.06985888\n",
      "Iteration 3, loss = 1.04319331\n",
      "Iteration 4, loss = 1.01081810\n",
      "Iteration 5, loss = 0.97431734\n",
      "Iteration 6, loss = 0.93517748\n",
      "Iteration 7, loss = 0.89474088\n",
      "Iteration 8, loss = 0.85417044\n",
      "Iteration 9, loss = 0.81442648\n",
      "Iteration 10, loss = 0.77625684\n",
      "Iteration 11, loss = 0.74020046\n",
      "Iteration 12, loss = 0.70660328\n",
      "Iteration 13, loss = 0.67564351\n",
      "Iteration 14, loss = 0.64736279\n",
      "Iteration 15, loss = 0.62169889\n",
      "Iteration 16, loss = 0.59851690\n",
      "Iteration 17, loss = 0.57763617\n",
      "Iteration 18, loss = 0.55885224\n",
      "Iteration 19, loss = 0.54195318\n",
      "Iteration 20, loss = 0.52673117\n",
      "Iteration 21, loss = 0.51298989\n",
      "Iteration 22, loss = 0.50054883\n",
      "Iteration 23, loss = 0.48924531\n",
      "Iteration 24, loss = 0.47893494\n",
      "Iteration 25, loss = 0.46949107\n",
      "Iteration 26, loss = 0.46080361\n",
      "Iteration 27, loss = 0.45277764\n",
      "Iteration 28, loss = 0.44533172\n",
      "Iteration 29, loss = 0.43839633\n",
      "Iteration 30, loss = 0.43191231\n",
      "Iteration 31, loss = 0.42582940\n",
      "Iteration 32, loss = 0.42010492\n",
      "Iteration 33, loss = 0.41470259\n",
      "Iteration 34, loss = 0.40959145\n",
      "Iteration 35, loss = 0.40474502\n",
      "Iteration 36, loss = 0.40014042\n",
      "Iteration 37, loss = 0.39575779\n",
      "Iteration 38, loss = 0.39157970\n",
      "Iteration 39, loss = 0.38759070\n",
      "Iteration 40, loss = 0.38377694\n",
      "Iteration 41, loss = 0.38012591\n",
      "Iteration 42, loss = 0.37662619\n",
      "Iteration 43, loss = 0.37326726\n",
      "Iteration 44, loss = 0.37003941\n",
      "Iteration 45, loss = 0.36693360\n",
      "Iteration 46, loss = 0.36394140\n",
      "Iteration 47, loss = 0.36105497\n",
      "Iteration 48, loss = 0.35826698\n",
      "Iteration 49, loss = 0.35557057\n",
      "Iteration 50, loss = 0.35295940\n",
      "Iteration 51, loss = 0.35042754\n",
      "Iteration 52, loss = 0.34796953\n",
      "Iteration 53, loss = 0.34558028\n",
      "Iteration 54, loss = 0.34325513\n",
      "Iteration 55, loss = 0.34098979\n",
      "Iteration 56, loss = 0.33878030\n",
      "Iteration 57, loss = 0.33662305\n",
      "Iteration 58, loss = 0.33451472\n",
      "Iteration 59, loss = 0.33245229\n",
      "Iteration 60, loss = 0.33043299\n",
      "Iteration 61, loss = 0.32845428\n",
      "Iteration 62, loss = 0.32651383\n",
      "Iteration 63, loss = 0.32460952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 0.32273939\n",
      "Iteration 65, loss = 0.32090164\n",
      "Iteration 66, loss = 0.31909461\n",
      "Iteration 67, loss = 0.31731676\n",
      "Iteration 68, loss = 0.31556666\n",
      "Iteration 69, loss = 0.31384300\n",
      "Iteration 70, loss = 0.31214453\n",
      "Iteration 71, loss = 0.31047011\n",
      "Iteration 72, loss = 0.30881866\n",
      "Iteration 73, loss = 0.30718919\n",
      "Iteration 74, loss = 0.30558074\n",
      "Iteration 75, loss = 0.30399243\n",
      "Iteration 76, loss = 0.30242344\n",
      "Iteration 77, loss = 0.30087299\n",
      "Iteration 78, loss = 0.29934036\n",
      "Iteration 79, loss = 0.29782486\n",
      "Iteration 80, loss = 0.29632586\n",
      "Iteration 81, loss = 0.29484277\n",
      "Iteration 82, loss = 0.29337502\n",
      "Iteration 83, loss = 0.29192209\n",
      "Iteration 84, loss = 0.29048350\n",
      "Iteration 85, loss = 0.28905878\n",
      "Iteration 86, loss = 0.28764752\n",
      "Iteration 87, loss = 0.28624932\n",
      "Iteration 88, loss = 0.28486380\n",
      "Iteration 89, loss = 0.28349061\n",
      "Iteration 90, loss = 0.28212944\n",
      "Iteration 91, loss = 0.28077996\n",
      "Iteration 92, loss = 0.27944190\n",
      "Iteration 93, loss = 0.27811499\n",
      "Iteration 94, loss = 0.27679896\n",
      "Iteration 95, loss = 0.27549360\n",
      "Iteration 96, loss = 0.27419865\n",
      "Iteration 97, loss = 0.27291393\n",
      "Iteration 98, loss = 0.27163922\n",
      "Iteration 99, loss = 0.27037433\n",
      "Iteration 100, loss = 0.26911908\n",
      "Iteration 101, loss = 0.26787331\n",
      "Iteration 102, loss = 0.26663684\n",
      "Iteration 103, loss = 0.26540952\n",
      "Iteration 104, loss = 0.26419121\n",
      "Iteration 105, loss = 0.26298176\n",
      "Iteration 106, loss = 0.26178104\n",
      "Iteration 107, loss = 0.26058892\n",
      "Iteration 108, loss = 0.25940528\n",
      "Iteration 109, loss = 0.25822999\n",
      "Iteration 110, loss = 0.25706296\n",
      "Iteration 111, loss = 0.25590406\n",
      "Iteration 112, loss = 0.25475319\n",
      "Iteration 113, loss = 0.25361026\n",
      "Iteration 114, loss = 0.25247517\n",
      "Iteration 115, loss = 0.25134783\n",
      "Iteration 116, loss = 0.25022815\n",
      "Iteration 117, loss = 0.24911605\n",
      "Iteration 118, loss = 0.24801144\n",
      "Iteration 119, loss = 0.24691424\n",
      "Iteration 120, loss = 0.24582438\n",
      "Iteration 121, loss = 0.24474179\n",
      "Iteration 122, loss = 0.24366639\n",
      "Iteration 123, loss = 0.24259811\n",
      "Iteration 124, loss = 0.24153689\n",
      "Iteration 125, loss = 0.24048266\n",
      "Iteration 126, loss = 0.23943537\n",
      "Iteration 127, loss = 0.23839494\n",
      "Iteration 128, loss = 0.23736132\n",
      "Iteration 129, loss = 0.23633444\n",
      "Iteration 130, loss = 0.23531427\n",
      "Iteration 131, loss = 0.23430073\n",
      "Iteration 132, loss = 0.23329378\n",
      "Iteration 133, loss = 0.23229335\n",
      "Iteration 134, loss = 0.23129941\n",
      "Iteration 135, loss = 0.23031191\n",
      "Iteration 136, loss = 0.22933078\n",
      "Iteration 137, loss = 0.22835598\n",
      "Iteration 138, loss = 0.22738748\n",
      "Iteration 139, loss = 0.22642521\n",
      "Iteration 140, loss = 0.22546914\n",
      "Iteration 141, loss = 0.22451921\n",
      "Iteration 142, loss = 0.22357540\n",
      "Iteration 143, loss = 0.22263765\n",
      "Iteration 144, loss = 0.22170592\n",
      "Iteration 145, loss = 0.22078017\n",
      "Iteration 146, loss = 0.21986035\n",
      "Iteration 147, loss = 0.21894644\n",
      "Iteration 148, loss = 0.21803839\n",
      "Iteration 149, loss = 0.21713616\n",
      "Iteration 150, loss = 0.21623970\n",
      "Iteration 151, loss = 0.21534900\n",
      "Iteration 152, loss = 0.21446399\n",
      "Iteration 153, loss = 0.21358466\n",
      "Iteration 154, loss = 0.21271096\n",
      "Iteration 155, loss = 0.21184286\n",
      "Iteration 156, loss = 0.21098031\n",
      "Iteration 157, loss = 0.21012329\n",
      "Iteration 158, loss = 0.20927176\n",
      "Iteration 159, loss = 0.20842569\n",
      "Iteration 160, loss = 0.20758504\n",
      "Iteration 161, loss = 0.20674978\n",
      "Iteration 162, loss = 0.20591987\n",
      "Iteration 163, loss = 0.20509528\n",
      "Iteration 164, loss = 0.20427597\n",
      "Iteration 165, loss = 0.20346193\n",
      "Iteration 166, loss = 0.20265310\n",
      "Iteration 167, loss = 0.20184947\n",
      "Iteration 168, loss = 0.20105099\n",
      "Iteration 169, loss = 0.20025765\n",
      "Iteration 170, loss = 0.19946939\n",
      "Iteration 171, loss = 0.19868621\n",
      "Iteration 172, loss = 0.19790805\n",
      "Iteration 173, loss = 0.19713490\n",
      "Iteration 174, loss = 0.19636672\n",
      "Iteration 175, loss = 0.19560349\n",
      "Iteration 176, loss = 0.19484516\n",
      "Iteration 177, loss = 0.19409172\n",
      "Iteration 178, loss = 0.19334313\n",
      "Iteration 179, loss = 0.19259936\n",
      "Iteration 180, loss = 0.19186039\n",
      "Iteration 181, loss = 0.19112618\n",
      "Iteration 182, loss = 0.19039671\n",
      "Iteration 183, loss = 0.18967194\n",
      "Iteration 184, loss = 0.18895185\n",
      "Iteration 185, loss = 0.18823641\n",
      "Iteration 186, loss = 0.18752559\n",
      "Iteration 187, loss = 0.18681937\n",
      "Iteration 188, loss = 0.18611770\n",
      "Iteration 189, loss = 0.18542058\n",
      "Iteration 190, loss = 0.18472796\n",
      "Iteration 191, loss = 0.18403983\n",
      "Iteration 192, loss = 0.18335614\n",
      "Iteration 193, loss = 0.18267689\n",
      "Iteration 194, loss = 0.18200203\n",
      "Iteration 195, loss = 0.18133154\n",
      "Iteration 196, loss = 0.18066540\n",
      "Iteration 197, loss = 0.18000357\n",
      "Iteration 198, loss = 0.17934604\n",
      "Iteration 199, loss = 0.17869277\n",
      "Iteration 200, loss = 0.17804373\n",
      "Iteration 201, loss = 0.17739891\n",
      "Iteration 202, loss = 0.17675827\n",
      "Iteration 203, loss = 0.17612179\n",
      "Iteration 204, loss = 0.17548944\n",
      "Iteration 205, loss = 0.17486119\n",
      "Iteration 206, loss = 0.17423703\n",
      "Iteration 207, loss = 0.17361692\n",
      "Iteration 208, loss = 0.17300084\n",
      "Iteration 209, loss = 0.17238876\n",
      "Iteration 210, loss = 0.17178065\n",
      "Iteration 211, loss = 0.17117650\n",
      "Iteration 212, loss = 0.17057628\n",
      "Iteration 213, loss = 0.16997995\n",
      "Iteration 214, loss = 0.16938751\n",
      "Iteration 215, loss = 0.16879891\n",
      "Iteration 216, loss = 0.16821414\n",
      "Iteration 217, loss = 0.16763317\n",
      "Iteration 218, loss = 0.16705598\n",
      "Iteration 219, loss = 0.16648254\n",
      "Iteration 220, loss = 0.16591283\n",
      "Iteration 221, loss = 0.16534682\n",
      "Iteration 222, loss = 0.16478449\n",
      "Iteration 223, loss = 0.16422581\n",
      "Iteration 224, loss = 0.16367077\n",
      "Iteration 225, loss = 0.16311933\n",
      "Iteration 226, loss = 0.16257148\n",
      "Iteration 227, loss = 0.16202718\n",
      "Iteration 228, loss = 0.16148643\n",
      "Iteration 229, loss = 0.16094918\n",
      "Iteration 230, loss = 0.16041542\n",
      "Iteration 231, loss = 0.15988513\n",
      "Iteration 232, loss = 0.15935829\n",
      "Iteration 233, loss = 0.15883486\n",
      "Iteration 234, loss = 0.15831482\n",
      "Iteration 235, loss = 0.15779817\n",
      "Iteration 236, loss = 0.15728486\n",
      "Iteration 237, loss = 0.15677488\n",
      "Iteration 238, loss = 0.15626820\n",
      "Iteration 239, loss = 0.15576481\n",
      "Iteration 240, loss = 0.15526468\n",
      "Iteration 241, loss = 0.15476779\n",
      "Iteration 242, loss = 0.15427412\n",
      "Iteration 243, loss = 0.15378364\n",
      "Iteration 244, loss = 0.15329633\n",
      "Iteration 245, loss = 0.15281217\n",
      "Iteration 246, loss = 0.15233115\n",
      "Iteration 247, loss = 0.15185323\n",
      "Iteration 248, loss = 0.15137839\n",
      "Iteration 249, loss = 0.15090663\n",
      "Iteration 250, loss = 0.15043790\n",
      "Iteration 251, loss = 0.14997220\n",
      "Iteration 252, loss = 0.14950950\n",
      "Iteration 253, loss = 0.14904978\n",
      "Iteration 254, loss = 0.14859302\n",
      "Iteration 255, loss = 0.14813920\n",
      "Iteration 256, loss = 0.14768830\n",
      "Iteration 257, loss = 0.14724029\n",
      "Iteration 258, loss = 0.14679517\n",
      "Iteration 259, loss = 0.14635290\n",
      "Iteration 260, loss = 0.14591347\n",
      "Iteration 261, loss = 0.14547686\n",
      "Iteration 262, loss = 0.14504305\n",
      "Iteration 263, loss = 0.14461201\n",
      "Iteration 264, loss = 0.14418373\n",
      "Iteration 265, loss = 0.14375819\n",
      "Iteration 266, loss = 0.14333537\n",
      "Iteration 267, loss = 0.14291525\n",
      "Iteration 268, loss = 0.14249781\n",
      "Iteration 269, loss = 0.14208304\n",
      "Iteration 270, loss = 0.14167090\n",
      "Iteration 271, loss = 0.14126139\n",
      "Iteration 272, loss = 0.14085449\n",
      "Iteration 273, loss = 0.14045017\n",
      "Iteration 274, loss = 0.14004842\n",
      "Iteration 275, loss = 0.13964922\n",
      "Iteration 276, loss = 0.13925255\n",
      "Iteration 277, loss = 0.13885840\n",
      "Iteration 278, loss = 0.13846674\n",
      "Iteration 279, loss = 0.13807756\n",
      "Iteration 280, loss = 0.13769084\n",
      "Iteration 281, loss = 0.13730655\n",
      "Iteration 282, loss = 0.13692470\n",
      "Iteration 283, loss = 0.13654525\n",
      "Iteration 284, loss = 0.13616819\n",
      "Iteration 285, loss = 0.13579350\n",
      "Iteration 286, loss = 0.13542116\n",
      "Iteration 287, loss = 0.13505117\n",
      "Iteration 288, loss = 0.13468349\n",
      "Iteration 289, loss = 0.13431812\n",
      "Iteration 290, loss = 0.13395504\n",
      "Iteration 291, loss = 0.13359423\n",
      "Iteration 292, loss = 0.13323567\n",
      "Iteration 293, loss = 0.13287935\n",
      "Iteration 294, loss = 0.13252525\n",
      "Iteration 295, loss = 0.13217336\n",
      "Iteration 296, loss = 0.13182365\n",
      "Iteration 297, loss = 0.13147612\n",
      "Iteration 298, loss = 0.13113075\n",
      "Iteration 299, loss = 0.13078752\n",
      "Iteration 300, loss = 0.13044641\n",
      "Iteration 301, loss = 0.13010742\n",
      "Iteration 302, loss = 0.12977052\n",
      "Iteration 303, loss = 0.12943570\n",
      "Iteration 304, loss = 0.12910295\n",
      "Iteration 305, loss = 0.12877225\n",
      "Iteration 306, loss = 0.12844358\n",
      "Iteration 307, loss = 0.12811693\n",
      "Iteration 308, loss = 0.12779229\n",
      "Iteration 309, loss = 0.12746963\n",
      "Iteration 310, loss = 0.12714896\n",
      "Iteration 311, loss = 0.12683024\n",
      "Iteration 312, loss = 0.12651348\n",
      "Iteration 313, loss = 0.12619864\n",
      "Iteration 314, loss = 0.12588572\n",
      "Iteration 315, loss = 0.12557471\n",
      "Iteration 316, loss = 0.12526559\n",
      "Iteration 317, loss = 0.12495835\n",
      "Iteration 318, loss = 0.12465297\n",
      "Iteration 319, loss = 0.12434944\n",
      "Iteration 320, loss = 0.12404775\n",
      "Iteration 321, loss = 0.12374788\n",
      "Iteration 322, loss = 0.12344982\n",
      "Iteration 323, loss = 0.12315355\n",
      "Iteration 324, loss = 0.12285907\n",
      "Iteration 325, loss = 0.12256635\n",
      "Iteration 326, loss = 0.12227540\n",
      "Iteration 327, loss = 0.12198619\n",
      "Iteration 328, loss = 0.12169871\n",
      "Iteration 329, loss = 0.12141295\n",
      "Iteration 330, loss = 0.12112889\n",
      "Iteration 331, loss = 0.12084653\n",
      "Iteration 332, loss = 0.12056585\n",
      "Iteration 333, loss = 0.12028684\n",
      "Iteration 334, loss = 0.12000949\n",
      "Iteration 335, loss = 0.11973378\n",
      "Iteration 336, loss = 0.11945970\n",
      "Iteration 337, loss = 0.11918724\n",
      "Iteration 338, loss = 0.11891640\n",
      "Iteration 339, loss = 0.11864715\n",
      "Iteration 340, loss = 0.11837949\n",
      "Iteration 341, loss = 0.11811340\n",
      "Iteration 342, loss = 0.11784887\n",
      "Iteration 343, loss = 0.11758589\n",
      "Iteration 344, loss = 0.11732446\n",
      "Iteration 345, loss = 0.11706455\n",
      "Iteration 346, loss = 0.11680616\n",
      "Iteration 347, loss = 0.11654928\n",
      "Iteration 348, loss = 0.11629389\n",
      "Iteration 349, loss = 0.11603999\n",
      "Iteration 350, loss = 0.11578756\n",
      "Iteration 351, loss = 0.11553660\n",
      "Iteration 352, loss = 0.11528708\n",
      "Iteration 353, loss = 0.11503902\n",
      "Iteration 354, loss = 0.11479238\n",
      "Iteration 355, loss = 0.11454716\n",
      "Iteration 356, loss = 0.11430335\n",
      "Iteration 357, loss = 0.11406095\n",
      "Iteration 358, loss = 0.11381994\n",
      "Iteration 359, loss = 0.11358030\n",
      "Iteration 360, loss = 0.11334204\n",
      "Iteration 361, loss = 0.11310514\n",
      "Iteration 362, loss = 0.11286959\n",
      "Iteration 363, loss = 0.11263538\n",
      "Iteration 364, loss = 0.11240250\n",
      "Iteration 365, loss = 0.11217094\n",
      "Iteration 366, loss = 0.11194070\n",
      "Iteration 367, loss = 0.11171176\n",
      "Iteration 368, loss = 0.11148411\n",
      "Iteration 369, loss = 0.11125775\n",
      "Iteration 370, loss = 0.11103267\n",
      "Iteration 371, loss = 0.11080885\n",
      "Iteration 372, loss = 0.11058628\n",
      "Iteration 373, loss = 0.11036497\n",
      "Iteration 374, loss = 0.11014489\n",
      "Iteration 375, loss = 0.10992605\n",
      "Iteration 376, loss = 0.10970842\n",
      "Iteration 377, loss = 0.10949201\n",
      "Iteration 378, loss = 0.10927680\n",
      "Iteration 379, loss = 0.10906279\n",
      "Iteration 380, loss = 0.10884997\n",
      "Iteration 381, loss = 0.10863832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 382, loss = 0.10842784\n",
      "Iteration 383, loss = 0.10821853\n",
      "Iteration 384, loss = 0.10801037\n",
      "Iteration 385, loss = 0.10780335\n",
      "Iteration 386, loss = 0.10759748\n",
      "Iteration 387, loss = 0.10739273\n",
      "Iteration 388, loss = 0.10718910\n",
      "Iteration 389, loss = 0.10698659\n",
      "Iteration 390, loss = 0.10678518\n",
      "Iteration 391, loss = 0.10658487\n",
      "Iteration 392, loss = 0.10638565\n",
      "Iteration 393, loss = 0.10618752\n",
      "Iteration 394, loss = 0.10599045\n",
      "Iteration 395, loss = 0.10579446\n",
      "Iteration 396, loss = 0.10559952\n",
      "Iteration 397, loss = 0.10540564\n",
      "Iteration 398, loss = 0.10521280\n",
      "Iteration 399, loss = 0.10502100\n",
      "Iteration 400, loss = 0.10483023\n",
      "Iteration 401, loss = 0.10464048\n",
      "Iteration 402, loss = 0.10445175\n",
      "Iteration 403, loss = 0.10426403\n",
      "Iteration 404, loss = 0.10407731\n",
      "Iteration 405, loss = 0.10389159\n",
      "Iteration 406, loss = 0.10370685\n",
      "Iteration 407, loss = 0.10352309\n",
      "Iteration 408, loss = 0.10334031\n",
      "Iteration 409, loss = 0.10315850\n",
      "Iteration 410, loss = 0.10297765\n",
      "Iteration 411, loss = 0.10279775\n",
      "Iteration 412, loss = 0.10261880\n",
      "Iteration 413, loss = 0.10244079\n",
      "Iteration 414, loss = 0.10226371\n",
      "Iteration 415, loss = 0.10208757\n",
      "Iteration 416, loss = 0.10191234\n",
      "Iteration 417, loss = 0.10173804\n",
      "Iteration 418, loss = 0.10156464\n",
      "Iteration 419, loss = 0.10139214\n",
      "Iteration 420, loss = 0.10122054\n",
      "Iteration 421, loss = 0.10104983\n",
      "Iteration 422, loss = 0.10088001\n",
      "Iteration 423, loss = 0.10071106\n",
      "Iteration 424, loss = 0.10054299\n",
      "Iteration 425, loss = 0.10037578\n",
      "Iteration 426, loss = 0.10020944\n",
      "Iteration 427, loss = 0.10004395\n",
      "Iteration 428, loss = 0.09987930\n",
      "Iteration 429, loss = 0.09971551\n",
      "Iteration 430, loss = 0.09955255\n",
      "Iteration 431, loss = 0.09939042\n",
      "Iteration 432, loss = 0.09922912\n",
      "Iteration 433, loss = 0.09906863\n",
      "Iteration 434, loss = 0.09890897\n",
      "Iteration 435, loss = 0.09875011\n",
      "Iteration 436, loss = 0.09859206\n",
      "Iteration 437, loss = 0.09843481\n",
      "Iteration 438, loss = 0.09827835\n",
      "Iteration 439, loss = 0.09812268\n",
      "Iteration 440, loss = 0.09796779\n",
      "Iteration 441, loss = 0.09781368\n",
      "Iteration 442, loss = 0.09766034\n",
      "Iteration 443, loss = 0.09750777\n",
      "Iteration 444, loss = 0.09735596\n",
      "Iteration 445, loss = 0.09720491\n",
      "Iteration 446, loss = 0.09705461\n",
      "Iteration 447, loss = 0.09690506\n",
      "Iteration 448, loss = 0.09675625\n",
      "Iteration 449, loss = 0.09660818\n",
      "Iteration 450, loss = 0.09646084\n",
      "Iteration 451, loss = 0.09631423\n",
      "Iteration 452, loss = 0.09616834\n",
      "Iteration 453, loss = 0.09602317\n",
      "Iteration 454, loss = 0.09587871\n",
      "Iteration 455, loss = 0.09573496\n",
      "Iteration 456, loss = 0.09559192\n",
      "Iteration 457, loss = 0.09544957\n",
      "Iteration 458, loss = 0.09530792\n",
      "Iteration 459, loss = 0.09516696\n",
      "Iteration 460, loss = 0.09502668\n",
      "Iteration 461, loss = 0.09488709\n",
      "Iteration 462, loss = 0.09474817\n",
      "Iteration 463, loss = 0.09460993\n",
      "Iteration 464, loss = 0.09447235\n",
      "Iteration 465, loss = 0.09433544\n",
      "Iteration 466, loss = 0.09419919\n",
      "Iteration 467, loss = 0.09406359\n",
      "Iteration 468, loss = 0.09392864\n",
      "Iteration 469, loss = 0.09379434\n",
      "Iteration 470, loss = 0.09366068\n",
      "Iteration 471, loss = 0.09352766\n",
      "Iteration 472, loss = 0.09339527\n",
      "Iteration 473, loss = 0.09326352\n",
      "Iteration 474, loss = 0.09313239\n",
      "Iteration 475, loss = 0.09300188\n",
      "Iteration 476, loss = 0.09287199\n",
      "Iteration 477, loss = 0.09274272\n",
      "Iteration 478, loss = 0.09261405\n",
      "Iteration 479, loss = 0.09248599\n",
      "Iteration 480, loss = 0.09235854\n",
      "Iteration 481, loss = 0.09223168\n",
      "Iteration 482, loss = 0.09210542\n",
      "Iteration 483, loss = 0.09197975\n",
      "Iteration 484, loss = 0.09185466\n",
      "Iteration 485, loss = 0.09173016\n",
      "Iteration 486, loss = 0.09160624\n",
      "Iteration 487, loss = 0.09148290\n",
      "Iteration 488, loss = 0.09136013\n",
      "Iteration 489, loss = 0.09123793\n",
      "Iteration 490, loss = 0.09111629\n",
      "Iteration 491, loss = 0.09099521\n",
      "Iteration 492, loss = 0.09087470\n",
      "Iteration 493, loss = 0.09075473\n",
      "Iteration 494, loss = 0.09063532\n",
      "Iteration 495, loss = 0.09051646\n",
      "Iteration 496, loss = 0.09039814\n",
      "Iteration 497, loss = 0.09028037\n",
      "Iteration 498, loss = 0.09016313\n",
      "Iteration 499, loss = 0.09004642\n",
      "Iteration 500, loss = 0.08993025\n",
      "Iteration 1, loss = 1.07143694\n",
      "Iteration 2, loss = 1.05169133\n",
      "Iteration 3, loss = 1.02449514\n",
      "Iteration 4, loss = 0.99153866\n",
      "Iteration 5, loss = 0.95446758\n",
      "Iteration 6, loss = 0.91481901\n",
      "Iteration 7, loss = 0.87397057\n",
      "Iteration 8, loss = 0.83310327\n",
      "Iteration 9, loss = 0.79317940\n",
      "Iteration 10, loss = 0.75493611\n",
      "Iteration 11, loss = 0.71889381\n",
      "Iteration 12, loss = 0.68537692\n",
      "Iteration 13, loss = 0.65454311\n",
      "Iteration 14, loss = 0.62641687\n",
      "Iteration 15, loss = 0.60092320\n",
      "Iteration 16, loss = 0.57791861\n",
      "Iteration 17, loss = 0.55721735\n",
      "Iteration 18, loss = 0.53861228\n",
      "Iteration 19, loss = 0.52189034\n",
      "Iteration 20, loss = 0.50684337\n",
      "Iteration 21, loss = 0.49327506\n",
      "Iteration 22, loss = 0.48100498\n",
      "Iteration 23, loss = 0.46987042\n",
      "Iteration 24, loss = 0.45972689\n",
      "Iteration 25, loss = 0.45044751\n",
      "Iteration 26, loss = 0.44192195\n",
      "Iteration 27, loss = 0.43405497\n",
      "Iteration 28, loss = 0.42676482\n",
      "Iteration 29, loss = 0.41998169\n",
      "Iteration 30, loss = 0.41364612\n",
      "Iteration 31, loss = 0.40770755\n",
      "Iteration 32, loss = 0.40212302\n",
      "Iteration 33, loss = 0.39685595\n",
      "Iteration 34, loss = 0.39187510\n",
      "Iteration 35, loss = 0.38715368\n",
      "Iteration 36, loss = 0.38266853\n",
      "Iteration 37, loss = 0.37839950\n",
      "Iteration 38, loss = 0.37432887\n",
      "Iteration 39, loss = 0.37044094\n",
      "Iteration 40, loss = 0.36672164\n",
      "Iteration 41, loss = 0.36315827\n",
      "Iteration 42, loss = 0.35973924\n",
      "Iteration 43, loss = 0.35645395\n",
      "Iteration 44, loss = 0.35329259\n",
      "Iteration 45, loss = 0.35024612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.34730613\n",
      "Iteration 47, loss = 0.34446483\n",
      "Iteration 48, loss = 0.34171498\n",
      "Iteration 49, loss = 0.33904986\n",
      "Iteration 50, loss = 0.33646327\n",
      "Iteration 51, loss = 0.33394947\n",
      "Iteration 52, loss = 0.33150316\n",
      "Iteration 53, loss = 0.32911950\n",
      "Iteration 54, loss = 0.32679402\n",
      "Iteration 55, loss = 0.32452267\n",
      "Iteration 56, loss = 0.32230173\n",
      "Iteration 57, loss = 0.32012782\n",
      "Iteration 58, loss = 0.31799788\n",
      "Iteration 59, loss = 0.31590911\n",
      "Iteration 60, loss = 0.31385898\n",
      "Iteration 61, loss = 0.31184522\n",
      "Iteration 62, loss = 0.30986573\n",
      "Iteration 63, loss = 0.30791863\n",
      "Iteration 64, loss = 0.30600219\n",
      "Iteration 65, loss = 0.30411486\n",
      "Iteration 66, loss = 0.30225519\n",
      "Iteration 67, loss = 0.30042188\n",
      "Iteration 68, loss = 0.29861372\n",
      "Iteration 69, loss = 0.29682961\n",
      "Iteration 70, loss = 0.29506851\n",
      "Iteration 71, loss = 0.29332948\n",
      "Iteration 72, loss = 0.29161164\n",
      "Iteration 73, loss = 0.28991416\n",
      "Iteration 74, loss = 0.28823628\n",
      "Iteration 75, loss = 0.28657729\n",
      "Iteration 76, loss = 0.28493652\n",
      "Iteration 77, loss = 0.28331334\n",
      "Iteration 78, loss = 0.28170717\n",
      "Iteration 79, loss = 0.28011747\n",
      "Iteration 80, loss = 0.27854371\n",
      "Iteration 81, loss = 0.27698542\n",
      "Iteration 82, loss = 0.27544215\n",
      "Iteration 83, loss = 0.27391349\n",
      "Iteration 84, loss = 0.27239902\n",
      "Iteration 85, loss = 0.27089839\n",
      "Iteration 86, loss = 0.26941126\n",
      "Iteration 87, loss = 0.26793729\n",
      "Iteration 88, loss = 0.26647619\n",
      "Iteration 89, loss = 0.26502767\n",
      "Iteration 90, loss = 0.26359146\n",
      "Iteration 91, loss = 0.26216732\n",
      "Iteration 92, loss = 0.26075501\n",
      "Iteration 93, loss = 0.25935431\n",
      "Iteration 94, loss = 0.25796501\n",
      "Iteration 95, loss = 0.25658692\n",
      "Iteration 96, loss = 0.25521984\n",
      "Iteration 97, loss = 0.25386360\n",
      "Iteration 98, loss = 0.25251803\n",
      "Iteration 99, loss = 0.25118297\n",
      "Iteration 100, loss = 0.24985827\n",
      "Iteration 101, loss = 0.24854379\n",
      "Iteration 102, loss = 0.24723938\n",
      "Iteration 103, loss = 0.24594491\n",
      "Iteration 104, loss = 0.24466026\n",
      "Iteration 105, loss = 0.24338530\n",
      "Iteration 106, loss = 0.24211991\n",
      "Iteration 107, loss = 0.24086397\n",
      "Iteration 108, loss = 0.23961739\n",
      "Iteration 109, loss = 0.23838004\n",
      "Iteration 110, loss = 0.23715184\n",
      "Iteration 111, loss = 0.23593268\n",
      "Iteration 112, loss = 0.23472246\n",
      "Iteration 113, loss = 0.23352110\n",
      "Iteration 114, loss = 0.23232849\n",
      "Iteration 115, loss = 0.23114456\n",
      "Iteration 116, loss = 0.22996922\n",
      "Iteration 117, loss = 0.22880238\n",
      "Iteration 118, loss = 0.22764398\n",
      "Iteration 119, loss = 0.22649391\n",
      "Iteration 120, loss = 0.22535213\n",
      "Iteration 121, loss = 0.22421853\n",
      "Iteration 122, loss = 0.22309307\n",
      "Iteration 123, loss = 0.22197566\n",
      "Iteration 124, loss = 0.22086623\n",
      "Iteration 125, loss = 0.21976472\n",
      "Iteration 126, loss = 0.21867106\n",
      "Iteration 127, loss = 0.21758518\n",
      "Iteration 128, loss = 0.21650703\n",
      "Iteration 129, loss = 0.21543654\n",
      "Iteration 130, loss = 0.21437364\n",
      "Iteration 131, loss = 0.21331829\n",
      "Iteration 132, loss = 0.21227041\n",
      "Iteration 133, loss = 0.21122995\n",
      "Iteration 134, loss = 0.21019685\n",
      "Iteration 135, loss = 0.20917106\n",
      "Iteration 136, loss = 0.20815252\n",
      "Iteration 137, loss = 0.20714118\n",
      "Iteration 138, loss = 0.20613698\n",
      "Iteration 139, loss = 0.20513986\n",
      "Iteration 140, loss = 0.20414978\n",
      "Iteration 141, loss = 0.20316668\n",
      "Iteration 142, loss = 0.20219052\n",
      "Iteration 143, loss = 0.20122123\n",
      "Iteration 144, loss = 0.20025878\n",
      "Iteration 145, loss = 0.19930310\n",
      "Iteration 146, loss = 0.19835416\n",
      "Iteration 147, loss = 0.19741189\n",
      "Iteration 148, loss = 0.19647627\n",
      "Iteration 149, loss = 0.19554723\n",
      "Iteration 150, loss = 0.19462473\n",
      "Iteration 151, loss = 0.19370872\n",
      "Iteration 152, loss = 0.19279916\n",
      "Iteration 153, loss = 0.19189600\n",
      "Iteration 154, loss = 0.19099919\n",
      "Iteration 155, loss = 0.19010869\n",
      "Iteration 156, loss = 0.18922446\n",
      "Iteration 157, loss = 0.18834645\n",
      "Iteration 158, loss = 0.18747461\n",
      "Iteration 159, loss = 0.18660891\n",
      "Iteration 160, loss = 0.18574930\n",
      "Iteration 161, loss = 0.18489573\n",
      "Iteration 162, loss = 0.18404817\n",
      "Iteration 163, loss = 0.18320657\n",
      "Iteration 164, loss = 0.18237089\n",
      "Iteration 165, loss = 0.18154108\n",
      "Iteration 166, loss = 0.18071712\n",
      "Iteration 167, loss = 0.17989894\n",
      "Iteration 168, loss = 0.17908652\n",
      "Iteration 169, loss = 0.17827981\n",
      "Iteration 170, loss = 0.17747878\n",
      "Iteration 171, loss = 0.17668338\n",
      "Iteration 172, loss = 0.17589357\n",
      "Iteration 173, loss = 0.17510931\n",
      "Iteration 174, loss = 0.17433056\n",
      "Iteration 175, loss = 0.17355729\n",
      "Iteration 176, loss = 0.17278946\n",
      "Iteration 177, loss = 0.17202702\n",
      "Iteration 178, loss = 0.17126994\n",
      "Iteration 179, loss = 0.17051818\n",
      "Iteration 180, loss = 0.16977170\n",
      "Iteration 181, loss = 0.16903047\n",
      "Iteration 182, loss = 0.16829444\n",
      "Iteration 183, loss = 0.16756359\n",
      "Iteration 184, loss = 0.16683786\n",
      "Iteration 185, loss = 0.16611723\n",
      "Iteration 186, loss = 0.16540166\n",
      "Iteration 187, loss = 0.16469111\n",
      "Iteration 188, loss = 0.16398554\n",
      "Iteration 189, loss = 0.16328493\n",
      "Iteration 190, loss = 0.16258923\n",
      "Iteration 191, loss = 0.16189841\n",
      "Iteration 192, loss = 0.16121243\n",
      "Iteration 193, loss = 0.16053125\n",
      "Iteration 194, loss = 0.15985485\n",
      "Iteration 195, loss = 0.15918319\n",
      "Iteration 196, loss = 0.15851623\n",
      "Iteration 197, loss = 0.15785394\n",
      "Iteration 198, loss = 0.15719629\n",
      "Iteration 199, loss = 0.15654323\n",
      "Iteration 200, loss = 0.15589475\n",
      "Iteration 201, loss = 0.15525079\n",
      "Iteration 202, loss = 0.15461133\n",
      "Iteration 203, loss = 0.15397634\n",
      "Iteration 204, loss = 0.15334579\n",
      "Iteration 205, loss = 0.15271963\n",
      "Iteration 206, loss = 0.15209784\n",
      "Iteration 207, loss = 0.15148039\n",
      "Iteration 208, loss = 0.15086724\n",
      "Iteration 209, loss = 0.15025836\n",
      "Iteration 210, loss = 0.14965372\n",
      "Iteration 211, loss = 0.14905328\n",
      "Iteration 212, loss = 0.14845702\n",
      "Iteration 213, loss = 0.14786491\n",
      "Iteration 214, loss = 0.14727691\n",
      "Iteration 215, loss = 0.14669299\n",
      "Iteration 216, loss = 0.14611312\n",
      "Iteration 217, loss = 0.14553727\n",
      "Iteration 218, loss = 0.14496542\n",
      "Iteration 219, loss = 0.14439752\n",
      "Iteration 220, loss = 0.14383355\n",
      "Iteration 221, loss = 0.14327349\n",
      "Iteration 222, loss = 0.14271730\n",
      "Iteration 223, loss = 0.14216494\n",
      "Iteration 224, loss = 0.14161640\n",
      "Iteration 225, loss = 0.14107164\n",
      "Iteration 226, loss = 0.14053064\n",
      "Iteration 227, loss = 0.13999336\n",
      "Iteration 228, loss = 0.13945978\n",
      "Iteration 229, loss = 0.13892987\n",
      "Iteration 230, loss = 0.13840360\n",
      "Iteration 231, loss = 0.13788094\n",
      "Iteration 232, loss = 0.13736186\n",
      "Iteration 233, loss = 0.13684635\n",
      "Iteration 234, loss = 0.13633436\n",
      "Iteration 235, loss = 0.13582587\n",
      "Iteration 236, loss = 0.13532087\n",
      "Iteration 237, loss = 0.13481931\n",
      "Iteration 238, loss = 0.13432117\n",
      "Iteration 239, loss = 0.13382643\n",
      "Iteration 240, loss = 0.13333505\n",
      "Iteration 241, loss = 0.13284703\n",
      "Iteration 242, loss = 0.13236231\n",
      "Iteration 243, loss = 0.13188089\n",
      "Iteration 244, loss = 0.13140274\n",
      "Iteration 245, loss = 0.13092783\n",
      "Iteration 246, loss = 0.13045613\n",
      "Iteration 247, loss = 0.12998762\n",
      "Iteration 248, loss = 0.12952228\n",
      "Iteration 249, loss = 0.12906008\n",
      "Iteration 250, loss = 0.12860099\n",
      "Iteration 251, loss = 0.12814500\n",
      "Iteration 252, loss = 0.12769208\n",
      "Iteration 253, loss = 0.12724220\n",
      "Iteration 254, loss = 0.12679534\n",
      "Iteration 255, loss = 0.12635147\n",
      "Iteration 256, loss = 0.12591058\n",
      "Iteration 257, loss = 0.12547264\n",
      "Iteration 258, loss = 0.12503763\n",
      "Iteration 259, loss = 0.12460552\n",
      "Iteration 260, loss = 0.12417629\n",
      "Iteration 261, loss = 0.12374992\n",
      "Iteration 262, loss = 0.12332639\n",
      "Iteration 263, loss = 0.12290567\n",
      "Iteration 264, loss = 0.12248774\n",
      "Iteration 265, loss = 0.12207258\n",
      "Iteration 266, loss = 0.12166017\n",
      "Iteration 267, loss = 0.12125049\n",
      "Iteration 268, loss = 0.12084351\n",
      "Iteration 269, loss = 0.12043922\n",
      "Iteration 270, loss = 0.12003759\n",
      "Iteration 271, loss = 0.11963860\n",
      "Iteration 272, loss = 0.11924223\n",
      "Iteration 273, loss = 0.11884847\n",
      "Iteration 274, loss = 0.11845728\n",
      "Iteration 275, loss = 0.11806865\n",
      "Iteration 276, loss = 0.11768257\n",
      "Iteration 277, loss = 0.11729900\n",
      "Iteration 278, loss = 0.11691793\n",
      "Iteration 279, loss = 0.11653935\n",
      "Iteration 280, loss = 0.11616322\n",
      "Iteration 281, loss = 0.11578954\n",
      "Iteration 282, loss = 0.11541828\n",
      "Iteration 283, loss = 0.11504942\n",
      "Iteration 284, loss = 0.11468295\n",
      "Iteration 285, loss = 0.11431884\n",
      "Iteration 286, loss = 0.11395708\n",
      "Iteration 287, loss = 0.11359765\n",
      "Iteration 288, loss = 0.11324053\n",
      "Iteration 289, loss = 0.11288570\n",
      "Iteration 290, loss = 0.11253315\n",
      "Iteration 291, loss = 0.11218285\n",
      "Iteration 292, loss = 0.11183480\n",
      "Iteration 293, loss = 0.11148896\n",
      "Iteration 294, loss = 0.11114533\n",
      "Iteration 295, loss = 0.11080388\n",
      "Iteration 296, loss = 0.11046461\n",
      "Iteration 297, loss = 0.11012749\n",
      "Iteration 298, loss = 0.10979250\n",
      "Iteration 299, loss = 0.10945964\n",
      "Iteration 300, loss = 0.10912887\n",
      "Iteration 301, loss = 0.10880019\n",
      "Iteration 302, loss = 0.10847359\n",
      "Iteration 303, loss = 0.10814903\n",
      "Iteration 304, loss = 0.10782652\n",
      "Iteration 305, loss = 0.10750602\n",
      "Iteration 306, loss = 0.10718754\n",
      "Iteration 307, loss = 0.10687104\n",
      "Iteration 308, loss = 0.10655652\n",
      "Iteration 309, loss = 0.10624396\n",
      "Iteration 310, loss = 0.10593334\n",
      "Iteration 311, loss = 0.10562465\n",
      "Iteration 312, loss = 0.10531788\n",
      "Iteration 313, loss = 0.10501301\n",
      "Iteration 314, loss = 0.10471002\n",
      "Iteration 315, loss = 0.10440890\n",
      "Iteration 316, loss = 0.10410964\n",
      "Iteration 317, loss = 0.10381222\n",
      "Iteration 318, loss = 0.10351663\n",
      "Iteration 319, loss = 0.10322285\n",
      "Iteration 320, loss = 0.10293087\n",
      "Iteration 321, loss = 0.10264067\n",
      "Iteration 322, loss = 0.10235225\n",
      "Iteration 323, loss = 0.10206559\n",
      "Iteration 324, loss = 0.10178067\n",
      "Iteration 325, loss = 0.10149748\n",
      "Iteration 326, loss = 0.10121600\n",
      "Iteration 327, loss = 0.10093624\n",
      "Iteration 328, loss = 0.10065816\n",
      "Iteration 329, loss = 0.10038177\n",
      "Iteration 330, loss = 0.10010703\n",
      "Iteration 331, loss = 0.09983396\n",
      "Iteration 332, loss = 0.09956252\n",
      "Iteration 333, loss = 0.09929271\n",
      "Iteration 334, loss = 0.09902452\n",
      "Iteration 335, loss = 0.09875792\n",
      "Iteration 336, loss = 0.09849292\n",
      "Iteration 337, loss = 0.09822950\n",
      "Iteration 338, loss = 0.09796765\n",
      "Iteration 339, loss = 0.09770735\n",
      "Iteration 340, loss = 0.09744859\n",
      "Iteration 341, loss = 0.09719137\n",
      "Iteration 342, loss = 0.09693566\n",
      "Iteration 343, loss = 0.09668147\n",
      "Iteration 344, loss = 0.09642877\n",
      "Iteration 345, loss = 0.09617756\n",
      "Iteration 346, loss = 0.09592782\n",
      "Iteration 347, loss = 0.09567954\n",
      "Iteration 348, loss = 0.09543272\n",
      "Iteration 349, loss = 0.09518734\n",
      "Iteration 350, loss = 0.09494339\n",
      "Iteration 351, loss = 0.09470086\n",
      "Iteration 352, loss = 0.09445974\n",
      "Iteration 353, loss = 0.09422001\n",
      "Iteration 354, loss = 0.09398168\n",
      "Iteration 355, loss = 0.09374472\n",
      "Iteration 356, loss = 0.09350914\n",
      "Iteration 357, loss = 0.09327490\n",
      "Iteration 358, loss = 0.09304202\n",
      "Iteration 359, loss = 0.09281048\n",
      "Iteration 360, loss = 0.09258026\n",
      "Iteration 361, loss = 0.09235136\n",
      "Iteration 362, loss = 0.09212376\n",
      "Iteration 363, loss = 0.09189747\n",
      "Iteration 364, loss = 0.09167246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 365, loss = 0.09144874\n",
      "Iteration 366, loss = 0.09122628\n",
      "Iteration 367, loss = 0.09100508\n",
      "Iteration 368, loss = 0.09078514\n",
      "Iteration 369, loss = 0.09056644\n",
      "Iteration 370, loss = 0.09034897\n",
      "Iteration 371, loss = 0.09013272\n",
      "Iteration 372, loss = 0.08991769\n",
      "Iteration 373, loss = 0.08970386\n",
      "Iteration 374, loss = 0.08949123\n",
      "Iteration 375, loss = 0.08927979\n",
      "Iteration 376, loss = 0.08906953\n",
      "Iteration 377, loss = 0.08886044\n",
      "Iteration 378, loss = 0.08865251\n",
      "Iteration 379, loss = 0.08844574\n",
      "Iteration 380, loss = 0.08824011\n",
      "Iteration 381, loss = 0.08803562\n",
      "Iteration 382, loss = 0.08783227\n",
      "Iteration 383, loss = 0.08763003\n",
      "Iteration 384, loss = 0.08742890\n",
      "Iteration 385, loss = 0.08722888\n",
      "Iteration 386, loss = 0.08702996\n",
      "Iteration 387, loss = 0.08683212\n",
      "Iteration 388, loss = 0.08663537\n",
      "Iteration 389, loss = 0.08643969\n",
      "Iteration 390, loss = 0.08624508\n",
      "Iteration 391, loss = 0.08605152\n",
      "Iteration 392, loss = 0.08585902\n",
      "Iteration 393, loss = 0.08566756\n",
      "Iteration 394, loss = 0.08547714\n",
      "Iteration 395, loss = 0.08528774\n",
      "Iteration 396, loss = 0.08509937\n",
      "Iteration 397, loss = 0.08491201\n",
      "Iteration 398, loss = 0.08472565\n",
      "Iteration 399, loss = 0.08454030\n",
      "Iteration 400, loss = 0.08435594\n",
      "Iteration 401, loss = 0.08417256\n",
      "Iteration 402, loss = 0.08399017\n",
      "Iteration 403, loss = 0.08380874\n",
      "Iteration 404, loss = 0.08362829\n",
      "Iteration 405, loss = 0.08344878\n",
      "Iteration 406, loss = 0.08327023\n",
      "Iteration 407, loss = 0.08309263\n",
      "Iteration 408, loss = 0.08291596\n",
      "Iteration 409, loss = 0.08274023\n",
      "Iteration 410, loss = 0.08256542\n",
      "Iteration 411, loss = 0.08239153\n",
      "Iteration 412, loss = 0.08221855\n",
      "Iteration 413, loss = 0.08204648\n",
      "Iteration 414, loss = 0.08187531\n",
      "Iteration 415, loss = 0.08170503\n",
      "Iteration 416, loss = 0.08153564\n",
      "Iteration 417, loss = 0.08136713\n",
      "Iteration 418, loss = 0.08119950\n",
      "Iteration 419, loss = 0.08103273\n",
      "Iteration 420, loss = 0.08086683\n",
      "Iteration 421, loss = 0.08070179\n",
      "Iteration 422, loss = 0.08053760\n",
      "Iteration 423, loss = 0.08037425\n",
      "Iteration 424, loss = 0.08021174\n",
      "Iteration 425, loss = 0.08005007\n",
      "Iteration 426, loss = 0.07988923\n",
      "Iteration 427, loss = 0.07972920\n",
      "Iteration 428, loss = 0.07957000\n",
      "Iteration 429, loss = 0.07941161\n",
      "Iteration 430, loss = 0.07925402\n",
      "Iteration 431, loss = 0.07909724\n",
      "Iteration 432, loss = 0.07894125\n",
      "Iteration 433, loss = 0.07878605\n",
      "Iteration 434, loss = 0.07863163\n",
      "Iteration 435, loss = 0.07847800\n",
      "Iteration 436, loss = 0.07832513\n",
      "Iteration 437, loss = 0.07817304\n",
      "Iteration 438, loss = 0.07802171\n",
      "Iteration 439, loss = 0.07787114\n",
      "Iteration 440, loss = 0.07772132\n",
      "Iteration 441, loss = 0.07757226\n",
      "Iteration 442, loss = 0.07742393\n",
      "Iteration 443, loss = 0.07727635\n",
      "Iteration 444, loss = 0.07712950\n",
      "Iteration 445, loss = 0.07698337\n",
      "Iteration 446, loss = 0.07683797\n",
      "Iteration 447, loss = 0.07669329\n",
      "Iteration 448, loss = 0.07654933\n",
      "Iteration 449, loss = 0.07640607\n",
      "Iteration 450, loss = 0.07626352\n",
      "Iteration 451, loss = 0.07612167\n",
      "Iteration 452, loss = 0.07598052\n",
      "Iteration 453, loss = 0.07584006\n",
      "Iteration 454, loss = 0.07570028\n",
      "Iteration 455, loss = 0.07556119\n",
      "Iteration 456, loss = 0.07542278\n",
      "Iteration 457, loss = 0.07528503\n",
      "Iteration 458, loss = 0.07514796\n",
      "Iteration 459, loss = 0.07501155\n",
      "Iteration 460, loss = 0.07487581\n",
      "Iteration 461, loss = 0.07474072\n",
      "Iteration 462, loss = 0.07460628\n",
      "Iteration 463, loss = 0.07447249\n",
      "Iteration 464, loss = 0.07433934\n",
      "Iteration 465, loss = 0.07420683\n",
      "Iteration 466, loss = 0.07407496\n",
      "Iteration 467, loss = 0.07394371\n",
      "Iteration 468, loss = 0.07381310\n",
      "Iteration 469, loss = 0.07368311\n",
      "Iteration 470, loss = 0.07355373\n",
      "Iteration 471, loss = 0.07342498\n",
      "Iteration 472, loss = 0.07329683\n",
      "Iteration 473, loss = 0.07316929\n",
      "Iteration 474, loss = 0.07304236\n",
      "Iteration 475, loss = 0.07291602\n",
      "Iteration 476, loss = 0.07279028\n",
      "Iteration 477, loss = 0.07266513\n",
      "Iteration 478, loss = 0.07254057\n",
      "Iteration 479, loss = 0.07241660\n",
      "Iteration 480, loss = 0.07229320\n",
      "Iteration 481, loss = 0.07217039\n",
      "Iteration 482, loss = 0.07204814\n",
      "Iteration 483, loss = 0.07192647\n",
      "Iteration 484, loss = 0.07180536\n",
      "Iteration 485, loss = 0.07168482\n",
      "Iteration 486, loss = 0.07156484\n",
      "Iteration 487, loss = 0.07144541\n",
      "Iteration 488, loss = 0.07132653\n",
      "Iteration 489, loss = 0.07120820\n",
      "Iteration 490, loss = 0.07109042\n",
      "Iteration 491, loss = 0.07097318\n",
      "Iteration 492, loss = 0.07085648\n",
      "Iteration 493, loss = 0.07074032\n",
      "Iteration 494, loss = 0.07062469\n",
      "Iteration 495, loss = 0.07050958\n",
      "Iteration 496, loss = 0.07039500\n",
      "Iteration 497, loss = 0.07028095\n",
      "Iteration 498, loss = 0.07016741\n",
      "Iteration 499, loss = 0.07005439\n",
      "Iteration 500, loss = 0.06994189\n",
      "Iteration 1, loss = 1.06536635\n",
      "Iteration 2, loss = 1.04609171\n",
      "Iteration 3, loss = 1.01953356\n",
      "Iteration 4, loss = 0.98733547\n",
      "Iteration 5, loss = 0.95110106\n",
      "Iteration 6, loss = 0.91233313\n",
      "Iteration 7, loss = 0.87238410\n",
      "Iteration 8, loss = 0.83241849\n",
      "Iteration 9, loss = 0.79338886\n",
      "Iteration 10, loss = 0.75602657\n",
      "Iteration 11, loss = 0.72084754\n",
      "Iteration 12, loss = 0.68817121\n",
      "Iteration 13, loss = 0.65814915\n",
      "Iteration 14, loss = 0.63079888\n",
      "Iteration 15, loss = 0.60603840\n",
      "Iteration 16, loss = 0.58371794\n",
      "Iteration 17, loss = 0.56364693\n",
      "Iteration 18, loss = 0.54561507\n",
      "Iteration 19, loss = 0.52940787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 20, loss = 0.51481725\n",
      "Iteration 21, loss = 0.50164824\n",
      "Iteration 22, loss = 0.48972268\n",
      "Iteration 23, loss = 0.47888083\n",
      "Iteration 24, loss = 0.46898156\n",
      "Iteration 25, loss = 0.45990164\n",
      "Iteration 26, loss = 0.45153449\n",
      "Iteration 27, loss = 0.44378865\n",
      "Iteration 28, loss = 0.43658609\n",
      "Iteration 29, loss = 0.42986060\n",
      "Iteration 30, loss = 0.42355619\n",
      "Iteration 31, loss = 0.41762556\n",
      "Iteration 32, loss = 0.41202881\n",
      "Iteration 33, loss = 0.40673217\n",
      "Iteration 34, loss = 0.40170699\n",
      "Iteration 35, loss = 0.39692876\n",
      "Iteration 36, loss = 0.39237636\n",
      "Iteration 37, loss = 0.38803142\n",
      "Iteration 38, loss = 0.38387777\n",
      "Iteration 39, loss = 0.37990098\n",
      "Iteration 40, loss = 0.37608806\n",
      "Iteration 41, loss = 0.37242719\n",
      "Iteration 42, loss = 0.36890750\n",
      "Iteration 43, loss = 0.36551895\n",
      "Iteration 44, loss = 0.36225219\n",
      "Iteration 45, loss = 0.35909853\n",
      "Iteration 46, loss = 0.35604985\n",
      "Iteration 47, loss = 0.35309860\n",
      "Iteration 48, loss = 0.35023774\n",
      "Iteration 49, loss = 0.34746073\n",
      "Iteration 50, loss = 0.34476154\n",
      "Iteration 51, loss = 0.34213459\n",
      "Iteration 52, loss = 0.33957476\n",
      "Iteration 53, loss = 0.33707736\n",
      "Iteration 54, loss = 0.33463811\n",
      "Iteration 55, loss = 0.33225312\n",
      "Iteration 56, loss = 0.32991885\n",
      "Iteration 57, loss = 0.32763209\n",
      "Iteration 58, loss = 0.32538993\n",
      "Iteration 59, loss = 0.32318976\n",
      "Iteration 60, loss = 0.32102919\n",
      "Iteration 61, loss = 0.31890604\n",
      "Iteration 62, loss = 0.31681837\n",
      "Iteration 63, loss = 0.31476437\n",
      "Iteration 64, loss = 0.31274241\n",
      "Iteration 65, loss = 0.31075099\n",
      "Iteration 66, loss = 0.30878871\n",
      "Iteration 67, loss = 0.30685429\n",
      "Iteration 68, loss = 0.30494655\n",
      "Iteration 69, loss = 0.30306438\n",
      "Iteration 70, loss = 0.30120674\n",
      "Iteration 71, loss = 0.29937266\n",
      "Iteration 72, loss = 0.29756124\n",
      "Iteration 73, loss = 0.29577162\n",
      "Iteration 74, loss = 0.29400301\n",
      "Iteration 75, loss = 0.29225465\n",
      "Iteration 76, loss = 0.29052583\n",
      "Iteration 77, loss = 0.28881590\n",
      "Iteration 78, loss = 0.28712422\n",
      "Iteration 79, loss = 0.28545022\n",
      "Iteration 80, loss = 0.28379334\n",
      "Iteration 81, loss = 0.28215308\n",
      "Iteration 82, loss = 0.28052896\n",
      "Iteration 83, loss = 0.27892053\n",
      "Iteration 84, loss = 0.27732736\n",
      "Iteration 85, loss = 0.27574908\n",
      "Iteration 86, loss = 0.27418531\n",
      "Iteration 87, loss = 0.27263570\n",
      "Iteration 88, loss = 0.27109995\n",
      "Iteration 89, loss = 0.26957774\n",
      "Iteration 90, loss = 0.26806880\n",
      "Iteration 91, loss = 0.26657286\n",
      "Iteration 92, loss = 0.26508967\n",
      "Iteration 93, loss = 0.26361899\n",
      "Iteration 94, loss = 0.26216060\n",
      "Iteration 95, loss = 0.26071429\n",
      "Iteration 96, loss = 0.25927986\n",
      "Iteration 97, loss = 0.25785711\n",
      "Iteration 98, loss = 0.25644587\n",
      "Iteration 99, loss = 0.25504595\n",
      "Iteration 100, loss = 0.25365721\n",
      "Iteration 101, loss = 0.25227947\n",
      "Iteration 102, loss = 0.25091258\n",
      "Iteration 103, loss = 0.24955641\n",
      "Iteration 104, loss = 0.24821080\n",
      "Iteration 105, loss = 0.24687563\n",
      "Iteration 106, loss = 0.24555077\n",
      "Iteration 107, loss = 0.24423609\n",
      "Iteration 108, loss = 0.24293147\n",
      "Iteration 109, loss = 0.24163680\n",
      "Iteration 110, loss = 0.24035196\n",
      "Iteration 111, loss = 0.23907685\n",
      "Iteration 112, loss = 0.23781137\n",
      "Iteration 113, loss = 0.23655541\n",
      "Iteration 114, loss = 0.23530887\n",
      "Iteration 115, loss = 0.23407167\n",
      "Iteration 116, loss = 0.23284371\n",
      "Iteration 117, loss = 0.23162491\n",
      "Iteration 118, loss = 0.23041516\n",
      "Iteration 119, loss = 0.22921440\n",
      "Iteration 120, loss = 0.22802254\n",
      "Iteration 121, loss = 0.22683950\n",
      "Iteration 122, loss = 0.22566520\n",
      "Iteration 123, loss = 0.22449957\n",
      "Iteration 124, loss = 0.22334252\n",
      "Iteration 125, loss = 0.22219400\n",
      "Iteration 126, loss = 0.22105392\n",
      "Iteration 127, loss = 0.21992222\n",
      "Iteration 128, loss = 0.21879883\n",
      "Iteration 129, loss = 0.21768368\n",
      "Iteration 130, loss = 0.21657670\n",
      "Iteration 131, loss = 0.21547783\n",
      "Iteration 132, loss = 0.21438701\n",
      "Iteration 133, loss = 0.21330417\n",
      "Iteration 134, loss = 0.21222925\n",
      "Iteration 135, loss = 0.21116219\n",
      "Iteration 136, loss = 0.21010292\n",
      "Iteration 137, loss = 0.20905140\n",
      "Iteration 138, loss = 0.20800756\n",
      "Iteration 139, loss = 0.20697133\n",
      "Iteration 140, loss = 0.20594268\n",
      "Iteration 141, loss = 0.20492153\n",
      "Iteration 142, loss = 0.20390783\n",
      "Iteration 143, loss = 0.20290153\n",
      "Iteration 144, loss = 0.20190258\n",
      "Iteration 145, loss = 0.20091091\n",
      "Iteration 146, loss = 0.19992648\n",
      "Iteration 147, loss = 0.19894922\n",
      "Iteration 148, loss = 0.19797910\n",
      "Iteration 149, loss = 0.19701605\n",
      "Iteration 150, loss = 0.19606003\n",
      "Iteration 151, loss = 0.19511098\n",
      "Iteration 152, loss = 0.19416886\n",
      "Iteration 153, loss = 0.19323361\n",
      "Iteration 154, loss = 0.19230518\n",
      "Iteration 155, loss = 0.19138353\n",
      "Iteration 156, loss = 0.19046860\n",
      "Iteration 157, loss = 0.18956034\n",
      "Iteration 158, loss = 0.18865872\n",
      "Iteration 159, loss = 0.18776367\n",
      "Iteration 160, loss = 0.18687515\n",
      "Iteration 161, loss = 0.18599312\n",
      "Iteration 162, loss = 0.18511752\n",
      "Iteration 163, loss = 0.18424832\n",
      "Iteration 164, loss = 0.18338546\n",
      "Iteration 165, loss = 0.18252890\n",
      "Iteration 166, loss = 0.18167859\n",
      "Iteration 167, loss = 0.18083449\n",
      "Iteration 168, loss = 0.17999655\n",
      "Iteration 169, loss = 0.17916472\n",
      "Iteration 170, loss = 0.17833897\n",
      "Iteration 171, loss = 0.17751924\n",
      "Iteration 172, loss = 0.17670550\n",
      "Iteration 173, loss = 0.17589770\n",
      "Iteration 174, loss = 0.17509579\n",
      "Iteration 175, loss = 0.17429974\n",
      "Iteration 176, loss = 0.17350950\n",
      "Iteration 177, loss = 0.17272502\n",
      "Iteration 178, loss = 0.17194626\n",
      "Iteration 179, loss = 0.17117319\n",
      "Iteration 180, loss = 0.17040576\n",
      "Iteration 181, loss = 0.16964393\n",
      "Iteration 182, loss = 0.16888765\n",
      "Iteration 183, loss = 0.16813689\n",
      "Iteration 184, loss = 0.16739160\n",
      "Iteration 185, loss = 0.16665174\n",
      "Iteration 186, loss = 0.16591728\n",
      "Iteration 187, loss = 0.16518816\n",
      "Iteration 188, loss = 0.16446436\n",
      "Iteration 189, loss = 0.16374583\n",
      "Iteration 190, loss = 0.16303253\n",
      "Iteration 191, loss = 0.16232443\n",
      "Iteration 192, loss = 0.16162147\n",
      "Iteration 193, loss = 0.16092363\n",
      "Iteration 194, loss = 0.16023087\n",
      "Iteration 195, loss = 0.15954314\n",
      "Iteration 196, loss = 0.15886041\n",
      "Iteration 197, loss = 0.15818264\n",
      "Iteration 198, loss = 0.15750979\n",
      "Iteration 199, loss = 0.15684182\n",
      "Iteration 200, loss = 0.15617870\n",
      "Iteration 201, loss = 0.15552039\n",
      "Iteration 202, loss = 0.15486685\n",
      "Iteration 203, loss = 0.15421804\n",
      "Iteration 204, loss = 0.15357394\n",
      "Iteration 205, loss = 0.15293449\n",
      "Iteration 206, loss = 0.15229968\n",
      "Iteration 207, loss = 0.15166945\n",
      "Iteration 208, loss = 0.15104377\n",
      "Iteration 209, loss = 0.15042261\n",
      "Iteration 210, loss = 0.14980594\n",
      "Iteration 211, loss = 0.14919371\n",
      "Iteration 212, loss = 0.14858590\n",
      "Iteration 213, loss = 0.14798247\n",
      "Iteration 214, loss = 0.14738338\n",
      "Iteration 215, loss = 0.14678859\n",
      "Iteration 216, loss = 0.14619809\n",
      "Iteration 217, loss = 0.14561182\n",
      "Iteration 218, loss = 0.14502977\n",
      "Iteration 219, loss = 0.14445189\n",
      "Iteration 220, loss = 0.14387815\n",
      "Iteration 221, loss = 0.14330852\n",
      "Iteration 222, loss = 0.14274296\n",
      "Iteration 223, loss = 0.14218145\n",
      "Iteration 224, loss = 0.14162395\n",
      "Iteration 225, loss = 0.14107043\n",
      "Iteration 226, loss = 0.14052086\n",
      "Iteration 227, loss = 0.13997520\n",
      "Iteration 228, loss = 0.13943343\n",
      "Iteration 229, loss = 0.13889551\n",
      "Iteration 230, loss = 0.13836141\n",
      "Iteration 231, loss = 0.13783110\n",
      "Iteration 232, loss = 0.13730455\n",
      "Iteration 233, loss = 0.13678174\n",
      "Iteration 234, loss = 0.13626262\n",
      "Iteration 235, loss = 0.13574718\n",
      "Iteration 236, loss = 0.13523537\n",
      "Iteration 237, loss = 0.13472718\n",
      "Iteration 238, loss = 0.13422257\n",
      "Iteration 239, loss = 0.13372151\n",
      "Iteration 240, loss = 0.13322397\n",
      "Iteration 241, loss = 0.13272993\n",
      "Iteration 242, loss = 0.13223936\n",
      "Iteration 243, loss = 0.13175222\n",
      "Iteration 244, loss = 0.13126850\n",
      "Iteration 245, loss = 0.13078816\n",
      "Iteration 246, loss = 0.13031117\n",
      "Iteration 247, loss = 0.12983751\n",
      "Iteration 248, loss = 0.12936715\n",
      "Iteration 249, loss = 0.12890007\n",
      "Iteration 250, loss = 0.12843623\n",
      "Iteration 251, loss = 0.12797561\n",
      "Iteration 252, loss = 0.12751819\n",
      "Iteration 253, loss = 0.12706393\n",
      "Iteration 254, loss = 0.12661282\n",
      "Iteration 255, loss = 0.12616482\n",
      "Iteration 256, loss = 0.12571991\n",
      "Iteration 257, loss = 0.12527807\n",
      "Iteration 258, loss = 0.12483926\n",
      "Iteration 259, loss = 0.12440348\n",
      "Iteration 260, loss = 0.12397068\n",
      "Iteration 261, loss = 0.12354084\n",
      "Iteration 262, loss = 0.12311395\n",
      "Iteration 263, loss = 0.12268997\n",
      "Iteration 264, loss = 0.12226889\n",
      "Iteration 265, loss = 0.12185068\n",
      "Iteration 266, loss = 0.12143531\n",
      "Iteration 267, loss = 0.12102276\n",
      "Iteration 268, loss = 0.12061301\n",
      "Iteration 269, loss = 0.12020604\n",
      "Iteration 270, loss = 0.11980182\n",
      "Iteration 271, loss = 0.11940033\n",
      "Iteration 272, loss = 0.11900155\n",
      "Iteration 273, loss = 0.11860545\n",
      "Iteration 274, loss = 0.11821202\n",
      "Iteration 275, loss = 0.11782123\n",
      "Iteration 276, loss = 0.11743306\n",
      "Iteration 277, loss = 0.11704748\n",
      "Iteration 278, loss = 0.11666448\n",
      "Iteration 279, loss = 0.11628404\n",
      "Iteration 280, loss = 0.11590613\n",
      "Iteration 281, loss = 0.11553074\n",
      "Iteration 282, loss = 0.11515783\n",
      "Iteration 283, loss = 0.11478740\n",
      "Iteration 284, loss = 0.11441942\n",
      "Iteration 285, loss = 0.11405388\n",
      "Iteration 286, loss = 0.11369074\n",
      "Iteration 287, loss = 0.11333000\n",
      "Iteration 288, loss = 0.11297162\n",
      "Iteration 289, loss = 0.11261561\n",
      "Iteration 290, loss = 0.11226192\n",
      "Iteration 291, loss = 0.11191055\n",
      "Iteration 292, loss = 0.11156147\n",
      "Iteration 293, loss = 0.11121467\n",
      "Iteration 294, loss = 0.11087013\n",
      "Iteration 295, loss = 0.11052782\n",
      "Iteration 296, loss = 0.11018774\n",
      "Iteration 297, loss = 0.10984986\n",
      "Iteration 298, loss = 0.10951416\n",
      "Iteration 299, loss = 0.10918063\n",
      "Iteration 300, loss = 0.10884925\n",
      "Iteration 301, loss = 0.10852000\n",
      "Iteration 302, loss = 0.10819286\n",
      "Iteration 303, loss = 0.10786782\n",
      "Iteration 304, loss = 0.10754486\n",
      "Iteration 305, loss = 0.10722396\n",
      "Iteration 306, loss = 0.10690511\n",
      "Iteration 307, loss = 0.10658828\n",
      "Iteration 308, loss = 0.10627347\n",
      "Iteration 309, loss = 0.10596066\n",
      "Iteration 310, loss = 0.10564982\n",
      "Iteration 311, loss = 0.10534095\n",
      "Iteration 312, loss = 0.10503402\n",
      "Iteration 313, loss = 0.10472903\n",
      "Iteration 314, loss = 0.10442595\n",
      "Iteration 315, loss = 0.10412477\n",
      "Iteration 316, loss = 0.10382548\n",
      "Iteration 317, loss = 0.10352806\n",
      "Iteration 318, loss = 0.10323249\n",
      "Iteration 319, loss = 0.10293877\n",
      "Iteration 320, loss = 0.10264686\n",
      "Iteration 321, loss = 0.10235677\n",
      "Iteration 322, loss = 0.10206848\n",
      "Iteration 323, loss = 0.10178196\n",
      "Iteration 324, loss = 0.10149721\n",
      "Iteration 325, loss = 0.10121421\n",
      "Iteration 326, loss = 0.10093295\n",
      "Iteration 327, loss = 0.10065342\n",
      "Iteration 328, loss = 0.10037560\n",
      "Iteration 329, loss = 0.10009947\n",
      "Iteration 330, loss = 0.09982503\n",
      "Iteration 331, loss = 0.09955225\n",
      "Iteration 332, loss = 0.09928114\n",
      "Iteration 333, loss = 0.09901166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 334, loss = 0.09874382\n",
      "Iteration 335, loss = 0.09847759\n",
      "Iteration 336, loss = 0.09821297\n",
      "Iteration 337, loss = 0.09794993\n",
      "Iteration 338, loss = 0.09768848\n",
      "Iteration 339, loss = 0.09742859\n",
      "Iteration 340, loss = 0.09717026\n",
      "Iteration 341, loss = 0.09691347\n",
      "Iteration 342, loss = 0.09665820\n",
      "Iteration 343, loss = 0.09640446\n",
      "Iteration 344, loss = 0.09615222\n",
      "Iteration 345, loss = 0.09590147\n",
      "Iteration 346, loss = 0.09565220\n",
      "Iteration 347, loss = 0.09540441\n",
      "Iteration 348, loss = 0.09515807\n",
      "Iteration 349, loss = 0.09491317\n",
      "Iteration 350, loss = 0.09466972\n",
      "Iteration 351, loss = 0.09442768\n",
      "Iteration 352, loss = 0.09418706\n",
      "Iteration 353, loss = 0.09394785\n",
      "Iteration 354, loss = 0.09371002\n",
      "Iteration 355, loss = 0.09347357\n",
      "Iteration 356, loss = 0.09323849\n",
      "Iteration 357, loss = 0.09300477\n",
      "Iteration 358, loss = 0.09277240\n",
      "Iteration 359, loss = 0.09254137\n",
      "Iteration 360, loss = 0.09231166\n",
      "Iteration 361, loss = 0.09208327\n",
      "Iteration 362, loss = 0.09185618\n",
      "Iteration 363, loss = 0.09163039\n",
      "Iteration 364, loss = 0.09140589\n",
      "Iteration 365, loss = 0.09118267\n",
      "Iteration 366, loss = 0.09096071\n",
      "Iteration 367, loss = 0.09074001\n",
      "Iteration 368, loss = 0.09052055\n",
      "Iteration 369, loss = 0.09030233\n",
      "Iteration 370, loss = 0.09008535\n",
      "Iteration 371, loss = 0.08986958\n",
      "Iteration 372, loss = 0.08965501\n",
      "Iteration 373, loss = 0.08944165\n",
      "Iteration 374, loss = 0.08922948\n",
      "Iteration 375, loss = 0.08901849\n",
      "Iteration 376, loss = 0.08880868\n",
      "Iteration 377, loss = 0.08860003\n",
      "Iteration 378, loss = 0.08839253\n",
      "Iteration 379, loss = 0.08818618\n",
      "Iteration 380, loss = 0.08798097\n",
      "Iteration 381, loss = 0.08777688\n",
      "Iteration 382, loss = 0.08757392\n",
      "Iteration 383, loss = 0.08737207\n",
      "Iteration 384, loss = 0.08717132\n",
      "Iteration 385, loss = 0.08697167\n",
      "Iteration 386, loss = 0.08677310\n",
      "Iteration 387, loss = 0.08657562\n",
      "Iteration 388, loss = 0.08637920\n",
      "Iteration 389, loss = 0.08618385\n",
      "Iteration 390, loss = 0.08598955\n",
      "Iteration 391, loss = 0.08579630\n",
      "Iteration 392, loss = 0.08560409\n",
      "Iteration 393, loss = 0.08541291\n",
      "Iteration 394, loss = 0.08522275\n",
      "Iteration 395, loss = 0.08503361\n",
      "Iteration 396, loss = 0.08484548\n",
      "Iteration 397, loss = 0.08465835\n",
      "Iteration 398, loss = 0.08447221\n",
      "Iteration 399, loss = 0.08428706\n",
      "Iteration 400, loss = 0.08410288\n",
      "Iteration 401, loss = 0.08391968\n",
      "Iteration 402, loss = 0.08373745\n",
      "Iteration 403, loss = 0.08355617\n",
      "Iteration 404, loss = 0.08337584\n",
      "Iteration 405, loss = 0.08319646\n",
      "Iteration 406, loss = 0.08301801\n",
      "Iteration 407, loss = 0.08284050\n",
      "Iteration 408, loss = 0.08266390\n",
      "Iteration 409, loss = 0.08248822\n",
      "Iteration 410, loss = 0.08231346\n",
      "Iteration 411, loss = 0.08213959\n",
      "Iteration 412, loss = 0.08196662\n",
      "Iteration 413, loss = 0.08179454\n",
      "Iteration 414, loss = 0.08162335\n",
      "Iteration 415, loss = 0.08145303\n",
      "Iteration 416, loss = 0.08128358\n",
      "Iteration 417, loss = 0.08111500\n",
      "Iteration 418, loss = 0.08094728\n",
      "Iteration 419, loss = 0.08078041\n",
      "Iteration 420, loss = 0.08061438\n",
      "Iteration 421, loss = 0.08044920\n",
      "Iteration 422, loss = 0.08028484\n",
      "Iteration 423, loss = 0.08012132\n",
      "Iteration 424, loss = 0.07995862\n",
      "Iteration 425, loss = 0.07979673\n",
      "Iteration 426, loss = 0.07963566\n",
      "Iteration 427, loss = 0.07947539\n",
      "Iteration 428, loss = 0.07931592\n",
      "Iteration 429, loss = 0.07915724\n",
      "Iteration 430, loss = 0.07899935\n",
      "Iteration 431, loss = 0.07884225\n",
      "Iteration 432, loss = 0.07868592\n",
      "Iteration 433, loss = 0.07853036\n",
      "Iteration 434, loss = 0.07837556\n",
      "Iteration 435, loss = 0.07822153\n",
      "Iteration 436, loss = 0.07806825\n",
      "Iteration 437, loss = 0.07791573\n",
      "Iteration 438, loss = 0.07776394\n",
      "Iteration 439, loss = 0.07761290\n",
      "Iteration 440, loss = 0.07746259\n",
      "Iteration 441, loss = 0.07731301\n",
      "Iteration 442, loss = 0.07716415\n",
      "Iteration 443, loss = 0.07701602\n",
      "Iteration 444, loss = 0.07686859\n",
      "Iteration 445, loss = 0.07672188\n",
      "Iteration 446, loss = 0.07657587\n",
      "Iteration 447, loss = 0.07643056\n",
      "Iteration 448, loss = 0.07628594\n",
      "Iteration 449, loss = 0.07614202\n",
      "Iteration 450, loss = 0.07599878\n",
      "Iteration 451, loss = 0.07585622\n",
      "Iteration 452, loss = 0.07571433\n",
      "Iteration 453, loss = 0.07557312\n",
      "Iteration 454, loss = 0.07543258\n",
      "Iteration 455, loss = 0.07529269\n",
      "Iteration 456, loss = 0.07515346\n",
      "Iteration 457, loss = 0.07501489\n",
      "Iteration 458, loss = 0.07487697\n",
      "Iteration 459, loss = 0.07473968\n",
      "Iteration 460, loss = 0.07460304\n",
      "Iteration 461, loss = 0.07446704\n",
      "Iteration 462, loss = 0.07433166\n",
      "Iteration 463, loss = 0.07419692\n",
      "Iteration 464, loss = 0.07406279\n",
      "Iteration 465, loss = 0.07392929\n",
      "Iteration 466, loss = 0.07379640\n",
      "Iteration 467, loss = 0.07366412\n",
      "Iteration 468, loss = 0.07353244\n",
      "Iteration 469, loss = 0.07340137\n",
      "Iteration 470, loss = 0.07327090\n",
      "Iteration 471, loss = 0.07314102\n",
      "Iteration 472, loss = 0.07301173\n",
      "Iteration 473, loss = 0.07288303\n",
      "Iteration 474, loss = 0.07275491\n",
      "Iteration 475, loss = 0.07262737\n",
      "Iteration 476, loss = 0.07250040\n",
      "Iteration 477, loss = 0.07237401\n",
      "Iteration 478, loss = 0.07224818\n",
      "Iteration 479, loss = 0.07212292\n",
      "Iteration 480, loss = 0.07199821\n",
      "Iteration 481, loss = 0.07187407\n",
      "Iteration 482, loss = 0.07175047\n",
      "Iteration 483, loss = 0.07162743\n",
      "Iteration 484, loss = 0.07150493\n",
      "Iteration 485, loss = 0.07138297\n",
      "Iteration 486, loss = 0.07126155\n",
      "Iteration 487, loss = 0.07114067\n",
      "Iteration 488, loss = 0.07102031\n",
      "Iteration 489, loss = 0.07090049\n",
      "Iteration 490, loss = 0.07078119\n",
      "Iteration 491, loss = 0.07066241\n",
      "Iteration 492, loss = 0.07054415\n",
      "Iteration 493, loss = 0.07042640\n",
      "Iteration 494, loss = 0.07030917\n",
      "Iteration 495, loss = 0.07019244\n",
      "Iteration 496, loss = 0.07007622\n",
      "Iteration 497, loss = 0.06996050\n",
      "Iteration 498, loss = 0.06984528\n",
      "Iteration 499, loss = 0.06973055\n",
      "Iteration 500, loss = 0.06961631\n",
      "Iteration 1, loss = 1.07068620\n",
      "Iteration 2, loss = 1.05097957\n",
      "Iteration 3, loss = 1.02385933\n",
      "Iteration 4, loss = 0.99103085\n",
      "Iteration 5, loss = 0.95415446\n",
      "Iteration 6, loss = 0.91477967\n",
      "Iteration 7, loss = 0.87429288\n",
      "Iteration 8, loss = 0.83387932\n",
      "Iteration 9, loss = 0.79450048\n",
      "Iteration 10, loss = 0.75688783\n",
      "Iteration 11, loss = 0.72155183\n",
      "Iteration 12, loss = 0.68880377\n",
      "Iteration 13, loss = 0.65878634\n",
      "Iteration 14, loss = 0.63150841\n",
      "Iteration 15, loss = 0.60687987\n",
      "Iteration 16, loss = 0.58474336\n",
      "Iteration 17, loss = 0.56490096\n",
      "Iteration 18, loss = 0.54713531\n",
      "Iteration 19, loss = 0.53122508\n",
      "Iteration 20, loss = 0.51695570\n",
      "Iteration 21, loss = 0.50412616\n",
      "Iteration 22, loss = 0.49255284\n",
      "Iteration 23, loss = 0.48207125\n",
      "Iteration 24, loss = 0.47253622\n",
      "Iteration 25, loss = 0.46382124\n",
      "Iteration 26, loss = 0.45581715\n",
      "Iteration 27, loss = 0.44843054\n",
      "Iteration 28, loss = 0.44158206\n",
      "Iteration 29, loss = 0.43520460\n",
      "Iteration 30, loss = 0.42924169\n",
      "Iteration 31, loss = 0.42364583\n",
      "Iteration 32, loss = 0.41837711\n",
      "Iteration 33, loss = 0.41340191\n",
      "Iteration 34, loss = 0.40869178\n",
      "Iteration 35, loss = 0.40422244\n",
      "Iteration 36, loss = 0.39997302\n",
      "Iteration 37, loss = 0.39592534\n",
      "Iteration 38, loss = 0.39206338\n",
      "Iteration 39, loss = 0.38837284\n",
      "Iteration 40, loss = 0.38484080\n",
      "Iteration 41, loss = 0.38145548\n",
      "Iteration 42, loss = 0.37820604\n",
      "Iteration 43, loss = 0.37508245\n",
      "Iteration 44, loss = 0.37207537\n",
      "Iteration 45, loss = 0.36917613\n",
      "Iteration 46, loss = 0.36637665\n",
      "Iteration 47, loss = 0.36366943\n",
      "Iteration 48, loss = 0.36104752\n",
      "Iteration 49, loss = 0.35850451\n",
      "Iteration 50, loss = 0.35603449\n",
      "Iteration 51, loss = 0.35363205\n",
      "Iteration 52, loss = 0.35129225\n",
      "Iteration 53, loss = 0.34901062\n",
      "Iteration 54, loss = 0.34678309\n",
      "Iteration 55, loss = 0.34460598\n",
      "Iteration 56, loss = 0.34247598\n",
      "Iteration 57, loss = 0.34039010\n",
      "Iteration 58, loss = 0.33834564\n",
      "Iteration 59, loss = 0.33634019\n",
      "Iteration 60, loss = 0.33437155\n",
      "Iteration 61, loss = 0.33243772\n",
      "Iteration 62, loss = 0.33053690\n",
      "Iteration 63, loss = 0.32866744\n",
      "Iteration 64, loss = 0.32682782\n",
      "Iteration 65, loss = 0.32501665\n",
      "Iteration 66, loss = 0.32323262\n",
      "Iteration 67, loss = 0.32147455\n",
      "Iteration 68, loss = 0.31974129\n",
      "Iteration 69, loss = 0.31803179\n",
      "Iteration 70, loss = 0.31634507\n",
      "Iteration 71, loss = 0.31468019\n",
      "Iteration 72, loss = 0.31303627\n",
      "Iteration 73, loss = 0.31141248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74, loss = 0.30980805\n",
      "Iteration 75, loss = 0.30822223\n",
      "Iteration 76, loss = 0.30665434\n",
      "Iteration 77, loss = 0.30510372\n",
      "Iteration 78, loss = 0.30356976\n",
      "Iteration 79, loss = 0.30205190\n",
      "Iteration 80, loss = 0.30054959\n",
      "Iteration 81, loss = 0.29906233\n",
      "Iteration 82, loss = 0.29758965\n",
      "Iteration 83, loss = 0.29613112\n",
      "Iteration 84, loss = 0.29468632\n",
      "Iteration 85, loss = 0.29325487\n",
      "Iteration 86, loss = 0.29183641\n",
      "Iteration 87, loss = 0.29043061\n",
      "Iteration 88, loss = 0.28903716\n",
      "Iteration 89, loss = 0.28765575\n",
      "Iteration 90, loss = 0.28628611\n",
      "Iteration 91, loss = 0.28492798\n",
      "Iteration 92, loss = 0.28358112\n",
      "Iteration 93, loss = 0.28224529\n",
      "Iteration 94, loss = 0.28092027\n",
      "Iteration 95, loss = 0.27960586\n",
      "Iteration 96, loss = 0.27830186\n",
      "Iteration 97, loss = 0.27700807\n",
      "Iteration 98, loss = 0.27572432\n",
      "Iteration 99, loss = 0.27445045\n",
      "Iteration 100, loss = 0.27318628\n",
      "Iteration 101, loss = 0.27193166\n",
      "Iteration 102, loss = 0.27068645\n",
      "Iteration 103, loss = 0.26945050\n",
      "Iteration 104, loss = 0.26822368\n",
      "Iteration 105, loss = 0.26700586\n",
      "Iteration 106, loss = 0.26579691\n",
      "Iteration 107, loss = 0.26459671\n",
      "Iteration 108, loss = 0.26340516\n",
      "Iteration 109, loss = 0.26222213\n",
      "Iteration 110, loss = 0.26104753\n",
      "Iteration 111, loss = 0.25988126\n",
      "Iteration 112, loss = 0.25872322\n",
      "Iteration 113, loss = 0.25757331\n",
      "Iteration 114, loss = 0.25643144\n",
      "Iteration 115, loss = 0.25529754\n",
      "Iteration 116, loss = 0.25417150\n",
      "Iteration 117, loss = 0.25305327\n",
      "Iteration 118, loss = 0.25194275\n",
      "Iteration 119, loss = 0.25083986\n",
      "Iteration 120, loss = 0.24974455\n",
      "Iteration 121, loss = 0.24865673\n",
      "Iteration 122, loss = 0.24757633\n",
      "Iteration 123, loss = 0.24650329\n",
      "Iteration 124, loss = 0.24543755\n",
      "Iteration 125, loss = 0.24437903\n",
      "Iteration 126, loss = 0.24332768\n",
      "Iteration 127, loss = 0.24228344\n",
      "Iteration 128, loss = 0.24124624\n",
      "Iteration 129, loss = 0.24021603\n",
      "Iteration 130, loss = 0.23919276\n",
      "Iteration 131, loss = 0.23817635\n",
      "Iteration 132, loss = 0.23716677\n",
      "Iteration 133, loss = 0.23616396\n",
      "Iteration 134, loss = 0.23516787\n",
      "Iteration 135, loss = 0.23417843\n",
      "Iteration 136, loss = 0.23319562\n",
      "Iteration 137, loss = 0.23221936\n",
      "Iteration 138, loss = 0.23124962\n",
      "Iteration 139, loss = 0.23028635\n",
      "Iteration 140, loss = 0.22932950\n",
      "Iteration 141, loss = 0.22837902\n",
      "Iteration 142, loss = 0.22743488\n",
      "Iteration 143, loss = 0.22649701\n",
      "Iteration 144, loss = 0.22556538\n",
      "Iteration 145, loss = 0.22463995\n",
      "Iteration 146, loss = 0.22372067\n",
      "Iteration 147, loss = 0.22280749\n",
      "Iteration 148, loss = 0.22190038\n",
      "Iteration 149, loss = 0.22099930\n",
      "Iteration 150, loss = 0.22010420\n",
      "Iteration 151, loss = 0.21921504\n",
      "Iteration 152, loss = 0.21833178\n",
      "Iteration 153, loss = 0.21745438\n",
      "Iteration 154, loss = 0.21658280\n",
      "Iteration 155, loss = 0.21571701\n",
      "Iteration 156, loss = 0.21485696\n",
      "Iteration 157, loss = 0.21400262\n",
      "Iteration 158, loss = 0.21315394\n",
      "Iteration 159, loss = 0.21231089\n",
      "Iteration 160, loss = 0.21147343\n",
      "Iteration 161, loss = 0.21064153\n",
      "Iteration 162, loss = 0.20981515\n",
      "Iteration 163, loss = 0.20899424\n",
      "Iteration 164, loss = 0.20817878\n",
      "Iteration 165, loss = 0.20736873\n",
      "Iteration 166, loss = 0.20656405\n",
      "Iteration 167, loss = 0.20576471\n",
      "Iteration 168, loss = 0.20497067\n",
      "Iteration 169, loss = 0.20418190\n",
      "Iteration 170, loss = 0.20339836\n",
      "Iteration 171, loss = 0.20262002\n",
      "Iteration 172, loss = 0.20184684\n",
      "Iteration 173, loss = 0.20107879\n",
      "Iteration 174, loss = 0.20031583\n",
      "Iteration 175, loss = 0.19955794\n",
      "Iteration 176, loss = 0.19880507\n",
      "Iteration 177, loss = 0.19805720\n",
      "Iteration 178, loss = 0.19731429\n",
      "Iteration 179, loss = 0.19657631\n",
      "Iteration 180, loss = 0.19584323\n",
      "Iteration 181, loss = 0.19511501\n",
      "Iteration 182, loss = 0.19439162\n",
      "Iteration 183, loss = 0.19367303\n",
      "Iteration 184, loss = 0.19295920\n",
      "Iteration 185, loss = 0.19225011\n",
      "Iteration 186, loss = 0.19154573\n",
      "Iteration 187, loss = 0.19084601\n",
      "Iteration 188, loss = 0.19015094\n",
      "Iteration 189, loss = 0.18946047\n",
      "Iteration 190, loss = 0.18877459\n",
      "Iteration 191, loss = 0.18809325\n",
      "Iteration 192, loss = 0.18741643\n",
      "Iteration 193, loss = 0.18674409\n",
      "Iteration 194, loss = 0.18607621\n",
      "Iteration 195, loss = 0.18541276\n",
      "Iteration 196, loss = 0.18475370\n",
      "Iteration 197, loss = 0.18409901\n",
      "Iteration 198, loss = 0.18344865\n",
      "Iteration 199, loss = 0.18280260\n",
      "Iteration 200, loss = 0.18216083\n",
      "Iteration 201, loss = 0.18152330\n",
      "Iteration 202, loss = 0.18089000\n",
      "Iteration 203, loss = 0.18026088\n",
      "Iteration 204, loss = 0.17963592\n",
      "Iteration 205, loss = 0.17901510\n",
      "Iteration 206, loss = 0.17839838\n",
      "Iteration 207, loss = 0.17778573\n",
      "Iteration 208, loss = 0.17717714\n",
      "Iteration 209, loss = 0.17657256\n",
      "Iteration 210, loss = 0.17597197\n",
      "Iteration 211, loss = 0.17537534\n",
      "Iteration 212, loss = 0.17478265\n",
      "Iteration 213, loss = 0.17419387\n",
      "Iteration 214, loss = 0.17360897\n",
      "Iteration 215, loss = 0.17302792\n",
      "Iteration 216, loss = 0.17245069\n",
      "Iteration 217, loss = 0.17187727\n",
      "Iteration 218, loss = 0.17130762\n",
      "Iteration 219, loss = 0.17074171\n",
      "Iteration 220, loss = 0.17017953\n",
      "Iteration 221, loss = 0.16962103\n",
      "Iteration 222, loss = 0.16906621\n",
      "Iteration 223, loss = 0.16851502\n",
      "Iteration 224, loss = 0.16796745\n",
      "Iteration 225, loss = 0.16742347\n",
      "Iteration 226, loss = 0.16688305\n",
      "Iteration 227, loss = 0.16634617\n",
      "Iteration 228, loss = 0.16581280\n",
      "Iteration 229, loss = 0.16528292\n",
      "Iteration 230, loss = 0.16475651\n",
      "Iteration 231, loss = 0.16423353\n",
      "Iteration 232, loss = 0.16371396\n",
      "Iteration 233, loss = 0.16319778\n",
      "Iteration 234, loss = 0.16268496\n",
      "Iteration 235, loss = 0.16217549\n",
      "Iteration 236, loss = 0.16166933\n",
      "Iteration 237, loss = 0.16116646\n",
      "Iteration 238, loss = 0.16066686\n",
      "Iteration 239, loss = 0.16017050\n",
      "Iteration 240, loss = 0.15967736\n",
      "Iteration 241, loss = 0.15918742\n",
      "Iteration 242, loss = 0.15870065\n",
      "Iteration 243, loss = 0.15821703\n",
      "Iteration 244, loss = 0.15773654\n",
      "Iteration 245, loss = 0.15725916\n",
      "Iteration 246, loss = 0.15678485\n",
      "Iteration 247, loss = 0.15631361\n",
      "Iteration 248, loss = 0.15584540\n",
      "Iteration 249, loss = 0.15538020\n",
      "Iteration 250, loss = 0.15491800\n",
      "Iteration 251, loss = 0.15445877\n",
      "Iteration 252, loss = 0.15400249\n",
      "Iteration 253, loss = 0.15354913\n",
      "Iteration 254, loss = 0.15309868\n",
      "Iteration 255, loss = 0.15265111\n",
      "Iteration 256, loss = 0.15220641\n",
      "Iteration 257, loss = 0.15176454\n",
      "Iteration 258, loss = 0.15132550\n",
      "Iteration 259, loss = 0.15088925\n",
      "Iteration 260, loss = 0.15045579\n",
      "Iteration 261, loss = 0.15002508\n",
      "Iteration 262, loss = 0.14959711\n",
      "Iteration 263, loss = 0.14917186\n",
      "Iteration 264, loss = 0.14874930\n",
      "Iteration 265, loss = 0.14832942\n",
      "Iteration 266, loss = 0.14791220\n",
      "Iteration 267, loss = 0.14749762\n",
      "Iteration 268, loss = 0.14708566\n",
      "Iteration 269, loss = 0.14667629\n",
      "Iteration 270, loss = 0.14626950\n",
      "Iteration 271, loss = 0.14586528\n",
      "Iteration 272, loss = 0.14546359\n",
      "Iteration 273, loss = 0.14506442\n",
      "Iteration 274, loss = 0.14466776\n",
      "Iteration 275, loss = 0.14427359\n",
      "Iteration 276, loss = 0.14388188\n",
      "Iteration 277, loss = 0.14349261\n",
      "Iteration 278, loss = 0.14310578\n",
      "Iteration 279, loss = 0.14272136\n",
      "Iteration 280, loss = 0.14233933\n",
      "Iteration 281, loss = 0.14195968\n",
      "Iteration 282, loss = 0.14158238\n",
      "Iteration 283, loss = 0.14120742\n",
      "Iteration 284, loss = 0.14083479\n",
      "Iteration 285, loss = 0.14046446\n",
      "Iteration 286, loss = 0.14009642\n",
      "Iteration 287, loss = 0.13973065\n",
      "Iteration 288, loss = 0.13936714\n",
      "Iteration 289, loss = 0.13900586\n",
      "Iteration 290, loss = 0.13864680\n",
      "Iteration 291, loss = 0.13828995\n",
      "Iteration 292, loss = 0.13793528\n",
      "Iteration 293, loss = 0.13758279\n",
      "Iteration 294, loss = 0.13723245\n",
      "Iteration 295, loss = 0.13688425\n",
      "Iteration 296, loss = 0.13653817\n",
      "Iteration 297, loss = 0.13619420\n",
      "Iteration 298, loss = 0.13585232\n",
      "Iteration 299, loss = 0.13551251\n",
      "Iteration 300, loss = 0.13517477\n",
      "Iteration 301, loss = 0.13483907\n",
      "Iteration 302, loss = 0.13450540\n",
      "Iteration 303, loss = 0.13417374\n",
      "Iteration 304, loss = 0.13384408\n",
      "Iteration 305, loss = 0.13351641\n",
      "Iteration 306, loss = 0.13319071\n",
      "Iteration 307, loss = 0.13286696\n",
      "Iteration 308, loss = 0.13254515\n",
      "Iteration 309, loss = 0.13222527\n",
      "Iteration 310, loss = 0.13190730\n",
      "Iteration 311, loss = 0.13159123\n",
      "Iteration 312, loss = 0.13127704\n",
      "Iteration 313, loss = 0.13096472\n",
      "Iteration 314, loss = 0.13065426\n",
      "Iteration 315, loss = 0.13034564\n",
      "Iteration 316, loss = 0.13003884\n",
      "Iteration 317, loss = 0.12973386\n",
      "Iteration 318, loss = 0.12943068\n",
      "Iteration 319, loss = 0.12912929\n",
      "Iteration 320, loss = 0.12882968\n",
      "Iteration 321, loss = 0.12853182\n",
      "Iteration 322, loss = 0.12823571\n",
      "Iteration 323, loss = 0.12794134\n",
      "Iteration 324, loss = 0.12764869\n",
      "Iteration 325, loss = 0.12735775\n",
      "Iteration 326, loss = 0.12706851\n",
      "Iteration 327, loss = 0.12678095\n",
      "Iteration 328, loss = 0.12649506\n",
      "Iteration 329, loss = 0.12621084\n",
      "Iteration 330, loss = 0.12592826\n",
      "Iteration 331, loss = 0.12564732\n",
      "Iteration 332, loss = 0.12536800\n",
      "Iteration 333, loss = 0.12509029\n",
      "Iteration 334, loss = 0.12481418\n",
      "Iteration 335, loss = 0.12453966\n",
      "Iteration 336, loss = 0.12426672\n",
      "Iteration 337, loss = 0.12399534\n",
      "Iteration 338, loss = 0.12372551\n",
      "Iteration 339, loss = 0.12345723\n",
      "Iteration 340, loss = 0.12319048\n",
      "Iteration 341, loss = 0.12292524\n",
      "Iteration 342, loss = 0.12266152\n",
      "Iteration 343, loss = 0.12239929\n",
      "Iteration 344, loss = 0.12213854\n",
      "Iteration 345, loss = 0.12187928\n",
      "Iteration 346, loss = 0.12162148\n",
      "Iteration 347, loss = 0.12136513\n",
      "Iteration 348, loss = 0.12111023\n",
      "Iteration 349, loss = 0.12085675\n",
      "Iteration 350, loss = 0.12060471\n",
      "Iteration 351, loss = 0.12035407\n",
      "Iteration 352, loss = 0.12010484\n",
      "Iteration 353, loss = 0.11985700\n",
      "Iteration 354, loss = 0.11961054\n",
      "Iteration 355, loss = 0.11936545\n",
      "Iteration 356, loss = 0.11912172\n",
      "Iteration 357, loss = 0.11887935\n",
      "Iteration 358, loss = 0.11863832\n",
      "Iteration 359, loss = 0.11839862\n",
      "Iteration 360, loss = 0.11816025\n",
      "Iteration 361, loss = 0.11792319\n",
      "Iteration 362, loss = 0.11768743\n",
      "Iteration 363, loss = 0.11745297\n",
      "Iteration 364, loss = 0.11721979\n",
      "Iteration 365, loss = 0.11698789\n",
      "Iteration 366, loss = 0.11675726\n",
      "Iteration 367, loss = 0.11652789\n",
      "Iteration 368, loss = 0.11629976\n",
      "Iteration 369, loss = 0.11607288\n",
      "Iteration 370, loss = 0.11584723\n",
      "Iteration 371, loss = 0.11562280\n",
      "Iteration 372, loss = 0.11539958\n",
      "Iteration 373, loss = 0.11517757\n",
      "Iteration 374, loss = 0.11495676\n",
      "Iteration 375, loss = 0.11473714\n",
      "Iteration 376, loss = 0.11451869\n",
      "Iteration 377, loss = 0.11430142\n",
      "Iteration 378, loss = 0.11408531\n",
      "Iteration 379, loss = 0.11387035\n",
      "Iteration 380, loss = 0.11365655\n",
      "Iteration 381, loss = 0.11344388\n",
      "Iteration 382, loss = 0.11323234\n",
      "Iteration 383, loss = 0.11302193\n",
      "Iteration 384, loss = 0.11281263\n",
      "Iteration 385, loss = 0.11260444\n",
      "Iteration 386, loss = 0.11239735\n",
      "Iteration 387, loss = 0.11219135\n",
      "Iteration 388, loss = 0.11198643\n",
      "Iteration 389, loss = 0.11178259\n",
      "Iteration 390, loss = 0.11157982\n",
      "Iteration 391, loss = 0.11137812\n",
      "Iteration 392, loss = 0.11117746\n",
      "Iteration 393, loss = 0.11097786\n",
      "Iteration 394, loss = 0.11077929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 395, loss = 0.11058176\n",
      "Iteration 396, loss = 0.11038525\n",
      "Iteration 397, loss = 0.11018976\n",
      "Iteration 398, loss = 0.10999528\n",
      "Iteration 399, loss = 0.10980181\n",
      "Iteration 400, loss = 0.10960933\n",
      "Iteration 401, loss = 0.10941784\n",
      "Iteration 402, loss = 0.10922734\n",
      "Iteration 403, loss = 0.10903782\n",
      "Iteration 404, loss = 0.10884926\n",
      "Iteration 405, loss = 0.10866167\n",
      "Iteration 406, loss = 0.10847504\n",
      "Iteration 407, loss = 0.10828935\n",
      "Iteration 408, loss = 0.10810461\n",
      "Iteration 409, loss = 0.10792081\n",
      "Iteration 410, loss = 0.10773794\n",
      "Iteration 411, loss = 0.10755599\n",
      "Iteration 412, loss = 0.10737497\n",
      "Iteration 413, loss = 0.10719485\n",
      "Iteration 414, loss = 0.10701564\n",
      "Iteration 415, loss = 0.10683734\n",
      "Iteration 416, loss = 0.10665992\n",
      "Iteration 417, loss = 0.10648340\n",
      "Iteration 418, loss = 0.10630775\n",
      "Iteration 419, loss = 0.10613298\n",
      "Iteration 420, loss = 0.10595909\n",
      "Iteration 421, loss = 0.10578605\n",
      "Iteration 422, loss = 0.10561388\n",
      "Iteration 423, loss = 0.10544255\n",
      "Iteration 424, loss = 0.10527208\n",
      "Iteration 425, loss = 0.10510245\n",
      "Iteration 426, loss = 0.10493365\n",
      "Iteration 427, loss = 0.10476568\n",
      "Iteration 428, loss = 0.10459854\n",
      "Iteration 429, loss = 0.10443221\n",
      "Iteration 430, loss = 0.10426671\n",
      "Iteration 431, loss = 0.10410200\n",
      "Iteration 432, loss = 0.10393811\n",
      "Iteration 433, loss = 0.10377501\n",
      "Iteration 434, loss = 0.10361270\n",
      "Iteration 435, loss = 0.10345118\n",
      "Iteration 436, loss = 0.10329044\n",
      "Iteration 437, loss = 0.10313048\n",
      "Iteration 438, loss = 0.10297129\n",
      "Iteration 439, loss = 0.10281287\n",
      "Iteration 440, loss = 0.10265521\n",
      "Iteration 441, loss = 0.10249830\n",
      "Iteration 442, loss = 0.10234215\n",
      "Iteration 443, loss = 0.10218674\n",
      "Iteration 444, loss = 0.10203208\n",
      "Iteration 445, loss = 0.10187815\n",
      "Iteration 446, loss = 0.10172496\n",
      "Iteration 447, loss = 0.10157250\n",
      "Iteration 448, loss = 0.10142075\n",
      "Iteration 449, loss = 0.10126973\n",
      "Iteration 450, loss = 0.10111942\n",
      "Iteration 451, loss = 0.10096982\n",
      "Iteration 452, loss = 0.10082092\n",
      "Iteration 453, loss = 0.10067272\n",
      "Iteration 454, loss = 0.10052522\n",
      "Iteration 455, loss = 0.10037841\n",
      "Iteration 456, loss = 0.10023229\n",
      "Iteration 457, loss = 0.10008684\n",
      "Iteration 458, loss = 0.09994208\n",
      "Iteration 459, loss = 0.09979799\n",
      "Iteration 460, loss = 0.09965457\n",
      "Iteration 461, loss = 0.09951181\n",
      "Iteration 462, loss = 0.09936972\n",
      "Iteration 463, loss = 0.09922828\n",
      "Iteration 464, loss = 0.09908749\n",
      "Iteration 465, loss = 0.09894735\n",
      "Iteration 466, loss = 0.09880786\n",
      "Iteration 467, loss = 0.09866901\n",
      "Iteration 468, loss = 0.09853079\n",
      "Iteration 469, loss = 0.09839320\n",
      "Iteration 470, loss = 0.09825624\n",
      "Iteration 471, loss = 0.09811991\n",
      "Iteration 472, loss = 0.09798420\n",
      "Iteration 473, loss = 0.09784910\n",
      "Iteration 474, loss = 0.09771461\n",
      "Iteration 475, loss = 0.09758074\n",
      "Iteration 476, loss = 0.09744747\n",
      "Iteration 477, loss = 0.09731480\n",
      "Iteration 478, loss = 0.09718272\n",
      "Iteration 479, loss = 0.09705124\n",
      "Iteration 480, loss = 0.09692035\n",
      "Iteration 481, loss = 0.09679005\n",
      "Iteration 482, loss = 0.09666033\n",
      "Iteration 483, loss = 0.09653119\n",
      "Iteration 484, loss = 0.09640262\n",
      "Iteration 485, loss = 0.09627463\n",
      "Iteration 486, loss = 0.09614720\n",
      "Iteration 487, loss = 0.09602034\n",
      "Iteration 488, loss = 0.09589404\n",
      "Iteration 489, loss = 0.09576830\n",
      "Iteration 490, loss = 0.09564312\n",
      "Iteration 491, loss = 0.09551848\n",
      "Iteration 492, loss = 0.09539439\n",
      "Iteration 493, loss = 0.09527085\n",
      "Iteration 494, loss = 0.09514785\n",
      "Iteration 495, loss = 0.09502539\n",
      "Iteration 496, loss = 0.09490346\n",
      "Iteration 497, loss = 0.09478206\n",
      "Iteration 498, loss = 0.09466119\n",
      "Iteration 499, loss = 0.09454085\n",
      "Iteration 500, loss = 0.09442103\n",
      "0.9666666666666668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Murilo\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_dt = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_dt.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.95      0.97        20\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      " Neural Network model accuracy(in %): 97.77777777777777\n",
      "[[13  0  0]\n",
      " [ 0 19  1]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import metrics \n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\" Neural Network model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision regions in 2D\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "#Plotting decision regions\n",
    "plot_decision_regions(X, y, clf=classifier_dt, legend=2)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.title('Decision Tree classifier on Iris')\n",
    "plt.legend(target_names_iris)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
